Automatisierte Generierung von
plattformübergreifenden
Wissensnetzwerken mit Metadaten
und Volltextindexierung


Bachelorarbeit
Andreas Waldis, Patrick Siegfried
8. Juni 2017


Projektpartner & Betreuer: Prof. Dr. sc. inf. Michael Kaufmann
Experte: Urs Zumstein


Departement Informatik, Hochschule Luzern


Bachelor Diplomarbeit an der Hochschule Luzern – Informatik


Titel: Automatisierte Generierung von plattformübergreifenden Wissensnetzwerken mit
Metadaten und Volltextindexierung


Diplomandin / Diplomand: Andreas Waldis


Diplomandin / Diplomand: Patrick Siegfried


Studiengang: BSc Informatik


Abschlussjahr: 2017


Dozentin / Dozent: Prof. Dr. sc. inf. Michael Kaufmann


Diplomexpertin / Diplomexperte: Urs Zumstein


Codierung / Klassifizierung der Arbeit:


þ A: Einsicht (Normalfall)


o B: Rücksprache (Dauer: Jahr / Jahre)


o C: Sperre (Dauer: Jahr / Jahre)


Eingangsvisum:


Visum: ____________________


o termingerecht (9. Juni 2017, bis 16:00 Uhr) o verspätet


Hinweis: Diese Ausgabe der Bachelor Diplomarbeit wurde von keinem Dozierenden
nachbearbeitet. Veröffentlichungen (auch auszugsweise) sind ohne das Einverständnis der
Studiengangleitung der Hochschule Luzern – Informatik nicht erlaubt.


Verdankung


An dieser Stelle möchten wir uns bei all denjenigen bedanken, welche uns während dieser Arbeit
motiviert und unterstützt haben. Im Speziellen Michael Kaufmann, welcher uns die Chance gab, an
seinem Forschungsprojekt mitzuarbeiten. Wir schätzten insbesondere die angenehme
Zusammenarbeit und die wöchentlichen Anregungen sehr. Ein Dankeschön gilt auch Kevin
Stadelmann, welcher uns mit seiner Arbeit wertvolle Unterstützung lieferte.


Weiter möchten wir uns auch bei unserem Diplomexperten Urs Zumstein für die Begleitung unserer
Arbeit bedanken.


Selbstständigkeit


Hiermit erklären wir, dass wir die vorliegende Arbeit „Automatisierte Generierung von
plattformübergreifenden Wissensnetzwerken mit Metadaten und Volltextindexierung“ selbstständig
angefertigt und keine anderen als die angegebenen Hilfsmittel verwendet haben.


Redlichkeit


Sämtliche verwendeten Textausschnitte, Zitate oder Inhalte anderer Verfasser werden in
Übereinstimmung mit Art. 25, Abs. 2 URG ausdrücklich als solche gekennzeichnet.


Ort / Datum, Unterschrift


Ort / Datum, Unterschrift


Copyright © 2017 Hochschule Luzern – Informatik


Alle Rechte vorbehalten. Kein Teil dieser Arbeit darf ohne die schriftliche Genehmigung der
Studiengangleitung der Hochschule Luzern – Informatik in irgendeiner Form reproduziert oder in
eine von Maschinen verwendete Sprache übertragen werden.


Abstract


The project Intuitive Knowledge Connectivity offers a technical web
interface giving the user the possibility to interconnect and label exter-
nal data sources. Based on this project the following thesis provides a
full-text index to search all the user-defined data sources. Additional-
ly it presents an algorithm to extract important phrases inside a text
based on an entire corpus of documents. It combines stochastic ap-
proaches with rule-based natural language processing methods. This
task consists of stages like preparation, keyword generation, score
calculation, selection. The shown algorithm addresses the project’s
demand of information retrieval. Furthermore, the implementation
embeds the resulting business logic into two separate services to sup-
port. In conclusion the evaluation is based on an expert review enclo-
sed with a swot analysis and an open discussion.


Das Projekt Intuitive Knowledge Connectivity bietet eine Web-Appli-
kation, welche die Verbindung und Beschriftung von Daten mehrerer
externen Quellen ermöglicht. Darauf basierend beschäftigt sich die
vorliegende Arbeit mit einem Volltextindex, welcher die Suche ü ber
alle benutzerdefinierten Datenquellen ermöglicht. Zusätzlich präsen-
tiert sie einen Algorithmus, welcher wichtige Begriffe eines einzelnen
Textes auf Basis eines Dokumentenkorpus extrahiert. Dabei verbindet
er statistische und sprachwissenschaftliche Ansätze. Dieser Vorgang
besteht aus Vorbereitung, Keyword Extraction, Berechnung einer Me-
trik und Auswahl. Der gezeigte Algorithmus beschäftigt sich mit dem
Problem der Informationsgewinnung. The Implementation teilt die
entstandene Applikation zur Unterstü tzung in zwei individuelle Ser-
vices auf. Abschliessend wurde eine Evaluation auf Basis eines Exper-
ten-Workshops, einer SWOT-Analyse und einer offenen Diskussion
durchgefü hrt.


Kapitel 1


Einleitung


Mit dem Forschungsprojekt IKC als Grundlage beschäftigt sich die
vorliegende Arbeit vertieft mit der Analyse von Datenquellen und
der Extraktion von relevanten Informationen. Aktuell werden Daten-
quellen verknü pft, jedoch wird deren Inhalt noch nirgends genutzt.


Mit der Analyse dieser Inhalte werden zusätzliche Informationen Teil
des Wissensnetzwerks. Dadurch wird aus einer reinen Verknü pfung
von Datenquellen eine tatsächliche Integration der Informationen.


Zum besseren Verständnis werden potentiell unbekannte oder pro-
jekt-spezifische Begriffe kursiv und fett dargestellt. Zu jedem Begriff
ist eine kurze Beschreibung im Glossar (Kapitel Glossar) am Ende
der Dokumentation zu finden. Im Anhang sind neben der Aufgaben-
stellung (Abschnitt A.3) auch das Arbeitsjounrnal (Abschnitt A.2) bei-
gelegt. Der dokumentierte Sourcecode$$$1$$$ ist auf dem entsprechenden
Repository der Hochschule Luzern verfü gbar.


1.1 Ausgangslage


Abbildung 1.1 zeigt den Kontext des zu entwicklenden Prototypen.
Der Benutzer arbeitet wie anhin mit dem ikc-core. Dieser wird aber
vor allem im Hintergrund aber auch in der Benutzeroberfläche auf
die neue Funktionalität hin angepasst und optimiert. Der Prototyp
nimmt Anfragen fü r Keyphrases und fü r Suchresultate fü r den ent-
sprechenden Begriff entgegen und liefert die angeforderten Resultate
zurü ck.


1https://gitlab.enterpriselab.ch/ikc


Abbildung 1.1: Kontextdiagramm


Keyphrases sind Begriffe, welche aus einem oder mehreren Wörtern
bestehend und den zugrunde liegenden Text kurz und prägnant fü r
den Benutzer zusammenfassen. Fü r weitere Informationen steht das
Glossar zur Verfü gung (Kapitel Glossar).


Basierend auf der Aufgabenstellung, dem Projekt-Kontext und den
bestehenden Systemen definieren die folgenden Punkte die Ausgangs-
lage:


• Der bestehende Prototyp ikc-core dient als Grundlage fü r die-
se Bachelorarbeit. Mit dem Fokus der reinen Datenanalyse wird
auf die Datenquellen Dropbox und Evernote verzichtet. Der re-
sultierende Prototyp baut auf der bestehenden Umgebung auf.


• Aufgrund der bisherigen Erfahrungen und dem bestehenden
Code wird als Programmiersprache weiterhin Typescript einge-
setzt. Dies hat unter anderem den Vorteil, dass Typescript so-
wohl client- als auch serverseitig lauffähig ist.


• Der Prototyp soll nach wie vor im Browser ausgefü hrt werden.
Jegliche Daten werden nur an benutzerdefinierten Orten gespei-
chert. Es wird keine zusätzlich Persistenz eingefü hrt.


• Als Plattform wird weiterhin das Docker-basierte Dokku einge-
setzt. Dieses wird bereits fü r den Proof of Concept (PoC) und
fü r das Prototyping verwendet.


1.2 Scope


Zur Abgrenzung definiert der Scope den Inhalt und den Umfang ei-
nes Projektes. Fü r dieses Projekt ist er wie folgt festgelegt:


• Da die Herkunft der Informationen fü r die Analyse nicht wich-
tig ist, beschränkt sich die Arbeit auf Text-Dateien. Versprechen


die Resultate einen grossen Mehrwert, können die Konzepte
später fü r die Verwendung mit weiteren Datenquellen erweitert
werden.


• Um mögliche Schwierigkeiten und Hindernisse mit Dropbox
von Beginn an auszuschliessen, wird eine neue Datenquelle und
Persistenzbasis auf der Grundlage von SFTP eingefü hrt.


• Der Projektpartner stellt die Testdaten auf Basis einer Samm-
lung von Wikipedia-Artikeln zur Verfü gung. Möglicherweise
muss die Auswahl aufgrund der grossen Datenmenge einge-
schränkt werden.


• Aufgrund der Anforderungsanalyse$$$2$$$ mit dem Projektpartner
wurde das Projekt noch spezifischer auf die Schlü sselwortex-
traktion fokussiert. Ein SFTP-Service ersetzt dabei eine Anbin-
dung an Dropbox und Evernote komplett.


• Die Implementation beschränkt sich auf die herkömmliche Ver-
sion des ikc-cores ohne die Visualisierung. Dies hat den Grund,
dass zum Zeitpunkt des Projektstarts die Visualisierung noch
nich ausreichend stabil lauffähig war.


1.3 Projektstrukturplan


Die Abbildung A.$$$1$$$ gewährt einen Ü berblick ü ber das Projekt. Sie
stellt die wichtigsten Bereiche und Phasen dar, in welche die Arbeit
grob eingegliedert werden kann:


1. Die Projektfü hrung beinhaltet die Planung des Vorhabens ü ber
den gegebenen Zeitraum. Ständige Kontrolle des Ist- gegenü ber
dem Soll-Zustand kann gegebenenfalls jederzeit zur Steuerung
oder Anpassungen des Zeitplans fü hren. Da das Projekt agil
organisiert ist, liegt das Augenmerk auf der Priorisierung der
Anforderungen.


2. In der Konzeption werden neben den Anforderungen auch vor-
stellbare Lösungsansätze in den Bereichen Architektur sowie
Schnittstellen geprü ft.


3. Nach einer anfänglichen Recherchephase werden zu Testzwe-
cken bereits erste Prototypen entwickelt. So sollen mögliche Op-
tionen ü berprü ft und gegebenenfalls später implementiert oder
weiterentwickelt werden. Nach einer Evaluation werden die ge-
eigneten Lösungen ausgewählt.


2Protokoll der Anforderungsanalyse Abschnitt A.4


4. In der Entwicklung werden die gesammelten Erkenntnisse ge-
sammelt und analysiert. Grundsätzlich soll auf den zuvor ent-
wickelten Prototypen aufgebaut werden. Zunächst wird eigen-
ständig, ohne Einbindung in den ikc-core entwickelt.


5. Sobald die Implementierung die erforderlichen Anforderungen
reibungslos erfü llt, wird sie in den bestehenden ikc-core inte-
griert. Nach letzten Optimierungen sind nun alle Anforderun-
gen erfü llt und intensivere Tests können durchgefü hrt werden.
Fü r die Entwicklung werden hauptsächlich Integrationstests ver-
wendet. Fü r einen besseren Ü berblick ist das Testkonzept in der
Tabelle A.$$$1$$$ zu finden. Anhand dieses wurde die Funktionalität
während der Entwicklung laufend getestet.


6. Nachdem die Entwicklungsarbeit abgeschlossen ist, folgt der
Projektabschluss. Dabei wird die endgü ltige Version des Pro-
jektreports erstellt und die Abschlusspräsentation gehalten.


1.4 Rahmenplan


Die Rahmenplanung, basierend auf dem Projektstrukturplan (Abbil-
dung A.2), repräsentiert die zeitliche Planung des Projekts. Dabei
werden Kalenderwochen anstelle von Daten oder Schulwochen ver-
wendet. Enthalten sind alle Projektphasen, Sprints und Meilensteine,
als auch alle Lieferobjekte welche im Unterabschnitt A.5.6 weiter aus-
gefü hrt werden. Die Dauer der Sprints wird bewusst unterschiedlich
ausgestaltet, um den verschiedenen Projektphasen und deren Inhal-
ten bestmöglich Rechnung zu tragen.


Ein wichtiger Teil der Rahmenplanung sind die Meilensteine. Sie un-
terteilen das Projekt in Phasen, welche dadurch klar voneinander ge-
trennt sind. Ebenfalls sind sie eine wichtige Orientierungshilfe im
Projekt und weisen den Weg zu einem erfolgreichen Abschluss. Sie
werden in Tabelle A.2 aufgelistet.


1.5 Projektziele


Projektziele werden definiert, um den Erfolg an ausgewählten Punk-
ten zu ü berprü fen und sicherstellen. Sie wurden in Absprache mit
dem Projektpartner definiert. Die Ziele sind in der folgenden Tabel-
le 1.1 aufgelistet und anschliessend genauer ausgefü hrt.


Tabelle 1.1: Projektziele


1. Z1: Der Prototyp bietet eine Volltextsuche ü ber den gesamten
Inhalt der Dokumente einer externen Datenquelle an. Die Such-
funktion ist in den ikc-core integriert.


2. Z2: Zu den einzelnen Dokumenten extrahiert der Prototyp aus
dem Text relevante Keyphrases.


3. Z3: Diese Keyphrases werden nach der Extraktion direkt Teil
des Wissensnetzwerkes. Diese können vom Benutzer wieder ent-
fernt werden.


4. Z4: Sowohl Keyphrase als auch Dokument werden als Node in-
nerhalb des ikc-core repräsentiert und sind miteinander verbun-
den.


1.6 Anforderungen


Fü r die weitere Unterteilung in Arbeitspakete und Stories werden die
Anforderungen zunächst in Prosa gesammelt. Diese entstammen dem
Kundenworkshop und der Aufgabenstellung. Sie widerspiegeln die
Projektziele (Abschnitt 1.5). Die Anforderungen werden in funktiona-
le und nicht-funktionale Anforderungen unterschieden. Die funktio-
nalen Anforderungen definieren direkt die Eigenschaften. Im Gegen-
satz dazu definieren nicht-funktionale Anforderungen die Leistung
und die Randbedingungen. Diese sind in der Tabelle Tabelle A.3, be-
ziehungsweise Tabelle A.$$$4$$$ zu finden.


Die Priorisierung erfolgt nach dem MoSCoW-System:


Tabelle 1.2: MosCow-Priorisierung


Neben den in der Aufgabenstellung vorgegebenen Lieferobjekte (Ta-
belle A.6) sind noch zusätzliche, interne Lieferobjekte (Tabelle A.7)
festlegt. Diese sind lediglich als Unterstü tzung der Projektkontrolle,
eine Art Orientierungshilfe, gedacht.


Kapitel 2


Stand der Technik


Im Folgenden werden bestehende literarische Grundlagen fü r die vor-
liegende Arbeit aus Forschung und Entwicklung kurz zusammenge-
fasst und die wichtigsten Punkte aufgezeigt. Der Inhalt befasst sich
vorwiegend mit Konzepten, Theorien und Begrifflichkeiten, welche
fü r das tiefere Verständnis der vorliegenden Thematik unabdingbar
sind. Auch wurden bestehende Verfahren, welche ähnliche Ziele ha-
ben, genauer untersucht.


2.1 Wissensmanagement


Die Grundlage dieser Arbeit bilden die Daten eines Nutzers. Diese
alleine sind aber noch von keinem grossen Nutzen. Wie Bellinger,
Castro und Mills (2004) zeigen, beschäftigt man sich häufig mit der
Ü bertragung und dem Empfang von Informationen. Viel weniger be-
fasst man sich aber mit derselbigen mit der Grundlage von Wissen
oder gar Verständnis.


Auch die Unterscheidung von Daten, Informationen und Wissen ist
selten explizit festgelegt. Fü r ein tieferes Verständnis hilft folgendes
Zitat aus Ackoff (1989). Es stellt die Dimensionen zwischen den ver-
schiedenen Ausdrü cken her: Daten bilden die Grundlage. Informatio-
nen basieren auf Daten, besitzen aber bereits einen grossen Mehrwert.
Der nächste Schritt ist der Aufbau von Wissen, und dieses basiert
wiederum auf Informationen und somit Daten. Versteht man nun das
Wissen zusätzlich, ist man am Ziel angelangt. Die Daten sind vollkom-
men genutzt, man erlangt den grössten Mehrwert. Eine Definition der
Ausdrü cke folgt im nächsten Absatz.


An ounce of information is worth a pound of data.
An ounce of knowledge is worth a pound of information.
An ounce of understanding is worth a pound of knowledge.


Bellinger et al. (2004) definieren diese Ausdrü cke folgendermassen:
Daten sind Zeichen oder Symbole, welche Eigenschaften von Inhalten
repräsentieren. Informationen bestehen aus aufbereiteten oder verar-
beiteten Daten. Diese Vorgänge haben das Ziel, die Bedeutung und
den Nutzen der Daten zu erhöhen. Wissen ist eine Sammlung an In-
formationen. Es ist ein Prozess, welcher zum Ziel hat, die Informatio-
nen nü tzlich zu gestalten.


Nach Chen (2005) ist der Wert von Informationen gegeben durch Zeit-
losigkeit, Zugänglichkeit, Zuverlässigkeit und Verfü gbarkeit. Ä hnlich
wichtig in Choo (1996) sind aber die Bedü rfnisse des Nutzers. Eine
weitere, offenere Definition stammt von Bierly III, Kessler und Chris-
tensen (2000): Informationen sind bedeutungsvolle und nü tzliche Da-
ten, welche das Verständnis von Informationen klarer machen. Wis-
sen nach Barlas, Ginart und Dorrity (2005) ist eine Aggregation von
ähnlichen oder zusammengehörigen Informationen.


2.1.1 Bedeutung in dieser Arbeit


In der vorliegenden Arbeit wird alles als Information verstanden, was
auf Nutzerdaten basiert und gleichzeitig fü r den Nutzer einen gewis-
sen Mehrwert an Bedeutung oder Verständnis darstellt. Die Aggre-
gation und vor allem die Verknü pfung der Informationen zu einem
Wissensnetzwerk definiert das Wissen im gegebenen Kontext.


2.2 Intuitive Knowledge Connectivity


Das Forschungsprojekt Intuitive Knowledge Connectivity (IKC)1 der
Hochschule Luzern Informatik ist die Grundlage fü r diese Bachelor-
Arbeit. Aus diesem Grund ist hier fü r das tiefere Verständnis der The-
matik eine kurze Einfü hrung zu finden (siehe auch Kaufmann et al.
(2016)).


Wie auch zu sehen in Manyika et al. (2011) bringt Big Data Heraus-
forderungen mit sich. Das steigende Volumen und die immer detail-
lierten Informationen, zusammen mit Trends wie Social Media und


1https://www.hslu.ch/en/lucerne-university-of-applied-sciences-and
-arts/research/projects/detail/?pid=3631


Internet Of Things, fü hren schnell zu exponentiellem Wachstum der
Daten. Die immense Flut gilt es auszuwerten und mit einem Mehr-
wert produktiv zu nutzen.


Im unternehmerischen Umfeld gibt es diverse Möglichkeiten fü r die
Verarbeitung und die Analyse der Daten. Diese agieren immer im
Kontext der Unternehmung als Ganzes. Vor ähnlichen Problemen,
wie grosse Unternehmungen, stehen aber auch immer mehr Einzel-
personen. IKC befasst sich darum mit dem persönlichen Datenmana-
gement in Zusammenhang mit Big Data und Cloud Computing.


Die bestehenden Ansätze der Verarbeitung bilden die Grundlage. IKC
geht aber einen Schritt weiter, indem es Verbindungen ü ber die Gren-
zen von verschiedenen Cloud-Dienstleistern ermöglicht. Einzelperso-
nen können so ihr persönliches, plattformü bergreifenden Wissens-
netzwerkes aufbauen. Durch die Verknü pfung von Daten aus den ver-
schieden Quellen entsteht neues und wertvolles Wissen. Gleichzeitig
ermöglicht die Aggregation eine einheitliche Verwaltung und Suche
der eigentlich verteilten Informationen.


Aus der Forschung ging ein PoC hervor, welcher die vorgeschlagenen
Konzepte anhand von Dropbox, Evernote und Weblinks erfolgreich
umsetzt. Dieser Prototyp bildet die Grundlage fü r die Implementati-
on dieser Arbeit.


2.3 Artifical Intelligence


Wie Russell und Norvig (2009) aufzeigen, ist Artifical Intelligence im
Allgemeinen als Versuch eines Systems, menschliches Verhalten be-
ziehungsweise Intelligenz zu automatisieren oder zu simulieren, defi-
niert. Die Definition solcher Systeme kann grundsätzlich in vier Kate-
gorien aufgeteilt werden:


• Systeme, die wie Menschen denken.


• Systeme, die rational denken.


• Systeme, die wie Menschen handeln.


• Systeme, die rational denken.


Menschliche Systeme versuchen Entscheidungen mit dem Menschen
ähnlichen Prozessen zu fällen. Während rationale Systeme Entschei-
dungen basierend auf den vorliegenden Daten treffen. Diese Unter-
schiede zeigen gleichzeitig auch die Evolution in der Forschung im
Bereich von Artifical Intelligence an. Bis 1970 wurde versucht, ein


System zu entwickeln, welches wie ein Mensch denkt und handelt.
Von 1970 an gelangten jedoch Systeme mit einer rationalen Intelli-
genz, welche jederzeit die richtige Entscheidung treffen können, in
den Fokus.


Systeme, welche Verfahren aus dem Feld von Artifical Intelligence
anwenden, werden auch Agenten genannt. Abhängig vom gewählten
Verfahren, entscheiden solche Agenten, auf Grund unterschiedlicher
Argumentation, wie auf ein Ereignis zu reagieren ist. Dabei werden
drei unterschiedliche Ebenen der Argumentation unterschieden:


• Assoziative Argumentation, was wenn der Agent bestimmte
Ereignisse beobachtet?


• Ursächliche Argumentation, was wenn der Agent bestimmte
Aktionen durchfü hrt?


• Gegensätzliche Argumentation, was wenn der Agent bestimm-
te Aktionen nicht durchgefü hrt hätte?


Diese verschiedenen Ebenen unterstü tzen die Entscheidung zur Aus-
wahl von Teildaten, welche fü r den jeweiligen Anwendungsfall wich-
tig sind.


So beeinflussen sich beispielsweise die Anzahl von Verbindungen
einer Mobilantenne neben einem Autobahnabschnitt und das Stau-
Aufkommen auf dem gleichen Abschnitt gegenseitig. Während der
Wochentag und der Geburtstag des Beifahrers keinen Einfluss auf
das Stauaufkommen haben. Dies obwohl sie zu diesem Zeitpunkt in
einer Beziehung zu einander stehen.


2.4 Machine Learning


Neben Natural Language Processing oder Computer Vision ist Ma-
chine Learning ein weiterer Aspekt aus dem Bereich von Artifical
Intelligence. Dabei gewinnt ein Agent, auf Grund von Mustern inner-
halb von Daten neue Informationen. Dieser Vorgang wird allgemein
als Lernen bezeichnet. Grundsätzlich können nach Russell und Nor-
vig (2009) folgende drei Kategorien des Lernens unterschieden wer-
den:


• Supervised learning: Anhand von beobachteten und beschrif-
teten Ereignissen und den dazugehörigen Aktionen lernt der
Agent eine Funktion, mit welcher er auf zukü nftige Ereignis-
se reagieren kann. Zum Beispiel die Erkennung von Frü chten


auf Bildern, anhand einer Bildmenge, welche mit der jeweiligen
Frucht beschriftet ist.


• Unsupervised learning: Innerhalb von beobachteten und unbe-
schrifteten Daten erkennt der Agent Muster, welche er nutzt, um
auf zukü nftige Ereignisse zu reagieren. So erkennt beispielswei-
se ein Navigationssystem Tage mit viel Stau anhand der Fahr-
zeit fü r eine zurü ckgelegte Strecke. Dies funktioniert ohne Da-
tensätze, welche einen Tag mit viel Stau beschreiben.


• Reinforcement learning: Basierend auf einer Belohnung lernt
ein Agent, mit welcher Reaktion er eine bestimmte Situation
entgegnen muss. So kann zum Beispiel ein lauffähiger Roboter
lernen, wie er Hindernisse im Gelände bewältigen kann. Dabei
ist es denkbar, dass die Aktion durch die verschiedenen Winkel
seiner Gelenke und der Geschwindigkeit gegeben ist. Diese op-
timiert er anschliessend anhand der Belohnung. Die Belohnung
ist ein Indikator fü r den Erfolg oder den Nichterfolg.


2.5 Natural Language Processing


Unter Natural Language Processing wird die maschinelle Verarbei-
tung von natü rlicher Sprache (Wort und Schrift) verstanden. Dabei
werden Methoden und Erkenntnisse aus der klassischen Linguistik
angewendet. So können Wortarten bestimmt, Wortstämme gebildet
oder Satzstrukturen analysiert werden. Mit Hilfe solcher Informatio-
nen können Algorithmen formuliert werden. Diese können zum Bei-
spiel eine Stimmungsanalyse von Texten durchzufü hren.


Generally speaking, systems based entirely on natural lan-
guage concepts are not at all competitive with systems ba-
sed on statistical analysis of texts.
Kantor (2001)


Grundsätzlich sind Algorithmen, welche ausschliesslich auf Konzep-
ten der natü rlichen Sprache aufbauen, nach Kantor (2001) nicht ver-
gleichbar mit denjenigen, welche lediglich auf statistischen Analysen
beruhen. Aber sprachwissenschaftliche Konzepte als zusätzliche Ba-
sis einer Analyse sind durchaus wertvoll.


2.5.1 Tokenization


Ein wichtiger Begriff in diesem Kontext ist Tokenization im Sinne von
Grefenstette und Tapanainen (1994). Ein zu verarbeitender Text ist in


diesem Stadium repräsentiert durch einen langen String. Er besteht
aus aneinandergereihten Zeichen. Der Vorgang des Tokenization teilt
den Text zunächst nach Satz- und Sonderzeichen auf. Anschliessend
werden die Sätze und Satzteile (beispielsweise nach Leerschlägen)
in einzelne Einheiten (Tokens) aufgesplittet. Diese können in einem
späteren Schritt in bestimmte syntaktische Klassen eingeteilt werden.


2.5.2 Stemming


Beim Stemming geht es nach Porter (1980) um die Entfernung von
Suffixen eines Wortes. Wörter, die grundsätzlich eine ähnliche Bedeu-
tung oder einen ähnlichen Ursprung haben, besitzen oftmals einen
gemeinsamen Wortstamm. Durch den Verzicht auf Suffixe können ge-
meinsame Wortstämme gefunden werden.


Dies kann beispielsweise soweit gehen, dass Nomen und Verben auf
einen gemeinsamen Wortstamm reduziert werden können. Dies ist
aber nicht das in jeder Situation gewü nschte Endergebnis, denn oft-
mals ist es wichtig, die Wortarten unterscheiden zu können. Mit Stem-
ming ist immer auch ein Informationsverlust verbunden.


2.5.3 Stopwords


Als Stopwords bezeichnet man, wie beschrieben in Manning, Ragha-
van, Schü tze et al. (2008), ü blicherweise sehr häufig vorkommende
Wörter, welche zwar eine grammatikalische Funktion aber fü r die Ge-
winnung von Informationen keine weitere Bedeutung haben. Deren
Funktion ist eher von syntaktischer als semantischer Natur. Fü r die
Ermittlung dieser Stopwords gibt es nach Manning et al. (2008) und
Wilbur und Sirotkin (1992) sowohl statistische als auch heuristische
(regelbasierte) Verfahren.


2.5.4 Part of Speech Tagging


Wie in Brill (1992, 1994); Manning, Schü tze et al. (1999) erläutert, ord-
net der Prozess des Part of Speech Tagging einem Wort eine entspre-
chenden Wortart zu. Dabei werden die verschiedenen Algorithmen
in regelbasierte und stochastische Algorithmen unterschieden.


Regelbasierte nutzten eine Menge an Regeln, welche zur Bestimmung
der Wortart verwendet werden. Solche Regeln können zum Beispiel,
ein Wort einer Wortart, eine Endung einer Wortart oder einem Wort
anhand von den umliegenden Wörter eine Wortart zuweisen. Diese
Regeln sind fix im Algorithmus Codiert.


Bei der Verwendung einem stochastischen Algorithmus werden als
erstes Wörter in Trainingsdaten manuell einer Wortart zugeordnet.
Anhand diese Trainingsdaten können nun Regeln berechnet werden,
welche sich der Wortabfolge orientieren. Bei der Verwendung des Al-
gorithmus wird pro Wort die Wahrscheindlichste Wortart verwendet.


2.6 Textanalyse


Durch den Prozess der Textanalyse (oder Text Mining nach Tan et
al. (1999)) können Informationen aus einem Text extrahiert werden.
Basierend auf diesen Informationen werden Anwendungen wie eine
Volltextsuche oder die Erkennung von Ä hnlichkeiten oder wichtigen
Stichworten ermöglicht. Je nach Anwendungsfall werden statistische
Verfahren mit Techniken aus dem Feld von Natural Language Pro-
cessing (Abschnitt 2.5) kombiniert. Ein Beispiel fü r Textanalyse ist
die Keyphrase Extraction (Unterabschnitt 2.6.2).


2.6.1 Volltextsuche


Unter Volltextsuche versteht man die Funktionalität eine Menge von
Wörtern innerhalb eines Dokumentenkorpus zu suchen und bei Er-
folg auch zu finden. Die Grundlage fü r die Volltextsuche bildet der
Volltextindex, welcher alle in einem Text vorhandenen Wörter bein-
haltet.


2.6.2 Extraktion


Ein mögliches Resultat der Textanalyse stellen Schlü sselwörter (Key-
phrases) dar. Sie fassen im Idealfall, wie gesehen in (Zhang, Xu, Tang
& Li, 2006, S. 85), den zugrundeliegenden Text kurz und prägnant zu-
sammen, stellen damit wichtige semantische Informationen dar. Wei-
ter bilden sie eine fundamentale Grundlage fü r Dokumentenanalyse
und -klassifizierung.


Fü r die Generierung dieser Keyphrases benutzt man Verfahren der
Keyphrase Extraction. Hierbei geht nach es Hulth (2004) um die Aus-
wahl einer kleinen Menge an Wörtern oder Begriffen aus einem Text,
welche den Inhalt oder die Bedeutung möglichst gut widerspiegeln.


Diese Keyphrases bestehen aus einem oder mehreren Konzepten. Kon-
zepte beziehen sich auf Objekte, Entitäten, Ereignisse und Themen,
welche fü r suchende Benutzer interessant sein könnten (Dalvi et al.
(2009)). Die Definition ist bewusst offen gewählt, da sich eine ge-


nauere Eingrenzung schwierig gestaltet. Eine weiterfü hrende Defini-
tion geht in Richtung Nutzen fü r Menschen: Sobald eine gewisse An-
zahl von Personen etwas als Konzept erkennt, kann dieses als Kon-
zept angesehen werden (Parameswaran, Garcia-Molina und Rajara-
man (2010)). Das Nutzung von Konzepten findet unter anderen bei
Suchmaschinen, automatischem Tagging statt.


Nach der Auswahl aller potentiellen Keyphrases wird zunächst ver-
sucht, semantisch nicht wertvolle Begriffe auszuschliessen. Dies ist
ein wichtiger Schritt, denn die anschliessende Berechnung der Rele-
vanz ist ein sehr aufwändiger Prozess.


Vorgeschlagene Methoden fü r diesen Vorgang nehmen nach (Zhang
et al., 2006, S. 85) oftmals sogenannte globale Kontext-Informationen
zum Gebrauch. Diese beinhalten unter anderem die Häufigkeit eines
Begriffes innerhalb eines Dokuments und auch die Häufigkeit eines
Begriffes innerhalb des gesamten Dokumentenkorpus.


Häufigkeit und Gewichtung


Ein Dokument, welches einen gesuchten Begriff öfters enthält als an-
dere, ist mit grosser Wahrscheinlichkeit von grösserer Bedeutung als
eines, welches den Begriff weniger oft enthält. Die Häufigkeit eines
gesuchten Begriffes innerhalb eines Dokumentes ist somit ein wich-
tiger Teil der Gewichtung. Die Gewichtung ist die Grundlage zum
Vergleich der Relevanz der verschiedenen Begriffe. Der einfachste An-
satz der Bestimmung dieser Gewichtung besteht darin, diese mit der
Vorkommenshäufigkeit innerhalb eines Dokumentes gleichzusetzen.
Diese Gewichtung bezeichnet man als term frequency. In der Notati-
on als tf zu finden, wobei t fü r einen Begriff (term) innerhalb eines t,d
Dokumentes d steht.


Die alleinige Beachtung der Häufigkeit zur Bestimmung der Relevanz
bringt aber Probleme, alle Begriffe werden als gleichermassen wichtig
eingestuft. Bestimmte Begriff können aber von vornherein direkt als
weniger wichtig eingestuft werden. Kommt ein Begriff in einem Kor-
pus durchgängig in allen Dokumenten häufig vor, ist es fü r ein ein-
zelnes Dokument keine geeignete Grundlage fü r die Bestimmung der
Relevanz. Denn dieses Wort hebt einen Text gegenü ber dem Korpus
nicht ab, ist somit keine ausreichende Repräsentation des Inhaltes.


Ein erster Ansatz nach (Manning et al. (2008)) ist die Gewichtung
auf Grund der Vorkommenshäufigkeit eines Begriffes innerhalb des
Korpus zu skalieren. Dieser Wert wird als collection frequency (cf) be-
zeichnet. Je häufiger der Begriff, umso tiefer die Gewichtung. Begriffe,


welche im Allgemeinen häufig auftauchen wü rden, so in ihrer Rele-
vanz niedriger.


FOOTNOTE:Tabelle 2.1: Vergleich cf und df 2


Da aber versucht wird, mit einer Gewichtung auf Basis von unter-
schiedlichen Dokumenten zu arbeiten, macht es Sinn ebenfalls einen
Wert auf derselben Basis zu verwenden. Darum wird die Anzahl Do-
kumente verwendet, welche einen bestimmten Begriff enthalten. Die-
ser Wert wird als Dokumentenhäufigkeit (document frequency, df) be-
zeichnet und mit df notiert.


Der Grund, warum die document frequency bevorzugt wird, ist in Ta-
belle 2.1 aufgezeigt. Der cf-Wert der beiden Wörter ’insurance’ und
’try’ ist in etwa identisch. Doch beim df-Wert gibt es einen erheblichen
Unterschied. Die beiden Werte können sich somit also durchaus sehr
unterschiedlich verhalten. Zusätzlich kann man noch anmerken, dass
der df-Wert mehr dem intuitiv erwarteten Resultat entspricht. Ein
Schlü sselwort ’insurance’ sollte relevanter sein als ein Schlü sselwort
’try’.


Um nun die Gewichtung mit der Basis der Dokumentenhäufigkeit zu
skalieren, wird folgende Formel eingefü hrt:


Diese bezeichnet man als inverse Dokumentenhäufigkeit (inverse do-
cument frequency (idf)) fü r einen Begriff t. Die Gesamtanzahl der Do-
kumente ist durch N gegeben.
Manning et al. (2008)


TF-IDF


TF-IDF ist ein Gewichtungsalgorithmus zur Bestimmung der Rele-
vanz eines Begriffes. Wie oben bereits erwähnt, nimmt er Bezug auf
globale Kontext-Informationen. Er kombiniert die auf den im Ab-
schnitt 2.6.2 definierten Werte.


2(Manning et al., 2008, S. 118)


tf-idf = tf idf t,d t,d f ×


Der TF-IDF Wert ist hoch, wenn ein Begriff in einem Dokument oft,
in allen anderen Dokumenten hingegen nicht vorkommt. Er ist tief,
wenn der Begriff in vielen oder allen Dokumenten vorkommt.


Vector Space Model


Wie Manning et al. (2008) beschreibt nutzt das Vector Space Model hoch-
dimensionale Vektoren, um ein Wort zu repräsentieren. So kann ei-
ne Keyphrase oder ein Dokument zum Beispiel durch die Häufigkeit
der vorkommenden Wörter dargestellt werden. Mit Hilfe der Cosine
Distance kann nun der Winkel zwischen dem Keyphrase-Vektor und
einem Dokument-Vektor berechnet werden. Dadurch kann die Rele-
vanz der Keyphrase fü r das Dokument ausgedrü ckt werden. Weiter
ist es möglich, Wörter als Vektor durch die Häufigkeit anderer Wörter
in seiner Umgebung auszudrü cken.


Okapi BM$$$25$$$


Okapi BM$$$25$$$ ist, wie von Robertson, Zaragoza et al. (2009) beschrie-
ben, eine Weiterentwicklung von TF-IDF. Es wurden verschiedene
Steuerung-Parameter eingefü hrt, welche die Reduktion der Auswir-
kung von hohen Keyphrase-Frequenzen als auch der Grad der Nor-
malisierung regeln.


2.7 React


Nach (Alex Banks, 2017, S. 11-16) ist React eine populäre, von Face-
book entwickelte Bibliothek fü r Benutzeroberflächen. Sie basiert auf
Javascript und ist optimiert fü r die Skalierung und den Umgang mit
sich laufend ändernden Daten. Wie zu sehen in Facebook (2017) ist
React deklarativ und Komponenten-basiert. Fü r einen Zustand gibt
es die entsprechende Komponente, welche bei Aktualisierungen neu
gerendert wird . Es verwendet unter anderem eine XML ähnliche Syn-
tax fü r den Aufbau dieser Komponenten.


Zur selben Zeit veröffentlichte Facebook auf eine Design-Architektur
namens Flux (Alex Banks, 2017, S. 12). Bei diesem Ansatz wird mit-
tels einem unidirektionalen Datenfluss mit Dateninkonsistenzen um-
gegangen. Dies geht in den Bereich der funktionalen Programmie-


rung. React arbeitet mit einer vereinfachten Version von Flux namens
Redux.


2.8 Architekturpatterns


• Microservices: Das Microservice-Pattern gehört zufolge Rich-
ards (2016)[S. 1-33] zu den Service-basierten Architekturen. Ser-
vices bilden die Grundlage der einzelnen Komponenten. Ser-
vice-basierte Architekturen sind immer auch verteilte Architek-
turen. Das bedeutet weiter, dass Zugriffe ü ber remote-access-
Protkolle, beispielsweise REST, vorgenommen werden. Services
laufen entkoppelt und eigenständig. Das bedeutet weiter, dass
diese einfach skaliert oder modular neu genutzt werden können.


• Message Passing: Wie in (William Gropp, 2014, S. 5) beschrie-
ben, versteht man unter Message Passing das Senden und Emp-
fangen von Nachrichten zwischen Prozessen.


• Message oriented middleware: Message oriented Middleware
(MoM) ist nach (Chappell, 2004, S. 77-100) ein Konzept, welches
sich mit der Ü bermittlung von Daten zwischen Applikationen
oder Prozessen beschäftigt. Dies erfolgt ü ber einen Kommunika-
tionskanal, worü ber unabhängige Informationseinheiten (Mes-
sages) ü bermittelt werden. Die Kommunikation zwischen zwei
Teilnehmern funktioniert asynchron. Durch die Message-basier-
te Kommunikation ist eine abstrakte Entkopplung gewährleistet.
Die Sender und Empfänger wissen nichts voneinander. Dieses
Messaging-System ist verantwortlich fü r die fehlerlose Ü bermit-
tlung an die jeweiligen Empfänger.


2.9 Kommunikation


Die Kommunikation beschäftigt sich mit den grundlegenden Techno-
logien zum Austausch von Informationen. Im Gegensatz zum Kapi-
tel Datentransfer (Abschnitt 2.10) befasst sich dieser Abschnitt mit
Verbindungen an sich und nicht mit der Form oder Kapselung der
Daten.


2.9.1 REST-API


Representational State Transfer (REST) ist, nach einer Vielzahl von
Autoren (Fielding (2000); Richardson und Ruby (2008); W3C (2004)),


ein Architekturstil zum Aufbau eines Web-Services. Es baut auf den
Grundlagen des Webs, insbesondere dem HTTP-Protokoll, auf und
ist optimiert fü r Web-Services.


Die Anfragen an den Server sind zustandslos. Das bedeutet, sie sind
komplett unabhängig von anderen Anfragen. Zusätzlich geben sie
bei mehrmaliger Ausfü hrung stets dieselbe Antwort zurü ck. Das be-
deutet weiter, dass jede Anfrage alle fü r die auf dem Server fü r die
Verarbeitung notwendigen Informationen enthält.


2.9.2 Websocket


Websockets (siehe IETF (2011); websocket.org (o. J.)) bieten eine asyn-
chrone bidirektionale (full duplex) Kommunikation zwischen Client
und Server. Dies funktioniert ü ber einen Kanal, welcher ü ber einen
einzigen Socket läuft. Dies bedeutet eine enorme Reduktion von un-
nötigem Netzwerkverkehr und Latenz im Vergleich zu (Long-)Polling-
Lösungen. Das vor allem, da versucht wird mittels zwei Verbindun-
gen das Verhalten von Websockets zu simulieren. Websockets können
automatisch mit netzwerkspezifischen Umständen, wie beispielswei-
se Proxy-Servern oder Firewalls, umgehen und machen so eine Ver-
bindung praktisch ü berall möglich.


2.9.3 socket.io


Socket.io ist eine WebSocket API, welche die Verwendung von Web-
sockets unterstü tzt (siehe Prusty (2016); Rai (2013); Walsh (2010)). Bei-
spielsweise erkennt socket.io selbstständig, ob WebSocket in der ver-
wendeten Umgebung unterstü tzt werden und sucht gegebenenfalls
Alternativen:


• WebSocket


• Flash Socket


• AJAX long-polling


• AJAX multipart streaming


• IFrame


• JSONP polling


Diese Auswahl funktioniert automatisch im Hintergrund ü ber eine ge-
meinsame API. Der Entwickler kann aber auch explizit angeben, wel-
chen Transport er verwenden will. Ein grosser Vorteil ist auch, dass di-
verse Bibliotheken zur Erweiterung der Funktionalität verfü gbar sind.


2.9.4 Stream


Wie gesehen in (Prusty, 2016, S. 56) ist ein Stream ganz allgemein
formuliert eine Sequenz von Daten, welcher ü ber die Zeit verfü gbar
gemacht wird. Da dies asynchron abläuft, werden EventHandler oder
Callbacks eingesetzt, welche aufgerufen werden, wann immer Daten
verfü gbar sind. Daten können so in Teilstü cken ü bermittelt werden.


2.10 Datentransfer


Nach der Sicherstellung einer Verbindung zwischen zwei Instanzen,
gilt es einen Weg zu finden, in welchem Format die erforderlichen
Daten bestmöglich ü bertragen werden können. Dafü r gibt es verschie-
dene Ansätze:


2.10.1 JSON


JavaScript Object Notation (JSON) ist ein schmales, text-basiertes und
sprachunabhängiges Datenü bertragungsformat. Nach Bray (2014) ist
es ein anerkannter Standard. Abgeleitet ist es von Javascript (ECMA
Script). Eine kleine Menge an Formatierungsregeln ist verantwortlich
fü r die Repräsentation von strukturierten Daten.


2.10.2 MessagePack


MessagePack$$$3$$$ ist ein weiteres Format zur binären Serialisierung von
Daten (ähnich Unterabschnitt 2.10.1). Nach Izzo (2016) ist es flexibler
und in bestimmen Fällen leistungsfähigerer als andere Lösungen wie
JSON oder XML. Dies auch in Fällen der Ü bermittlung ü ber das Netz-
werk, wo es im vorliegenden Projekt auch Anwendung findet.


2.11 Persistenz


Daten, wie der gesamte Dokumentenkorpus und die berechneten In-
dizes, mü ssen persistiert werden. Dafü r wurden folgende Möglich-
keiten in Betracht gezogen. Von grosser Wichtigkeit ist, dass der Be-
nutzer stets volle Transparenz ü ber den Inhalt und auch den Speicher-
ort seiner Daten hat. Zusätzlich sind Datensicherheit, Verschlü sselung
und Integrität essentiell.


3http://msgpack.org


2.11.1 Dropbox


Bis anhin speichert der ikc-core die Daten in einem dafü r vorgesehe-
nen Ordner auf der Dropbox des Benutzers. Dies hat den Grund, das
Dropbox ebenfalls als Datenquelle verwendet wird und bereits eine
ähnliche Anbindung verfü gbar ist. Der Benutzer hat so einfach und
jederzeit Einblick in die Daten seines Wissensnetzwerks.


Die Anbindung ist umgesetzt mit der Javascript Implementation der
Dropbox API V24. Die Autorisierung funktioniert mittels der oauth-
Bibliothek5. Zu beachten ist, dass die API gewisse Limitationen hat6:
Unter anderem ist die Anzahl einzelner Anfragen in einer gewissen
Zeitspanne begrenzt. Dem kann mittels Stapel-Methoden aber entge-
gengewirkt werden.


2.11.2 storj


Cloud-Storage oder allgemein der Begriff Cloud ist allgegenwärtig.
Der Markt ruft nach Cloud-Lösungen. Wie in (Wilkinson, Lowry &
Boshevski, 2014, S. 1-3) dargelegt, steckt hinter dem Begriff Cloud
aber häufig viel Marketing, um eine schon seit langem existierende
Technologie. Werden Daten in der Cloud gespeichert, werden sie vom
Client ü ber TCP/IP zu einem Server eines Datencenters ü bermittelt.
Dies passiert schon seit den Anfängen der Informatik auf dem selben
Weg. Viele dieser Cloud-Services funktionieren ü ber zentrale Server,
dies birgt Sicherheitsrisiken.


Storj strebt ein dezentralisiertes Speichernetzwerk an, welches sich
nicht auf Vertrauen zwischen Client und Host verlässt. Vor jeglicher
Ü bermittlung werden die Daten verschlü sselt. Um dieses Ziel zu errei-
chen und gleichzeitig Skalierbarkeit, Kosteneffizienz und die gewü n-
schte Sicherheit zu erreichen, ist viel technisches Wissen notwendig.


4https://www.dropbox.com/developers/documentation/http/
documentation


5https://www.dropbox.com/developers/reference/oauth-guide
6https://www.dropbox.com/developers/reference/data-ingress-guide


Das Prinzip des verteilten Netzwerks funktioniert, wie (Wilkinson,
Boshevski, Brandoff & Buterin, 2014, S. 2-16) erklärt, folgendermas-
sen: Die Abbildung 2.1 dient der Veranschaulichung. Das Netzwerk
besteht aus zahlreichen Peers. Storj ermöglicht diesen ü ber das Netz-
werk Verträge auszuhandeln, Daten zu ü bertragen und gleichzeitig
höchste Sicherheit, Integrität und Verfü gbarkeit zu garantieren. Jeder
Peer hat die gleichen Möglichkeiten und alle sind gleichwertig. Da-
ten werden nicht als Ganzes, sondern in Einzelteilen verteilt und ver-
schlü sselt ü ber das Netzwerk der Peers gespeichert. Die Verfü gbarkeit
kann durch Redundanz einzelner Peers gewährleistet werden.


2.11.3 SFTP


Ü ber das Protokoll SFTP ist ein sicherer Datentransfer, gleichzeitig
auch ein Systemzugriff, möglich. Die Spezifikation nach (Galbraith,
Saarenmaa, Ylonen & Lehtinen, 2006, S. 3) sagt weiter aus, dass eine
bereits aktive SSH-Verbindung und die entsprechende Authentifizie-
rung vorausgesetzt wird. Grundsätzlich funktioniert es nach einem
simplen Anfrage-Antwort-Modell.


Alle fü r den Prototypen notwendigen Operationen, wie beispielswei-
se den Inhalt eines Verzeichnisses lesen und das Lesen und Schrei-
ben von Dateienm, ist möglich. Weiter bietet das unterliegende SSH-
Protokoll viele weitere Funktionen.


Kapitel 3


Lösungsdesign


Das Lösungsdesign beinhaltet die Grundlagen fü r die erfolgreiche
Umsetzung des Prototyps. Darin wird basierend auf den Anforde-
rungen und der Aufgabenstellung die Software-Architektur definiert.
Während dies auf einer hohen Abstraktionsebene geschieht, erfolgt
in einem nächsten Schritt der Entwurf und das Design der Software.
Dabei dient die Architektur als Leitplanke, welche zusammen mit der
Recherche ü ber den aktuellen Stand der Technik (Kapitel 2) zu einem
konkreten Entwurf fü hrt.


3.1 Architektur


Der Architektur-Entwurf betrachtet die zu entwickelnde Software aus
einer abstrakten Sicht. Das Ziel ist eine grundsätzliche Ü bersicht ü ber
die Software, deren Komponenten, Schnittstellen und auch deren Ver-
teilung.


Basierend auf den Anforderungen (Abschnitt 1.6) an den zu entwi-
ckelnden Prototypen, wird im folgenden Abschnitt der zugrunde-
liegende Architektur-Entwurf ausgefü hrt. Die Architektur-Entschei-
dungen bilden die Grundlage fü r die Designentscheidungen des Soft-
ware-Entwurfs. Ausschlaggebend dafü r ist das aus Kruchten (1995)
stammende 4+1 Schichtenmodell.


3.1.1 Kontextsicht


Die Abbildung 3.1 gibt einen Ü berblick ü ber den Kontext des zu ent-
wickelnden Prototypen: Der ikc-core nutzt den Prototypen zur Er-
weiterung seiner Funktionalität. Damit können die Hauptanforderun-
gen, wie die Volltextsuche und die Extraktion von Schlü sselwörter,


abgedeckt werden. Diese beiden Funktionen sind fü r den Benutzer
ü ber das UserInterface innerhalb des ikc-core verfü gbar. Sowohl der
ikc-core als auch der Prototyp haben Zugriff auf die externen Daten-
quellen. Der Prototyp hat dort die Quellen fü r die Dateiinhalte und
hält dort auch Indizes fü r die Volltextsuche und die Schlü sselwort-Ex-
traktion.


3.1.2 Einflussfaktoren


Basierend auf dem Kontext des ikc-core, dem Scope und den funktio-
nalen und nichtfunktionalen Anforderungen beeinflussen verschiede-
nen Faktoren den Architektur-Entwurf.


Der ikc-core nutzt keine zusätzlichen
zentralen Services fü r die Persistenz von
Benutzerdaten oder deren Applikations-
Konfiguration. Es werden die lokalen
Ressourcen oder externe Datenquellen wie
Dropbox verwendet.


Tabelle 3.1: Einflussfaktoren


3.1.3 Architekturtreiber


Drei elementare Architekturtreiber beeinflussen die Konzeption des
Prototyps. Diese wurden teilweise bereits weiter oben als Einflussfak-
tor (Unterabschnitt 3.1.2) erwähnt, haben jedoch auch fü r den Proto-
typ eine zentrale Bedeutung.


• Entkopplung und Wiederverwendbarkeit: Module und Kom-
ponenten sind entkoppelt voneinander. Dadurch können diese
innerhalb der Applikation, oder auch fü r zukü nftige Projekte,
leicht wiederverwendet werden. Bei Performance-Engpässen ist
es weiter möglich, bestimmte Module auszulagern, dies sowohl
lokal innerhalb der Client-Applikation, als auch extern auf eine
Server-Umgebung.


• Verhinderung von zusätzlicher Persistenz: Applikationsdaten
und Konfiguration sind stets lediglich von den bestehenden Da-
tenquellen zu beziehen. Hierbei handelt es sich entweder um
den lokalen Cache des Benutzers oder die entsprechende exter-
ne Datenquelle. Durch diesen Umstand kann viel Zeit fü r die
Entwicklung und die Absicherung einer Persistenz-Infrastruk-
tur gespart werden. Der Benutzer hat so zusätzlich immer eine
transparente Kontrolle ü ber den Speicherort und auch den In-
halt der eigenen Daten.


• Inspiration durch React und Flux: Wie in Abschnitt 2.7 bereits
erwähnt, arbeitet React im Hintergrund mit einer eigenen Im-
plementation von Flux. Dieser orientiert sich stark am funktio-
nalen Programmier-Paradigma: Innere Zustände, also Variabeln,
sind wann immer möglich zu verhindern. Dies wirkt allfälligen
Seiteneffekten entgegen, macht den Code so nachvollziehbarer.
Auch der unidirektionale Datenfluss strebt ähnliche Ziele an.
Diese Ü berlegungen begleiteten die Entwicklung ständig. So
sind an vielen Orten Programmierkonstrukte zu finden, welche
prinzipiell verwandte Ansätze verfolgen.


3.1.4 Architekturziele


Der zu entwickelnde Prototyp baut auf dem bestehenden ikc-core auf.
Dabei soll das Augenmerk weiterhin auf den bereits bestehenden Ei-
genschaften der Architektur, insbesondere der Modularität und der
Erweiterbarkeit, gehalten werden. Dies ist im Kontext des zugrun-
deliegenden Forschungsprojekts von hoher Wichtigkeit. Die Vergan-
genheit hat gezeigt, dass eine solide aber gleichzeitig auch anpas-
sungsfähige Basis ein kritischer Faktor fü r die agile Weiterentwick-
lung ist. Einzig unter diesen Voraussetzungen ist es möglich, auf die
stetig ändernden Anforderungen entsprechend zu reagieren.


Die wichtigsten Ziele der Architektur sind in folgender Tabelle (Tabel-
le 3.2) kurz erläutert:


Tabelle 3.2: Ziele der Architektur


3.1.5 Bausteinsicht


Die Abbildung 3.2 zeigt die Bausteinsicht der Architektur des Pro-
totypen. Darin werden die verschiedenen Komponenten und Modu-
le und deren Beziehungen untereinander aufgezeigt. Hierbei werden
drei verschiedene Abstraktionslevel unterschieden.


Nachfolgend werden diese näher erläutert.


Bausteinsicht Level 0


Innerhalb des Level 0 werden die Zusammenhänge zwischen dem
User, dem ikc-core und dem Prototypen aufgezeigt. Ebenfalls ist
die Systemgrenze zwischen der bestehenden und der zu erstellenden
Komponente ersichtlich. Die Systemgrenze grenzt den Kern des Pro-
totyps ein.


User


Wie bisher hat der Benutzer ü ber die Benutzeroberfläche des ikc-core
Zugriff auf die gesamte Funktionalität. Die Zusatzfunktionen, welche
neu durch den Prototyp zur Verfü gung gestellt werden, sind fü r den
Benutzer in der Suchfunktion und den extrahierten Schlü sselwörtern
ersichtlich.


ikc-core


Neben der bisherigen Funktionalität nutzt der ikc-core zusätzlich
die neuen Funktionen, welche vom Prototypen zu Verfü gung gestellt
werden. Dazu zählt die Volltextsuche und die Schlü sselwort-Extrak-
tion. Er besteht prinzipiell aus verschiedenen UIElements, welche UI-
Services nutzen, um zugrundeliegende Logik zu kapseln. In dieser
Logik soll der Prototype integriert werden.


Prototype


Die Komponente Prototype kapselt die essentiellen Funktionen: die
Textanalyse und die Anbindung der externen Datenquelle. Darin wer-
den die Module DataService und IndexService unterschieden.


Bausteinsicht Level 1


Mithilfe des zweiten Abstraktionlevels (Level 1) werden die verschie-
denen Module und die Beziehungen innerhalb der Komponente auf-
gezeigt.


UI Elements


Die Interaktion mit dem Benutzer wird durch die Komponente UI-
Elements abgehandelt. Dabei werden alle sichtbaren Teile der Ober-
fläche in verschiedene Elemente gekapselt. Die resultierenden Ele-
mente sollen innerhalb der Applikation beliebig wiederverwendbar
sein (zum Beispiel SearchField). Mit Hilfe des Applikationzustandes
wird der Inhalt der Elemente festgelegt.


UI Services


Die Logik der Applikation wird in verschiedene UIServices aufge-
teilt, diese steuern das Verhalten der Applikation durch die Aktuali-
sierung des jeweiligen Applikationzustandes.


DataService


Das Modul DataService regelt den Zugriff auf externe Datenquellen.
Dank der Abstraktion des spezifischen Zugriffs können verschiedene
Quellen, je nach Bedarf, verwendet werden. Weiter kann der Zugriff
mittels der Freigabe-Token an andere Module weitergegeben werden.


Innerhalb der Komponente der Bachelorarbeit beschränkt man sich
auf die Verwendung von externen Datenquellen ü ber SFTP. Eine Er-
weiterung verschiedener Datenquellen soll jedoch möglich sein.


IndexService


Die Textanalyse wird mit dem Modul IndexService durchgefü hrt.
Die beiden Teilmodule Search und InformationExtraction nutzen
den Index als Basis fü r die Berechnung von Suchresultaten oder die
Extraktion von Schlü sselwörtern.


Bausteinsicht Level 2


Mit Hilfe des letzten Abstraktionslevels (Leve 2) der Bausteinsicht
werden die verschiedenen Teilmodule erläutert.


DataAccess


Eine der Hauptaufgaben des DataService-Moduls, der Zugriff auf
verschiedene externe Datenquellen, wird mittels des Teilmoduls Da-
taSharing abgehandelt. Darin wird der spezifische Zugriff auf die
Quelle beschrieben.


DataSharing


Neben dem Zugriff sollen Elemente von externen Quellen innerhalb
der Applikation via Freigabe-Token zu Verfü gung gestellt werden.
Diese werden innerhalb des DataSharing Teilmoduls gehalten. Nach
einmaliger Verwendung sollen diese verfallen.


Datasource


Ü ber das externe Modul DataSource findet der Zugriff auf externe
Datenquellen statt. Einerseits können hier Daten im Volltext bezogen
werden, andererseits können hier Konfigurationsdaten oder Indizes
gespeichert werden.


Index


Innerhalb des Teilmoduls Index wird der Textkorpus zusammen mit
allen wichtigen Informationen gehalten. Dazu gehört insbesondere
der Volltext-Index fü r die Suche, als auch Informationen zu der An-
zahl der Vorkommnisse potentieller Schlü sselwörter innerhalb des
Korpus. Er bildet somit die Grundlage fü r die Volltextsuche und die
Schlü sselwortextraktion.


Search


Das Teil-Modul Search handelt Suchanfragen ab. Die Resultate wer-
den basierend auf dem Volltext Index generiert.


InformationExtractor


Die Extraktion von Schlü sselwörtern wird durch das Teilmodul In-
formationExtractor abgehandelt. Basierend auf dem Index berech-
net er eine Auswahl aus den potentiellen Kandidaten.


3.1.6 Ablaufsicht


Die Abbildung 3.3 gewährt einen Ü berblick ü ber den Gesamtablauf
vom Beginn der Nutzung des ikc-core bis hin zur Betrachtung der
Suchresultate oder der extrahierten Schlü sselwörter.


Der Benutzer ist Ursprung der Abläufe: Er nutzt den ikc-core und
hat darin eine externe Datenquelle (beispielsweise SFTP) hinterlegt.
Sind diese Voraussetzungen erfü llt, holt sich der ikc-core beim Data-
Service die Freigabe fü r die benötigten Daten (shareData). Benötigte
Daten können die berechneten Indizes oder den Volltext der Dateien
beinhalten. Der DataService antwortet und gibt damit die Freigabe
zurü ck.


Die erhaltene Freigabe gibt der ikc-core im Prozess initIndex an den
IndexService weiter. Hat dieser den angeforderten Index bereits gela-
den, gibt er diesen an den ikc-core zurü ck. Ist dies nicht der Fall, holt
er sich beim DataService wiederum die benötigten Daten (loadData).
Diese können denselben Inhalt wie oben haben. Das ist abhängig da-
von, ob der Index bereits erstellt wurde. Ist dies nicht der Fall, muss
dieser auf Basis aller Dateien erstellt werden. Ist er bereits erstellt,
oder spätestens nach dessen Erstellung, wird dies an den ikc-core


gemeldet.


Nun kann der ikc-core die Funktionalität des IndexService nutzen:
Er hat Zugriff auf die Volltextsuche (search) und die extrahierten
Schlü sselwörter (extractKeywords).


3.1.7 Verteilung


Wie oben schon angesprochen, findet die Entwicklung sowohl client-
als auch serverseitig statt. Abbildung 3.4 gibt einen detaillierten Ü ber-
blick. Auf der Seite des Clients läuft der ikc-core im Browser. Dieser
verwendet fü r die Zwischenspeicherung von Daten eine in-browser
Datenbank.


3.2 Algorithmus


Der Kern des Prototyps bilden die verschiedenen Algorithmen. Ba-
sierend auf den externen Datenquellen ermöglichen sie eine Volltext-
Suche und die Extraktion der relevanten Keyphrases. Nachfolgend
werden die Funktionsweise als auch wichtigsten Ü berlegungen dazu
veranschaulicht. Basierend auf verschiedenen bestehenden Ansätzen
wird die Lösungsfindung fü r den hier verwendeten Algorithmus dar-
gelegt.


Um die relevanten Keyphrases fü r ein spezifisches Dokument zu ex-
trahieren, werden verschiedene Methoden aus dem Feld von Natu-
ral Language Processing kombiniert mit statistischen Analsysen und
heuristischen Vorgaben. Fü r die Analyse der Keyphrases werden N-


Gramme der Grösse eins bis vier berü cksichtigt. Nebst möglichst tref-
fenden Resultaten liegt der Fokus auch auf der raschen Aussortie-
rung von ungeeigneten Kandidaten. Da die Berechnung der Relevanz
der Rechen- und Speicher-intensivste Vorgang des Algorithmus ist,
kann die Gesamtleistung des Algorithmus durch frü he Aussschei-
dung unnötiger Kandidaten enorm gesteigert werden. Die Anzahl
möglicher N-Gramme ist definiert als:


Wobei N die maximale Länge eines N-Gramms und x die Länge des
Textes repräsentieren.


In der Abbildung 3.5 werden dafü r der konzeptionelle Ablauf gra-
phisch dargestellt und anschliessend die verschiedenen Elemente ge-
nauer erläutert.


3.2.1 Eingabe


Als Eingabe in den Algorithmus wird ein Text erwartet. Im Fall die-
ses Prototyps handelt es sich um einen Wikipedia-Artikel aus den
Trainingsdaten. Um die folgenden Schritte besser zu illustrieren, wird
der folgende Satz als Beispieltext verwendete. Mit einer Länge von 19
Wörter, besitzt er 70 mögliche N-Gramme mit Länge eins bis vier.


The Guardian newspaper was founded in 1821 as The Manche- ”
ster Guardian“, which head office is still located in Manchester.


Mit einer Länge von 19 Wörtern besitzt er 70 mögliche N-Gramme
mit einer Länge von eins bis vier.


3.2.2 Vorverarbeitung (1)


Im ersten Schritt wird der Text anhand verschiedener Vorgaben vorbe-
reitet. Dazu werden zu Beginn die Buchstaben nach einem Punkt, ei-
nem Fragezeichen, Ausrufezeichen oder einer Zeilenschaltung in die
Kleinschreibung umgewandelt. Dies dient dazu Wörter zu erkennen,
welche als Nomen genutzt werden, jedoch nicht aus dieser Wortart
stammen. Dadurch verändert sich der Beispielsatz geringfü gig.


the Guardian newspaper was founded in 1821 as The Manche- ”
ster Guardian“, which head office is still located in Manchester.


Weiter wird nun der Text in Fragmente aufgeteilt. Dabei wird davon
ausgegangen, dass sich relevante Keyphrases nicht ü ber ein Satzzei-
chen erstrecken. Es entsteht eine Abfolge von Textfragmenten.


(the Guardian newspaper was founded in 1821 as) (The Manche-
ster Guardian) (which head office is still located in Manchester)


Nach dem ersten Schritt wurde der Text mit 19 + 18 + 17 + 16 = 70
möglichen N-Grammen in eine Liste von Textfragmenten mit (8 + 7 +
FOOTNOTE:6 + 5) + (3 + 2 + 1) + (8 + 7 + 6 + 5) = 58 möglichen N-Grammen
transformiert. Dadurch konnten bereits 12 Kandidaten ausgeschlos-
sen werden.


3.2.3 Text Zerlegung (2)


Vor der Bildung der eigentlichen Kandidaten werden nun die Text-
fragmente durch Tokenization in eine Liste von einzelnen Wörter auf-
geteilt. Ebenfalls werden dabei unerwü nschte Sonderzeichen am An-
fang oder am Ende eines Wortes entfernt. Dabei handelt es sich in
erster Linie um alle anderen Satzzeichen, welche bei der Aufteilung
des Textes in Text-Fragmente nicht berü cksichtigt wurden.


[the, Guardian, newspaper, was, founded, in, 1821, as], [The,
Manchester, Guardian], [which, head, office, is, still, located, in,
Manchester]


3.2.4 Generierung möglicher Keyphrases (3)


Anhand der generierten Wortlisten können nun die verschiedenen N-
Gramme generiert werden. Es entsteht eine Liste von N-Grammen der
Länge von eins bis vier.


[the Guardian newspaper was, the Guardian newspaper, the Gu-
ardian, the, ... , located in Manchester, located in, located, in
Manchester, in, Manchester]


3.2.5 Filter mittels POS-Tagger (4)


Wie von Parameswaran et al. (2010) beschrieben, folgen relevante
Keyphrases bestimmten grammatikalischen Regeln. Diese sind wie
folgt definiert:


1. Eine relevante Keyphrase besteht mindestens aus einem Nomen.
Somit entfallen Keyphrases wie zum Beispiel was oder located in.


2. Eine relevante Keyphrase beginnt nicht mit einem Pronomen,
einem Verb oder einem Partikel. Diese Regel sortiert Keyphrases
wie the Guardian newspaper aus.


3. Eine relevante Keyphrase endet nicht mit einem Pronomen, Verb
oder Partikel. Diese Regel sortiert zusätztlich Keyphrases wie
Guardian newspaper was aus.


Allerdings gibt es auch Ausnahmen: Es existieren Keyphrases, wel-
che kein eigentliches Nomen enthalten. In der englischen Sprache
gibt es beispielsweise zahlreiche Eigennamen, welche ebenfalls po-
tentielle Keyphrases darstellen können. Diese beginnen oft mit einem
Grossbuchstaben. Also kann weiter folgende Regel definiert werden:


4. Eine relevante Keyphrase beginnt mit einem Grossbuchstaben.
Wie bereits erwähnt, wurden vorgängig Wörter innerhalb des
Textes nach bestimmten Satzzeichen in die Kleinschreibung um-
gewandelt. Dieser Vorgang schliesst diese Wörter von dieser Re-
gel aus. Dadurch wird zum Beispiel The Manchester Guardian
weiter beachtet, auch wenn es grundsätzlich der zweiten Regel
widerspricht, da diese mit einem Pronomen startet.


Um diese Regeln umszusetzen, wird ein Part-Of-Speech-Tagger ver-
wendet. Dabei handelt es sich um einen Algorithmus, welcher die
Wortarten von gegebenen Wörtern bestimmt. Um den Regeln zu ent-
sprechen, muss eine Keyphrase sowohl Regeln eins und drei folgend,
zusätzlich entweder der Regel zwei, Regel vier oder beiden folgen.
Mittels dieser drei Regeln können alle möglichen Kandidaten, ausser
den folgenden ignoriert werden:


[Guardian newspaper, Guardian, newspaper, 1821, The Man-
chester Guardian, The Manchester, The, Manchester Guardian,
Manchester, Guardian, head office, head, office, Manchester]


Damit bleiben noch 14 mögliche Keyphrases von ursprü nglich 70.
Nun werden gleiche Keyphrases zusammengefasst, indem sie zusam-
men mit der jeweiligen Anzahl Vorkommnisse kombiniert werden.
Das reduziert die Anzahl unterschiedlicher Keyphrases auf 12 (Tabel-
le 3.3). Ebenfalls wird jeweils das erste Wort einer Keyphrase kleinge-
schrieben, falls es sich nicht um ein Nomen handelt. So kann sicher-
gestellt werden, dass zum Beispiel das 1-Gramm The“ im nächsten ”
Schritt tiefer bewertet wird. Da die Grossschreibung nur im Kontext
der Keyphrase The Manchester Guardian von Bedeutung ist.


Tabelle 3.3: Keyphrase mit Anzahl Vorkommnissen


3.2.6 Berechnung der Relevanz (5)


Um die verschiedenen Keyphrases zu gewichten, wird ein individuel-
ler Score pro Keyphrase ausgerechnent. Dazu werden folgende Metri-
ken benötigt (Tabelle 3.4).


Tabelle 3.4: Benötigte Metriken


Basierend auf den eingefü hrten Metriken wird mit Hilfe einer ange-
passten Version der Similarity-Formel1 des Apache Lucene Projekts ge-
arbeitet. Die verwendet Formel fü r die Berechnung des Scores sieht
folgendermassen aus:


Gleichung 3.1 repräsentiert die Häufigkeit der Keyphrase innerhalb
des gegebenen Dokuments. Wenn sich Anzahl erhöht, steigt auch der
tf -Wert. Um den Einfluss diese Faktors zu reduzieren, wird der Wert
mittels der Wurzelfunktion etwas gedämpft.


Neben der Häufigkeit innerhalb des Dokuments ist die inverse Häu-
figkeit (Gleichung 3.2) ein wichtiger Wert. Diese basiert auf den Vor-
kommnissen einer Keyphrase ü ber den gesamten Korpus. Dadurch
wird die Einzigartigkeit der Keyphrase innerhalb des Korpus ausge-
drü ckt. Dieser Wert sinkt, je öfter die gleiche Keyphrase in anderen
Dokumenten enthalten ist. Mittels der Addition von eins im Nenner
innerhalb der Logarithmus-Funktion wird eine Division durch Null
verhindert.


Wü rde der Score lediglich mit den beiden Faktoren Gleichung 3.1 und
Gleichung 3.2 berechnet, so wären die berechneten Scores aus Doku-
menten mit unterschiedlicher Textlänge nicht vergleichbar. Auch die
Verwendung eines Schwellenwertes fü r die Begrenzung der Anzahl
Keyphrases wäre verunmöglicht. Eine Normalisierung ü ber die Do-
kumentenlänge ist somit notwendig. Dazu wird die Gleichung 3.3
eingesetzt.


3.2.7 Auswahl (6)


Schlussendlich werden die berechneten Keyphrases mit Hilfe eines
Schwelwerts und anhand deren Inhalt begrenzt. Damit können eine
sinnvolle Anzahl an Dokumenten zurü ckgegeben. Im Gegensatz zu
einer Begrenzung der Anzahl Keyphrases kann mit einem Schwell-
wert je nach Text eine Anzahl an Keyphrases zurü ckgegeben werden,


1TFIDFSimilarity (Lucene 4.5.0 API) (o. J.)


welche fü r den Text ideal ist. Beispielsweise macht es wenig Sinn, dass
aus einem kurzen Text gleich viele Keyphrases extrahiert werden wie
aus einem langem. Denn aufgrund der unterschiedlichen Länge ent-
halten die beiden Texte prinzipiell eher nicht gleich viel relevanten
Inhalt. Weiter werden Keyphrases gefiltert welche in anderen Key-
phrases enthalten sind. So werden zum Beispiel head und office ge-
filtert da beide im Keyphrase head office enthalten sind.


3.2.8 Dokumente für Schlüsselwort


Die gegenteilige Operation der Keyphrase-Extraktion ist die Extrakti-
on aller Dokumente fü r eine bestimmte Keyphrase. Dabei wird eine
Liste an Dokumenten erwartet, in welchen eine bestimmte Keyphrase
einen hohen Score ausweist.


Abbildung 3.6: Ablauf: relevante Dokumente fü r eine Keyphrase


1. Dazu werden in einem ersten Schritt alle möglichen Dokumen-
te ausgewählt. In dieser Menge an Dokumenten kommt die ent-
sprechende Keyphrase mindestens einmal vor, jedoch ungeach-
tet der Relevanz der Keyphrases fü r das jeweilige Dokument.


2. In einem nächsten Schritt wird fü r jedes Dokument der jeweili-
gen Score fü r die entsprechende Keyphrase berechnet. Die Be-
rechnung folgt dabei exakt dem bereits vorgestellten Ablauf
(Unterabschnitt 3.2.6).


3. Nun wird ebenfalls die Anzahl an Resultaten mit Hilfe eines
Schwellwerts reduziert. Somit kann die Auswahl auf die wirk-
lich relevanten Dokumente fü r die entsprechende Keyphrase
vermindert werden.


3.3 Integration des Prototypen


Basierend auf der ausgefü hrten Architektur (Abschnitt 3.1) werden
im folgenden Abschnitt die wichtigsten Software-Konzepte erläutert.
Diese werden kurz zusammengefasst, anschliessend folgt eine Be-
schreibung der Integration des Prototyps in den bestehenden ikc-
core.


3.3.1 Ü bersicht


Im Klassendiagramm auf Abbildung 3.7 sieht man einen Ü berblick
ü ber den zu entwickelnden Prototypen. Abgebildet sind die wich-
tigsten Klassen und deren Beziehungen untereinander. Zunächst lie-
gen die zwei Teile ikc-core und Prototype vor. Der ikc-core ist der
aus dem Forschungsprojekt IKC herausgehende bestehende Proto-
typ. Dieser nimmt Gebrauch von den beiden vom Prototypen zur
Verfü gung gestellten Schnittstellen des Index- und des DataServices.


Grundsätzlich besteht der Prototyp aus den Komponenten Index-
und DataService. Diese verwenden das von ausserhalb verfü gbare
MessageModel. Es enthält die Protokolle fü r die jeweilige Kommuni-
kation und wird somit auch vom ikc-core in Anspruch genommen.


Die genauen Abläufe und auch die Implementation des Index- bezie-
hungsweise DataServices ist unter dem Abschnitt der technischen
Umsetzung (Kapitel 4) zu finden.


Der Kern der Software bildet der Algorithmus. Dieser ist, neben der
Suchfunktionalität und dem Aufbau der Indizes, ein Hauptbestand-
teil des IndexService. Abbildung 4.12 gewährt einen Ü berblick ü ber
die beteiligten Komponenten. Als Grundlage benötigt der IndexSer-
vice alle zu indexierenden Dateien im Volltext. Deren Quelle ist der
DataService. Der Index- und der DataService bilden zusammen den
eigentlichen Prototypen. Wie in der Bausteinsicht auf Abbildung 3.2
zu erkennen, gibt es neben der Integration der Services auch eine
Einbindung in die bestehende Benutzeroberfläche des ikc-core.


3.3.2 Schnittstellen


Der Prototyp soll sich möglichst nahtlos in den ikc-core integrieren.
Um dies zu erreichen sollen in keiner Situation Funktionen des ikc-
cores blockiert werden durch den Prototyp.


So werden Suchresultate des Index innerhalb der bestehenden Suche
integriert und bei Bedarf aktualisiert. Die verschiedenen Resultate der
unterschiedlichen Quellen sollen in Echtzeit nach ihrem Eintreffen
dargestellt werden. Somit wird sich die Liste mit Resultaten trotz glei-
chem Suchbegriff ü ber die Zeit verändern, da weitere Resultate von
entfernten Quellen eintreffen. Bei der Auswahl eines Resultats wird
es herkömmlicher Node dargestellt.


Weiter werden extrahierte Keyphrases klar getrennt von den bestehen-
den Eigenschaften des Nodes als Chips oberhalb des Titels dargestellt
werden. Sowohl ein Dokument mit entsprechenden Keyphrases als
auch eine Keyphrase mit den verknü pften Dokumenten werden als
Node dargestellt. Abbildung 3.8 zeigt einen Entwurf dieser Integrati-
on.


Um die Schnittstellen des Prototyps ideal zu verwenden und die
obigen Oberflächenanpassungen umzusetzen, sind Anpassungen be-
ziehungsweise Erweiterungen in der Software-Struktur des ikc-core
nötig.


Auf obigem Klassendiagramm (Abbildung 3.7) ist ersichtlich, dass
sowohl der IndexService, als auch der DataService externe Schnitt-
stellen bereithalten.


3.4 Datenfreigabe


Fü r den Auftraggeber ist eine sichere Kommunikation, stetige Trans-
parenz und Kontrolle ü ber den Verbleib von benutzergenerierten Da-


ten von hoher Wichtigkeit. Um diesen Anforderungen gerecht zu
werden, wurde unter anderem ein Datenfreigabe-Konzept entwickelt.
Dieses basiert auf Einweg-Tokens, welche als Schlü ssel fü r die Freiga-
be verwendet werden. Dabei fordert einen Accessor beim Provider den
Zugriff auf eine Ressource an. Der Provider hält dabei die Informatio-
nen fü r den Zugriff auf die Ressourcen und der DataService regelt
den tasächlichen Zugriff auf die Ressource. In Abbildung 3.9 ist der
Ablauf genauer aufgezeigt:


• Als erster Schritt fordert der Accessor beim Provider Zugriff auf
eine Ressource.


• Der Provider sendet nun die nötigen Informationen fü r die Ver-
bindung auf die Ressource an den DataService und fordert einen
Token.


• Sobald der Token generiert wurde, erhält der Provider dieser
und sendet ihn an den Accessor.


• Anschliessend kann der Accessor mithilfe des Token beim Data-
Service den Inhalt der Ressource anfragen.


• Beim Erhalt eines Token ü berprü ft nun den DataService den To-
ken auf deren Gü ltigkeit ü berprü ft. Ist der Token gü ltig, werden
die Daten anhand der Informationen zur Verbindungen von der
externen Datenquelle bezogen und an den Accessor geliefert. An-
schliessend wird der Token gelöscht, so dass er nicht wiederholt
verwendet werden kann.


