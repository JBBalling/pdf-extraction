TECHNISCHE UNIVERSITÄT KAISERSLAUTERN


DOKTORARBEIT


Datentransformationen in
NoSQL-Datenbanken


Vom Fachbereich Informatik
der Technischen Universität Kaiserslautern
zur Verleihung des akademischen Grades
Doktor der Ingenieurswissenschaften (Dr.-Ing.)
genehmigte Dissertation


von


M.Sc. Johannes Schildgen


Dekan des Fachbereichs Informatik:


Prof. Dr.-Ing. Stefan Deßloch


Prüfungskommison:


Prüfungskommison:
Vorsitzender: Prof. Dr.-Ing. Reinhard Gotzhein
Berichterstatter: Prof. Dr.-Ing. Stefan Deßloch
Prof. Dr.-Ing. Wolfgang Lehner


Datum der wissenschaftlichen Aussprache:
25. September 2017


D 386


Zusammenfassung


NoSQL-Datenbanken werden als Alternative zu klassischen relationalen
Datenbanksystemen eingesetzt, um die Herausforderungen zu meistern,
die „Big Data“ mit sich bringt. Big Data wird über die drei V definiert: Es
sind große Datenmengen („Volume“), die schnell anwachsen („Velocity“)
und heterogene Strukturen haben („Variety“). NoSQL-Datenbanken besit-
zen zudem meist nur sehr einfache Anfragemethoden. Um auch komplexe
Datenanalysen durchzuführen, kommen meist Datenverarbeitungsframe-
works wie MapReduce, Spark oder Flink zum Einsatz. Diese sind jedoch
schwieriger in der Benutzung als SQL oder andere Anfragesprachen.


schwieriger in der Benutzung als SQL oder andere Anfragesprachen.
In dieser Arbeit wird die Datentransformationssprache NotaQL vorge-
stellt. Die Sprache verfolgt drei Ziele. Erstens ist sie mächtig, einfach zu er-
lernen und ermöglicht komplexe Transformationen in wenigen Code-Zei-
len. Zweitens ist die Sprache unabhängig von einem speziellen Datenbank-
managementsystem oder einem Datenmodell. Daten können von einem
System in ein anderes transformiert und Datenmodelle dementsprechend
ineinander überführt werden. Drittens ist es möglich, NotaQL-Skripte auf
verschiedene Arten auszuführen, sei es mittels eines Datenverarbeitsungs-
frameworks oder über die Abbildung in eine andere Sprache. Typische Da-
tentransformationen werden periodisch ausgeführt, um bei sich ändern-
den Basisdaten die Ergebnisse aktuell zu halten. Für solche Transformatio-
nen werden in dieser Arbeit verschiedene inkrementellen Ansätze mitein-
ander verglichen, die es möglich machen, dass NotaQL-Transformationen
die vorherigen Ergebnisse wiederbenutzen und Änderungen seit der letz-
ten Berechnung darauf anwenden können. Die NotaQL-Plattform unter-
stützt verschiedene inkrementelle und nicht-inkrementelle Ausführungsar-
ten und beinhaltet eine intelligente Advisor-Komponente, um Transforma-
tionen stets auf die bestmögliche Art auszuführen. Die vorgestellte Sprache
ist optimiert für die gebräuchlichen NoSQL-Datenbanken, also Key-Value-
Stores, Wide-Column-Stores, Dokumenten- und Graph-Datenbanken. Das
mächtige und erweiterbare Datenmodell der Sprache erlaubt die Nutzung
von Arrays, verschachtelten Objekten und Beziehungen zwischen Objek-
ten. Darüber hinaus kann NotaQL aber nicht nur auf NoSQL-Datenbanken,
sondern auch auf relationalen Datenbanken, Dateiformaten, Diensten und
Datenströmen eingesetzt werden. Stößt ein Benutzer an das Limit, sind
Kopplungen zu Programmiersprachen und existierenden Anwendungen
mittels der Entwicklung benutzerdefinierter Funktionen und Engines mög-
lich. Die Anwendungsmöglichkeiten von NotaQL sind Datentransforma-
tionen jeglicher Art, von Big-Data-Analysen und Polyglot-Persistence-An-
wendungen bis hin zu Datenmigrationen und -integrationen.


Abtract


NoSQL databases are a new family of databases systems that do not ne-
cessarily follow the relational model, they have simple access methods,
and they are made to handle Big Data. Characterized with the 3 Vs, Big
Data is typically defined in terms of Volume, Variety, and Velocity. Data-
processing frameworks like MapReduce, Spark, or Flink are often used to
perform complex data analyses due to the simple access methods of NoSQL
databases. These systems are powerfull but more difficult to use than query
languages such as SQL.


In this thesis, a data-transformation language called NotaQL is presen-
ted. This language follows three objectives. Firstly, it is easy to learn, concise
and powerful. Secondly, it is independent of the actual underlying databa-
se system and its data model. Data from different systems can be read and
written to perform a cross-system transformation. Thirdly, NotaQL trans-
formations can be executed in various ways. There are approaches that
make use of MapReduce, Spark, or other tools and languages. As typical
data transformations are executed periodically to bring the transformation
results up to date, this thesis shows and evaluates different approaches for
incremental transformations by reusing a previous result and analyzing on-
ly the changes since the former computation to produce the new result. Dif-
ferent incremental and non-incremental approaches are supported by the
NotaQL transformation platform. An intelligent advisor component can be
used to always execute a transformation in the best possible way.


used to always execute a transformation in the best possible way.
NotaQL supports all typical forms of NoSQL databases: key-value sto-
res, wide-column stores, document stores and graph databases. Its power-
ful and extensible data model allows for handling arrays, complex objects,
and relationships between objects. However, NotaQL is not limited to No-
SQL databases; it can also be used to analyze and transform data from rela-
tional databases, files, services, and data streams. When a user reaches the
limits of NotaQL, user-defined functions and engines can be developed to
extend the language and connect transformations to other systems.


The fields of application of the presented language are data transforma-
tions of any kind: from Big-Data analytics and polyglot-persistence appli-
cations to data migrations and information integration.


Danksagung


Ich möchte einen besonderen Dank meinem Betreuer und Doktorvater Prof.
Stefan Deßloch aussprechen. Er stand von Anfang an voll und ganz hin-
ter meiner Forschung und zeigte mir stets mit wertvollen Ratschlägen den
richtigen Weg. Des Weiteren bin ich ihm dankbar dafür, dass ich neben mei-
ner Forschung besonders viel Lehrerfahrung sammeln durfte. Auch den
beiden anderen Professoren des Lehrgebiets Informationssysteme, Prof. Theo
Härder und Prof. Sebastian Michel danke ich dafür, dass sie mir ihr Wissen
an Datenbanken weitergegeben haben und ich mich auch in deren Arbeits-
gruppen in die Lehre mit einbringen durfte.


gruppen in die Lehre mit einbringen durfte.
Ein großer Dank geht an zwei weitere Professoren, und zwar zum einen
Prof. Reinhard Gotzhein, der sich bereiterklärt hat, den Vorsitz des Promoti-
onskommittees zu übernehmen und an Prof. Wolfgang Lehner, den zweiten
Gutachter dieser Doktorarbeit.


Ohne meine Kollegen am Lehrstuhl wäre meine Arbeit nur halb so an-
genehm gewesen. Daher bedanke ich mich für unterhaltsame Mittagspau-
sen in der Mensa und aufregende Partien am Kickertisch bei Caetano Sauer,
Daniel Schall, Evica Milchevski, Heike Neu, Kiril Panev, Koninika Pal, Ma-
nuel Hoffmann, Steffen Reithermann, Weiping Qu und Yong Hu. Auch an
die wissenschaftlichen Mitarbeiter des Lehrgebiets zu meiner Studienzeit
geht ein großer Dank, insbesondere Karsten Schmidt, Sebastian Bächle und
Thomas Jörg. Ihr habt mir vorgemacht, wie man erfolgreich promoviert,
lehrt, forscht und betreut.


In meinen viereinhalb Jahren in der Arbeitsgruppe Heterogene Infor-
mationssysteme hatte ich das große Glück, ständig von hervorragenden
Studenten umgeben zu sein. Die Betreuung deren Bachelor- und Masterar-
beiten war nicht nur eine große Freunde für mich, sondern jede der Arbei-
ten leistete auch einen großen Beitrag für diese Doktorarbeit. Danke Fabian
Süß, Florian Haubold, Jan Adamczyk, Jonathan Priebe, Manuel Hoffmann,
Marc Schäfer, Michael Emde, Nico Schäfer, Peter Brucker, Philipp Thau,
Sougata Bhattacharjee, Stefan Braun, Thomas Lottermann, Tobias Hamann,
Voichita Droanca und Yannick Krück!


Zu guter Letzt möchte ich meiner Familie und ganz besonders Inna dan-
ken – dafür, dass ich euch habe und dass ihr für mich da seid.


Einle1itung


Der Begriff des Daten-Managements hat sich im Laufe des letzten Jahrzehnts
und mit dem Aufkommen von Web-Diensten wie sozialen Netzwerken und
Online-Shops drastisch verändert. Während in klassischen betrieblichen In-
formationssystemen in der Regel relationale Datenbankmanagementsyste-
me eingesetzt wurden, um strukturierte Daten zu speichern, finden heutzu-
tage vermehrt NoSQL-Datenbanksysteme [Edl+10] Verwendung. Diese Syste-
me bieten nicht nur Möglichkeiten, um große Datenmengen verteilt zu spei-
chern, sondern bringen auch eine gewisse Schema-Flexibilität mit sich. Eben-
diese flexiblen Schemata erlauben die Speicherung heterogener Daten. We-
der muss zu Beginn ein Datenschema definiert werden, noch müssen die
zu speichernden Datensätze zueinander homogen sein.


In relationalen Datenbanken bietet die Structured Query Language (SQL)
Möglichkeiten zur Definition von Tabellen samt ihrer Spalten und deren
Datentypen. In diesen Tabellen herrscht horizontale und vertikale Homogeni-
tät [Wie15, S. 35]. Horizontal bedeutet hier, dass jede Zeile einer Tabelle
die gleichen Spalten besitzt. Nullwerte machen es zwar möglich, einzelne
Spalten wertfrei zu lassen, belegen jedoch meistens trotzdem Speicher und
müssen organisiert werden. Für die einzelnen Spalten einer Tabelle bedeu-
tet die vertikale Homogenität, dass sich in einer Spalte über die komplette
Tabelle hinweg Daten ein und des selben Typs befinden.


Die Schema-Flexibilität und die teilweise sehr mächtigen Datenmodel-
le von NoSQL-Datenbanken machen es möglich, dass Anwendungen ohne
vorherige Datendefinitionsphasen beliebig strukturierte Objekte in die Da-
tenbank legen und diese mittels einfachen Zugriffsmethoden lesen können.
Da Datensätze genau so gespeichert werden können, wie die Anwendung
sie benötigt - also zum Beispiel ein Blogbeitrag-Objekt mit einem Listenat-
tribut, welches alle Kommentare zu diesem Beitrag und deren Verfasser
enthält -, sind die in verteilten Datenbanken sehr kostspieligen Verbun-
doperationen nicht mehr vonnöten. In NoSQL-Datenbanken findet man
dementsprechend nicht nur spezielle Datenmodelle, sondern auch neuar-
tige und meist sehr schlichte Anfragemechanismen.


tigeEusnzdeimgtesiiscths,edharssscdhileicHhteeteArnogfreangietämt,edchieainnisdmerenD.atenbankforschung be-
reits seit langer Zeit eine große Herausforderung bei der Datenintegration


und -migration darstellt [LN07], im Zeitalter von NoSQL auf neue Proble-
me stößt. Dazu zählt auch der Trend zur polyglotten Persistenz [SF12]. So-
genannte Polyglot-Persistence-Anwendungen greifen nicht auf eine einzi-
ge Datenbank zu, sondern verbinden sich mit verschiedenen Datenbanken,
Diensten und externen Anwendungen, um Daten zu lesen, zu analysieren
und miteinander zu verbinden. Der Grund dafür ist, dass es keine Einheits-
lösung gibt, sondern jedes System gewisse Vor- und Nachteile hat. Daher
werden Plattformen, Sprachen und Algorithmen zur Auflösung der in die-
sen Anwendungen vorliegenden Heterogenität benötigt. In dieser Arbeit
stellen wir als Lösungsansatz für diese neuen Herausforderungen bei der
Datentransformation die Sprache NotaQL sowie eine dazugehörige Platt-
form vor.


1.1 Big Data


Bei der Diskussion um die Definition des Begriffes Big Data startete man
mit den sogenannten drei Vs: „Volume, Velocity, Variety“ [Lan01]. Diese Be-
griffe verdeutlichen den weiter oben beschriebenen Trend, dass Datenmen-
gen größer werden, rasant anwachsen und heterogene Strukturen vorwei-
sen. Im Laufe der Zeit kamen zu der Big-Data-Definition weitere Vs hin-
zu, darunter ist eines die „Veracity“ [Zik+12]. Während die ersten drei Be-
griffe die Eigenschaften der Daten aus technischer Sicht betonen, bedeutet
die Veraciy (Richtigkeit), dass wir nicht mehr zwangsweise von einer wah-
ren Welt sprechen können. In Systemen, in denen vom Benutzer generierte
Daten gespeichert und verbreitet werden, kommen falsche Informationen
ebenso vor wie Widersprüche, fehlerhafte Übersetzungen sowie Nutzer-
Reaktionen auf fälschlich interpretierte Nachrichten.


Die Herausforderungen bei der Analyse von Big Data lassen sich aus den
soeben dargestellten Vs ableiten: Da wir es mit großen Datenmengen zu tun
haben, müssen Systeme in der Lage sein, diese enormen Mengen abspei-
chern und sie schnell und effizient analysieren zu können. Während sich
reine Platzprobleme leicht mit Aufrüstungen und dem Zuschalten weiterer
Festplatten und Solid-State-Drives lösen lassen, ist eine solche Aufrüstung
- das sogenannte Scale-up - auf Prozessor- und Arbeitsspeicherebene be-
grenzt und zudem meist sehr kostspielig. Stattdessen ist es mittels Scale-out
günstiger möglich, sowohl Speicherplatz als auch Rechenleistung zu erhö-
hen, indem weitere Rechner in ein Rechencluster eingefügt werden. Ver-
teilte Datenbanken und verteilte Dateisysteme setzen dabei meist auf Hash-
Partitionierung oder Bereichspartitionierung [DG92], um Daten basierend auf
ihren Identifikatoren oder Attributswerten auf den jeweiligen Rechnern zu
verteilen.


1.2 Motivation


Die Zeiten, in denen Anwendungen „nur SQL“ mit einer Datenbank spra-
chen, sind vorbei. Der Begriff NoSQL bedeutet so viel wie „Mehr als nur
SQL“ (Not only SQL) und findet in immer mehr Unternehmen und An-
wendungen Bestätigung. Big Data und agile Softwaremethoden sind nur
zwei der Gründe dafür, dass vermehrt nicht-relationale Datenbanksyste-
me zum Einsatz kommen. Aber auch wegen der Einsetzbarkeit im Cloud-
Computing und nicht zuletzt aus Gründen der Kostenersparnis entschei-
den sich immer mehr Unternehmen für NoSQL. Anders als bei relationalen
Datenbanksystemen gibt es bei NoSQL-Datenbanken keine einheitlichen
Datenmodelle und Anfragesprachen. Manche Systeme haben ein extrem
schlichtes Datenmodell, sind aber dafür besonders skalierbar und schnell.
Andere Systeme erlauben eine komplexe Datenmodellierung als Graph-
Struktur und bieten spezielle Sprachen zur Suche und zum Traviersieren
über Knoten und Kanten. Weitere Systeme unterstützen und spezialisieren
sich auf Textdaten, Benutzerkommentare, Bilder, Videos, Geo-Daten, Da-
tenströme oder soziale Netzwerke. Meist gibt es jedoch für eine Anwen-
dung keine Einheitslösung, sondern es erfordert die Kombination verschie-
dener Systeme. Die vorliegende Arbeit beschäftigt sich mit ebensolchen
Polyglot-Persistence-Anwendungen und beschreibt Ansätze, um Big-Data-
Analysen durchzuführen, Daten zwischen den verschiedenen Systemen zu
transferieren, Systeme durch andere zu ersetzen, Daten zu migrieren und
sie zu modifizieren.


1.3 Wissenschaftlicher Beitrag


In dieser Arbeit werden NoSQL-Datenbanksysteme analysiert und neuar-
tige Probleme aufgezeigt, die diese Systeme mit sich bringen. In der Litera-
tur existieren Ansätze, die sich mit der Überwindung von Heterogenität be-
schäftigen, beispielsweise mittels Wrappern, die einen einheitlichen Zugriff
auf unterschiedliche Datenquellen ermöglichen. Es wird untersucht, in wie
fern sich die meist aus relationalen Datenbanken stammenden existieren-
den Techniken auf die NoSQL-Welt übertragen lassen und welche Heraus-
forderungen es noch zu meistern gilt. Aktuell existieren keine Ansätze, die
das Ziel verfolgen, Heterogenität in der vollständigen NoSQL-Landschaft
zu überwinden. Stattdessen entstehen immer mehr Ansätze, die lediglich
auf einer speziellen Klasse von Systemen anwendbar ist.


Der Hauptteil der Arbeit besteht aus der Vorstellung der Sprache No-
taQL. Anhand dieser Sprache wird exemplarisch gezeigt, wie Datentrans-
formationen auf flexiblen Schemata und unterschiedlichen Datenmodel-
len erfolgen können. Mittels Datenverarbeitungsframeworks können viele der
aufgezeigten Probleme zwar bereits gelöst werden, jedoch erfordern diese
einen hohen Aufwand bei der Einarbeitung, Entwicklung von Transforma-
tionsprogrammen und der Wartung. Aus diesem Grund werden verschie-
dene Ansätze vorgestellt, welche auf existierenden Frameworks basieren,
um NotaQL-Transformationen effizient auszuführen.


Der zweite große Teil der Arbeit beschäftigt sich mit inkrementellen Da-
tentransformationen. Solche Art von Berechnungen kommen häufig im Data-
Warehousing zum Einsatz, da sie deutlich schneller durchzuführen sind als


Komplettberechnungen. Dazu müssen periodisch ausgeführten Transfor-
mationsprogrammen die Änderungen seit einer vorherigen Ausführung
vorliegen, damit diese basierend auf dem alten Ergebnis und ebendiesen
Änderungen das neue Ergebnis berechnen können. In dieser Arbeit werden
existierende Techniken zur Änderungserfassung und zur inkrementellen
Wartbarkeit untersucht und auf NoSQL-Datenbanken sowie die Sprache
NotaQL übertragen. Eine NotaQL-Plattform vereinigt verschiedene inkre-
mentelle und nicht-inkrementelle Ausführungsarten und wählt mit ihrer
Advisor-Komponente automatisch die als schnellste eingeschätzte Art.


Zusammengefasst sind die Beiträge der vorliegenden Arbeit wie folgt:


• Eine Datentransformationssprache, die für die Schema-Flexibilität und
die Datenmodelle von NoSQL-Datenbanken optimiert ist.


• Eine Ausführungsplattform, welche in der Lage ist, Datentransforma-
tionen virtuell und materialisiert auszuführen.


• Inkrementelle Berechnungsmethoden für die effiziente Ausführung
sich periodisch wiederholender Transformationen.


• Eine Analyse über die Anwendbarkeit der Sprache bei der Big-Data-
Analyse, bei Polyglott-Persistence-Anwendungen, Datenintegrationen
und Schema-Evolutionen.


1.4 Grenzen


Die Entwicklung der schnellsten Datentransformationsplattform ist nicht
Ziel dieser Arbeit. Stattdessen wird die Machbarkeit einer solchen Platt-
form gezeigt und Lösungen zur Überwindung von Heterogenität und zur
Unterstützung flexibler Schemata vorgestellt. Die händische Entwicklung
eines für einen bestimmten Anwendungszweck gedachtes Transformati-
onsprogramms wird in der Regel zu einer effizienteren Ausführung führen
als mit dem in dieser Arbeit vorgestelltem Ansatz. Auf der anderen Seite
sind solche Programme aufwändiger zu entwickeln und können nicht ein-
fach auf andersartige Systeme und Datenmengen übertragen werden.


Des Weiteren soll nicht Ziel der Arbeit sein, NotaQL als die benutzer-
freundlichste und mächtigste Transformationssprache anzupreisen. Die bei-
den genannten Aspekte der Benutzerfreundlichkeit und Mächtigkeit waren
zwar einige der Ziele bei der Entwicklung der Sprache, sind jedoch stets
ausbaufähig. Das zeigt sich auch daran, dass sich die Sprache im Laufe
der vergangenen Jahre ständig verändert und weiterentwickelt hat. Zum
Teil ist dies mit gestiegenen Anforderungen wegen komplexeren Transfor-
mationen oder zu unterstützenden Datenmodellen zu begründen, teilweise
wurde aber auch lediglich erkannt, dass gewisse Sprachkonstrukte elegan-
ter als andere sind. Diese Erfahrungen, Ideen und Lösungen führten zur
Sprache NotaQL, wie sie aktuell ist. Zum einen bedeutet das, dass weitere
Iterationen die Sprache NotaQL weiter verändern, zum anderen kann bei
der Entwicklung neuer Sprachen auf die hier vorgestellten Ansätze zurück-
gegriffen werden.


gegUrifmfeMn wissevrderesnt.ändnisse zu vermeiden, soll die folgende Aufzählung The-
men zeigen, die zwar mit der vorliegenden Arbeit verwandt sind, jedoch
als orthogonal oder außerhalb des Fokus angesehen werden:


• Performanz: Die Performanz hängt vom verwendeten Datenbanksys-
tem, dem zugrundeliegenden Transformationsframework, der Hard-
ware und mehr ab.


• Optimierungen: Die Ausführung von NotaQL-Transformationen und
die Implementierung der Operatoren kann stets optimiert werden;
diese sind vor allem auch von der Ausführungsart abhängig. Diese
Arbeit soll jedoch generisch und nicht auf eine bestimmte Ausfüh-
rungsart spezialisiert sein.


• Transaktionsbegriff: Fragestellungen über die atomare und isolierte Aus-
führung sowie das Verhalten im Fehlerfall wurden ausgeblendet, da
diese Themen in der Literatur bereits detailliert behandelt wurden
und ein Übertragen auf die generische NotaQL-Plattform den Rah-
men dieser Arbeit sprengen würde.


• Hardware- und Netzwerk-Charakteristika: Die vorgestellten Ansätze pro-
fitieren von Hardware-Optimierungen und verteilten Speicherungen,
setzen diese aber nicht voraus.


• Anfragesprachen: NotaQL ist keine Anfragesprache, daher werden zur
Interaktion zwischen der Anwendung und der Datenbank, in welcher
sich Ein- und Ausgabedaten von Transformationen befinden, weiter-
hin benötigt.


1.5 Überblick


Im zweiten Kapitel dieser Arbeit werden Grundlagen zu NoSQL-Daten-
banken und Datenverarbeitungsframeworks präsentiert. Auch Techniken
zur inkrementellen Wartbarkeit und Herausforderungen bei der Integrati-
on von Daten sind Teil dieses Kapitels. Danach werden in Kapitel 3 exis-
tierende Transformationssprachen für NoSQL untersucht und miteinander
verglichen.


verglichen.
Der Hauptteil der Arbeit, welcher die Vorstellung der Sprache NotaQL
ist, befindet sich in Kapitel 4. Nach einer Demonstration von Sprachkon-
strukten für einfache Attribut-Mappings, Selektionen und Aggregationen
folgen komplexere Sprachelemente zur Transformation von Graphen und
Datenströmen.


Die im zweiten Kapitel vorgestellten Datentransformationsframeworks
dienen im fünften Kapitel als Basis für verschiedene Ausführungsarten von
NotaQL. Anhand mehrerer solcher Frameworks, aber auch über alternati-
ve Wege mittels Übersetzungen in eine andere Sprachen oder über virtuelle
Transformationen werden unterschiedliche Ansätze präsentiert und vergli-
chen. Um die genannten Ansätze um eine inkrementelle Wartbarkeit zu er-
weitern, folgen in Kapitel 6 Implementierungsmöglichkeiten für die eben-
falls im zweiten Kapitel vorgestellten Änderungserfassungsmethoden.


falls im zweiten Kapitel vorgestellten Änderungserfassungsmethoden.
In Kapitel 7 werden einige Anwendungsmöglichkeiten präsentiert, für
welche sich die Sprache NotaQL einsetzen lässt. Zum Ende in Kapitel 8
erfolgt eine Zusammenfassung der Arbeit.


Grund2lagen


In diesem Kapitel werden die Grundlagen zu dieser Arbeit präsentiert. Da-
bei wird zunächst mit der Definition und Vorstellung von NoSQL-Daten-
banksystemen begonnen. Da diese Systeme Daten meist auf eine verteilte
Art speichern und gleichzeitig aber nur sehr einfache Zugriffsmethoden
bieten, kommen für komplexe Analysezwecke meist Datenverarbeitungs-
frameworks wie MapReduce oder Spark zum Einsatz, welche weiter unten
in diesem Kapitel vorgestellt werden. Im Anschluss daran folgt eine Prä-
sentation von Ansätzen zur inkrementellen Berechenbarkeit. Diese Techni-
ken sind vonnöten, um langdauerende und sich regelmäßig wiederholende
Datenanalysen zu beschleunigen. Diese Arbeit beschäftigt sich aber nicht
nur mit Datenanalysen, sondern mit Transformationen vielfältiger Art. Da-
zu zählen auch Datenintegrationen, also der Transfer von Daten zwischen
verschiedenen Systemen. Mit dem Thema der Datenintegration wird dieses
Grundlagen-Kapitel abgeschlossen.


2.1 NoSQL-Datenbanken


Eine Definition für NoSQL-Datenbanksysteme ist in [Edl+10, S. 2] zu fin-
den. Dort heißt es, dass einige - nicht zwangsläufig alle - der nachfolgenden
Kriterien erfüllt sein müssen: Ein nicht-relationales Datenmodell, verteil-
te Speicherung, Quelloffenheit, Schemaflexibilität, die Unterstützung für
Datenreplikation, eine einfache API und Flexibilität bezüglich der ACID-
Einhaltung. Da es mittlerweile weit über einhundert Systeme gibt, die diese
Kriterien mehr oder weniger erfüllen, erfolgt eine Klassifizierung meist in
die folgenden vier Kategorien:


• Key-Value-Stores,


• Wide-Column-Stores,


• Dokumentendatenbanken,


• Graphdatenbanken.


Tabelle 2.1: Terminologien verschiedener DBMS


Es existieren zwar auch Systeme, die in keine dieser Kategorien fallen, nichts-
destotrotz möchten wir im Rahmen dieser Arbeit den Fokus auf diese vier
Kategorien legen. Die meisten Konzepte sind nämlich auf auch andere Da-
tenbanksysteme übertragbar. Beispielsweise haben Objektdatenbanken auf-
grund der Referenzen gewisse Ähnlichkeiten zu Graphdatenbanken, und
XML-Datenbanken ähneln aufgrund ihrer hierarchischen Baumstruktur Do-
kumentendatenbanken. Konzentrieren wir uns zunächst auf die ersten drei
genannten Klassen von NoSQL-Datenbanken und gehen dann separat auf
Graphdatenbanken ein. Key-Value-Stores, Wide-Column-Stores und Doku-
mentendatenbanken haben nämlich die Eigenschaft, dass sie Datensätze in
einer Form speichern, die eine gewisse Gesamtheit (engl. aggregate) unter-
stützt [SF12]. Würde man Blogbeiträge und deren Kommentare in einer
relationalen Datenbank speichern, bestände diese aus zwei Tabellen, die
zum Anfragezeitpunkt erst mit einem Join verbunden werden müssen. Ei-
ne gesamtheitsorientierte NoSQL-Datenbank würde den Blogbeitrag samt
Kommentaren als eine Einheit speichern. Zusätzlich gilt für diese Systeme,
dass jeder Datensatz einen Identifikator besitzt. Über welchen Geltungsbe-
reich dieser eindeutig definiert sein muss, unterscheidet sich von Klasse zu
Klasse.


Da es keine einheitliche klassenübergreifende Terminologie gibt, wer-
den in dieser Arbeit in der Regel die allgemeinen Begriffe Datenquelle, Da-
tensatz, Attribut, Wert und ID verwendet. Bei relationalen Datenbanken wür-
de man beispielsweise von einer Tabelle als Datenquelle sprechen. Ein Da-
tensatz ist eine Zeile, ein Attribut eine Spalte und die ID einer Zeile steht in
denjenigen Spalten, über die man den Primärschlüssel definiert. Tabelle 2.1
zeigt eine Gegenüberstellung der Terminologien verschiedener Datenbank-
managementsysteme [SD16].


Key-Value-Stores


Ein zunächst sehr einfaches Datenmodell haben die Key-Value-Stores, da
sie lediglich Schlüssel-Wert-Paare in der Datenbank ablegen. Man kann sich
die Datenbank als eine Tabelle mit nur zwei Spalten (Schlüssel, Wert) vor-
stellen, wobei der Wert von einem beliebigen Typ sein kann. Je nach Imple-
mentierung gibt es global nur eine einzige Datenbank, in der alle Schlüssel
eindeutig sein müssen, oder es gibt mehrere Datenbanken, die als eigen-
ständig angesehen werden können. Im ersten Fall ist das Datenbanksystem
eine Art Map, die zu jedem Schlüssel einen Wert zuordnet:


Schlüssel W ert →


Im zweiten Fall existiert für jede Datenbank eine eigene Map:


Datenbank (Schlu¨ssel W ert) → →


Der Key-Value-Store Riak [Bas15] ist ein Vertreter für diesen zweiten Typ.
Die Datenbanken werden in Riak Buckets genannt. Beim Lese- und Schreib-
zugriff ist stets anzugeben, auf welchem Bucket man arbeiten möchte. Im
Datenbanksystem Redis [Red] gibt es zwar auch die Unterteilung in ver-
schiedene Datenbanken, aber dort wird üblicherweise im Gegensatz zu Ri-
ak Daten verschiedenen Typs in einer einzigen Datenbank zusammenge-
fasst. Während Buckets bei Riak Namen wie „Produkte“ oder „Kunden“
haben können, sind Datenbanken in Redis lediglich durchnummeriert. Sie
werden also eher verwendet, um die Daten von unabhängigen Anwen-
dungen voneinander zu separieren. Eine Anwendung arbeitet dann meist
nur auf einer einzigen Datenbank, deren Nummer direkt nach dem Ver-
bindungsaufbau angegeben wird. Innerhalb dieser Datenbank werden un-
terschiedliche Entitätsmengen meist mittels Schlüsselpräfixen voneinander
separiert, z. B. produkt/812.


Der Zugriff auf Schlüssel-Wert-Paare erfolgt ähnlich wie bei Maps mit
den Methoden GET und PUT. GET nimmt einen Schlüssel entgegen und lie-
fert den dazugehörigen Wert (oder null). PUT übergibt man ein Schlüssel-
Wert-Paar, um es zu speichern. Die PUT-Operation bietet ein sogenann-
tes Upsert-Verhalten, welches eine Mischung aus Insert und Update dar-
stellt. Das bedeutet, dass der Wert eines Schlüssels geändert wird, wenn
der Schlüssel bereits vorhanden ist. Andernfalls erfolgt ein Einfügen.


Die meisten Key-Value-Stores unterstützen eine Vielzahl von Datenty-
pen für die zu speichernden Werte. Neben primitiven Typen wie Zahlen
und Zeichenketten werden auch große binäre Objekte wie Bilder oder Vi-
deos unterstützt. Aber auch komplexe Typen wie Listen, geordnete und un-
geordnete (Multi-)Mengen und Maps können eingesetzt werden und bieten
passende Zugriffsmethoden.


passende Zugriffsmethoden.
Das folgende Beispiel zeigt, wie mit Redis-Kommandos auf Schlüssel-
Wert-Paare lesend und schreibend zugegriffen werden kann:


SET counter 9
INCR counter
GET counter // liefert 10
RPUSH pers:1:projekte "DB Projekt"
HSET pers:$$$1$$$ vorname "Rita"
HGET pers:$$$1$$$ vorname // liefert Rita


Im Beispiel wurde ein Counter-Wert gesetzt, um eins erhöht und anschlie-
ßend wieder gelesen. An das rechte Ende einer projekte-Liste wurde ein
neues Projekt angefügt und in einer Hash-Map bekommt die Person mit
dem Schlüssel pers:$$$1$$$ an der Stelle vorname den Wert Rita. Der letzte
Befehlt gibt schließlich ebendiesen Vornamen aus.


Wide-Column-Stores


Das Datenmodell von Wide-Column-Stores gleicht dem der Key-Value-Sto-
res, wenn als Werte die im vorherigen Beispiel zu sehende Hash-Map ver-
wendet wird. Es erlaubt also die Speicherung von Datensätzen, die eine
ID besitzen sowie beliebige Attribut-Wert-Paare. Genau wie bei relationa-
len Datenbanken spricht man bei Wide-Column-Stores auch von Tabellen,
die aus Zeilen und Spalten bestehen. Die Unterschiede liegen jedoch in der
Schemafreiheit, da jede Zeile beliebige Spalten haben kann, die nicht vorher
definiert werden müssen, sowie in der flexiblen Verwendung von Datenty-
pen. Im Wide-Colum-Store HBase [Apab] wird der Einfachheit halber nur
der Datentyp Byte-Array unterstützt. Jegliche Datenwerte müssen also auf
Anwendungsseite vor der Speicherung in einen Byte-Array gewandelt und
beim Lesen wieder rückkonvertiert werden.


Das vereinfachte Datenmodell eines Wide-Column-Stores lässt sich wie
folgt darstellen:


Eine bestimmte Spalte innerhalb einer Zeile einer Tabelle nennen wir auch
Zelle. Wenn das Datenbankmanagementsystem Versionierung unterstützt,
wird zu jeder Zelle eine Historie an Werten gespeichert, üblicherweise zu-
sammen mit den Zeitstempeln des Schreibens:


T abelle (Row-ID (Spalte (V ersion W ert))) → → → →


Der Datenzugriff erfolgt auch hier wieder mit den Kommandos GET und
PUT. Das folgende Beispiel gilt für das Datenbankmanagementsystem HBa-


se:


PUT 'pers', 'P1', 'info:vorname', 'Rita'
GET 'pers', 'P1'
SCAN 'pers'


Das erste Kommando fügt eine Zeile in die Tabelle pers mit der Row-ID
P1 sowie einer Vornamen-Spalte hinzu. HBase kategorisiert Spalten in so-
genannte Spaltenfamilien. Beim Anlegen einer Tabelle müssen die Spalten-
familien festgelegt werden. Diese dienen zur Aufteilung unterschiedlicher
Anwendungskonzepte innerhalb einer Zeile. Im genannten Beispiel wird
lediglich die Spaltenfamilie info verwendet, in der die Spalte info:vor-
name Platz findet. Informationen über Profilaufrufstatistiken oder Bezie-
hungen zwischen Personen könnten in anderen Spaltenfamilien abgelegt
werden. Das GET-Kommando liefert die Zeile mit der gegebenen Row-ID
samt aller ihrer Spalten. Über weitere Parameter kann eine Projektion nach
Spaltenfamilien oder einzelnen Spalten erfolgen. Das SCAN-Kommando lie-
fert einen Cursor, um die Tabelle zeilenweise zu lesen. Auch hier ist der Ein-
satz von Filtern möglich, oder aber die Angabe eines Bereiches, in dem die
Row-IDs zu liegen haben. Letzteres ist aufgrund der in HBase verwendeten
Bereichspartitionierung sehr effizient ausführbar.


Dokumentendatenbanken


Während das Datenmodell in den zwei bisher genannten Klassen relativ
einfach war, sind in Dokumentendatenbanken komplexe hierarchische Struk-
turen möglich. Das Datenmodell erscheint jedoch zunächst sehr simpel:


Datenbank (Kollektion Dokument ) → → ∗


Eine Kollektion, die sich in einer bestimmten Datenbank befindet, beinhaltet
eine Menge von Dokumenten. Die Komplexität kommt erst durch den mög-
lichen Aufbau dieser Dokumente zustande. Es gilt, dass die ID stets ein Teil
des Dokuments ist, also - anders als bei den oben vorgestellten Systemen
- nicht separat betrachtet wird. Zugriffe sind nicht nur über die ID mög-
lich, sondern es werden - ähnlich wie bei SQL - beliebige Selektionen auf
Sekundärattributen unterstützt. Zur effizienten Suche unterstützen Doku-
mentendatenbanken die Definition von Indexen auf ebendiesen Attributen.
Ein Index auf der Dokumenten-ID ist üblicherweise immer vorhanden.


Dokumente haben objektähnliche Strukturen. Sie beinhalten Felder, die
Werte haben. Diese Werte können neben Zahlen und Zeichenketten auch
Listen und verschachtelte Unterdokumente sein. Listen können ebenfalls
wieder Werte beliebigen Typs beinhalten und Unterdokumente wieder be-
liebige Felder. In MongoDB [Mona] und vielen anderen Dokumentendaten-
banken wird als Format, in dem die Dokumente dem Benutzer dargestellt
werden, JSON [JSOa] verwendet. Für die interne Speicherung wird auf effi-
zienter zu durchsuchende Strukturen wie das binäre JSON-Format BSON
zurückgegriffen. Das folgende Beispiel zeigt einen MongoDB findOne-
Befehl auf der Kollektion pers und das zurückgelieferte JSON-Dokument:


db.pers.findOne( { _id: 1 } )


_id: 1,
vorname: "Rita",
telefon: [
{ typ: "privat", nr: "06315555" },
{ typ: "mobil", nr: "017555555" }
]
}


Ein zweites Beispiel für ein Dokumentendatenbanksystem ist CouchDB.
Zwar verwaltet auch CouchDB Dokumente im JSON-Format und unter-
stützt damit beliebige strukturierte Objekte mit Arrays und Unterdoku-
menten, allerdings gibt es im Vergleich zu MongoDB einige fundamenta-
le Unterschiede. Der standardmäßige Zugriff auf Dokumente erfolgt über
eine REST-API unter der Verwendung von den HTTP-Methoden PUT, GET
und DELETE. Für komplexere Suchanfragen über Sekundärattribute wird
der Einsatz von Views empfohlen. Diese Sichten haben einen eindeutigen
Namen und werden über eine JavaScript-Funktion definiert, welche eine
MapReduce-artige Transformation ausführt. Anders als Sichten in SQL sind
CouchDB-Sichten materialisiert. Beim Einfügen, Ändern und Löschen von
Dokumenten, werden also alle Sichten gemäß ihrer JavaScript-Implemen-
tierung aktualisiert. Transformationen wie Projektionen und Selektionen


bestehend lediglich aus einer Map-Funktion. Komplexere Transformatio-
nen, in denen Gruppen aus mehreren Dokumenten gebildet werden und
in denen Aggregatfunktionen ausgeführt werden, bestehen aus den Funk-
tionen Map und Reduce. Weitere Details über MapReduce befinden sich in
Abschnitt 2.2. Ein weiterer Unterschied zwischen CouchDB und MongoDB
ist, dass es in einer CouchDB-Datenbank keine Kollektionen gibt. Alle Do-
kumente werden also zusammen in eine Datenbank geschrieben, was in
folgendem alternativen Datenmodell dargestellt werden soll:


Datenbank Dokument → ∗


Zur Unterscheidung verschiedenartiger Dokumententypen wird in Couch-
DB oft das Attribut doc_type verwendet. Die erste Operation in der Defi-
nition einer Sicht ist somit meistens if(doc.doc_type == "...").


Graphdatenbanken


Bei den drei zuvor genannten Kategorien von NoSQL-Datenbanksystemen
erfolgt der Zugriff auf die Datensätze entweder über eine gegebene ID oder
mittels eines sequentiellen oder eines Index-Scans auf einer einzigen Da-
tenquelle. Operationen wie Joins oder Selektionen auf Sekundärattributen
werden entweder nicht unterstützt, erfordern gewisse Hilfsstrukturen wie
Indexe, oder sie sind schlichtweg einfach nur sehr kostspielig. Graphda-
tenbanken sind für Traversierungen, also navigierende Zugriffe, optimiert. Die
einzelnen Datensätze werden Knoten genannt und sind vergleichbar mit
Dokumenten in Dokumentendatenbanken. Jeder Knoten kann eine beliebi-
ge Anzahl von Attributen haben (sogenannte Properties), in der die zu den
Knoten gehörenden Daten gespeichert werden. Das Konzept einer ID wird
nicht von allen Graphdatenbanken unterstützt. Manche setzen sie zwar in-
tern zur Unterscheidung von Knoten ein, bieten aber nicht die Möglich-
keit, eine ID manuell festzulegen. Viele Systeme bieten Unterstützung für
sogenannte Knoten-Labels. Diese beschriften einen Knoten mit einem oder
mehreren Typen, z. B. „Kunde“ oder „Produkt“. Alle Knoten werden zu-
sammen in einem Graphen gespeichert, sodass zusammen mit den Kanten,
die die Knoten miteinander verbinden, das Datenmodell als Graph im ma-
thematischen Sinne gesehen werden kann:


G = (V, E)


Der Property-Graph G besteht aus einer Menge von Knoten V und Kanten
E. Knoten besitzen beliebige Properties sowie optional Labels und eine ID.
Auch Kanten können Properties besitzen. Bei ihnen ist das Label Pflicht,
um die Art der Beziehung zwischen zwei Knoten anzugeben. Kanten sind
in den meisten Graphdatenbanken stets gerichtet, sie besitzen also jeweils
einen Start- und einen Ziel-Knoten.


Das folgende Beispiel zeigt, wie ein Graph bearbeitet und angefragt
werden kann. Die hier verwendete Sprache ist Gremlin [Rod15], die sich
in Verbindung mit gängigen Graphdatenbanksystemen wie zum Beispiel
Neo4J [Neo] verwenden lässt:


k = g.addVertex(1, [label:"Person", vorname:"Kai"])
u = g.addVertex(2, [label:"Person", vorname:"Ute"])


g.addEdge(null, k, u, "Freund", [seit:2016])
freunde = g.V.has("vorname", "Kai").both("Freund")


Im Beispiel werden zwei Personen-Knoten erstellt, die mit einer Kante des
Labels „Freund“ verbunden werden. Obwohl eine Freundschaftsbeziehung
symmetrisch sein sollte, werden in Graphdatenbanken üblicherweise nur
gerichtete Kanten verwendet. Die Richtung ist in unserem Beispielgraphen
zwar gegeben, aber irrelevant. Bei der Abfrage in der untersten Zeile wird
zunächst die Person mit dem Vornamen Kai gesucht und von diesem Kno-
ten aus über alle Kanten mit dem Label „Freund“ traversiert. Das Ergebnis
des both-Schrittes liefert alle Knoten, die über solche Kanten, egal ob ein-
oder ausgehend, erreichbar sind, also in diesem Fall alle Freunde von Kai.


2.2 Datenverarbeitungsframeworks


Wie wir im vorherigen Abschnitt gesehen haben, bieten typische NoSQL-
Systeme sehr einfache Abfragemöglichkeiten. Anders als bei relationalen
Datenbanksystemen kommt auf Anwendungsseite üblicherweise keine An-
fragesprache wie SQL zum Einsatz, stattdessen werden direkte API-Metho-
den wie PUT und GET verwendet. Dies erspart zwar den Aufwand, den das
Datensystem zum Parsen, zur Interpretation und Optimierung einer An-
frage benötigt, es schränkt aber die Mächtigkeit der möglichen Anfragen
stark ein. Der Hauptanwendungszweck der betrachteten Systeme ist es,
einen Datensatz zurückzugeben, der den gegebene ID-Wert besitzt. Übli-
cherweise kann diese typische GET-Operation um eine Projektion erweitert
werden, um nicht benötigte Attribute auszublenden. Viele Systeme bieten
auch Bereichs- oder Mustersuche anhand der ID an. So kann man in HBase
alle Zeilen lesen, deren Row-ID in einem bestimmten Bereich liegen und
von Redis eine Liste von Schlüsseln ausliefern lassen, die zu einem gege-
benen regulären Ausdruck passen. Prädikate auf anderen Attributen als
der ID sind in Wide-Column-Stores typischerweise nur mit kostspieligen
Filter-Operatoren möglich, die einen vollständigen Tabellenscan erfordern.
In Key-Value-Stores werden Wert-basierte Filter meist gar nicht unterstützt.
Dokumenten- und Graphdatenbanken bieten hier einen Sonderfall, da die-
se ähnliche Selektionsmethoden und Zugriffspfade bieten, wie man sie aus
relationalen Datenbanken kennt. Durch das flexible Schema kann jedoch
die Speicherung nicht so effizient wie in letzteren erfolgen, was schlussend-
lich auch wieder die Zugriffsgeschwindigkeit reduziert.


Geht man von einfachen Zugriffen und Suchoperationen zu Datenana-
lysemethoden, bieten die meisten NoSQL-Datenbankmanagementsysteme
dafür keine Unterstützung. Eine Ausnahme ist die MongoDB Aggregati-
on Pipeline, mit der eine Folge von Operationen definiert werden kann,
um Daten zu filtern, gruppieren, aggregieren und zu sortieren. Über wei-
tere Methoden sind auch das Auseinandernehmen von Listen sowie Ver-
bundoperationen möglich. In Kapitel 3.3 befinden sich weitere Details über
die Funktionsweise und die Einschränkungen der MongoDB Aggregation
Pipeline. Ein anderer Weg, komplexe Datenanalysen durchzuführen, bie-
ten Plattformen, die es erlauben, SQL-Anfragen an NoSQL-Datenbanken
zu stellen. Damit kann sich der Anwender aus den aus SQL bekannten
Möglichkeiten zur Gruppierung, Aggregation und mehr bedienen. Auch
auf diese wird später, in Kapitel 3.1 detailliert eingegangen.


Wenn große Datenmengen als Gesamtes analysiert oder transformiert
werden sollen, ist es geschickt, ein Datenverarbeitungsframework einzuset-
zen. Der Zugriff erfolgt dann anders als bei der Verwendung der Datenbank-
API-Methoden nicht auf einzelne Datensätze, sondern auf den komplet-
ten Datenbestand. Bei Wide-Column-Stores wird also üblicherweise eine
komplette Tabelle analysiert, in Dokumentendatenbanken eine Kollektion.
Auch Textdateien können auf diese Art vollständig analysiert werden. Die
im Folgenden vorgestellten Frameworks nutzen die Verteiltheit der Daten
aus und führen so viele Operationen wie möglich direkt auf denjenigen
Rechnern aus, auf denen die Daten gespeichert sind. Zur finalen Berech-
nung der Ergebnisse ist in verteilten Umgebungen meist ein Datentransport
über das Netzwerk vonnöten, der jedoch durch die lokale Vorverarbeitung
und durch geeignete Partitionierungstechniken optimiert werden kann. Die
Ausgabe einer Berechnung wird üblicherweise in eine Datenbank oder ei-
ne Datei geschrieben. Die direkte Verwendung des Ergebnisses innerhalb
einer Anwendung ist in vielen Frameworks aber auch möglich.


Bei Frameworks handelt es sich um Programmiergerüste, also um Schnitt-
stellenbeschreibungen sowie fertige Klassen und Methoden, die einem An-
wendungsprogrammierer gewisse Aufgaben abnehmen. Der Anwender ent-
wickelt weiterhin in einer Programmiersprache wie Java oder Python und
hat die komplette Mächtigkeit, die diese Sprachen mit sich bringen. Er muss
sich dabei allerdings nicht um die Entwicklung generischer Aufgaben küm-
mern, sondern kann sich auf die für den speziellen Datenanalysezweck spe-
zifischen Aspekte konzentrieren. Die Frameworks bringen folgende, nicht
zwangsweise alle, Funktionalitäten mit:


• Ein Programmiermodell, an das sich der Anwender bei der Entwick-
lung hält,


• die verteilte Ausführung benutzerdefinierter Methoden,


• Schnittstellen zu (verteilten) Datenbanken und Dateisystemen für die
Ein- und Ausgabe,


• Verteilung (Partitionierung) von Zwischen- und Endergebnissen auf
die Rechnerknoten im Netzwerk,


• Bereitstellung typischer Funktionen (Gruppieren, Aggregieren, Sor-
tieren, Zählen, . . . ),


• Wiederanlauf im Fehlerfall sowie das Wiederverwenden oder Ver-
werfen unvollständiger (Zwischen-)ergebnisse,


• Monitoring-Tools, um die Ausführung und Historie zu betrachten,


• Sicherheitsmechanismen wie Authentifizierung und Datenverschlüs-
selung.


Im Folgenden werden einige prominente Datenverarbeitungsframeworks
kurz vorgestellt.


MapReduce / Apache Hadoop Das von Google vorgestellte Programmier-
modell und Framework MapReduce [DG04] ist aufgrund seiner Einfachheit
und dennoch sehr großen Mächtigkeit sehr populär. Der berühmteste Klon
von Google’s MapReduce ist das in Java geschriebene quelloffene Frame-
work Apache Hadoop [Apaa]. Die Grundidee hinter MapReduce ist es, eine
Menge von Eingabedaten Datensatz für Datensatz mit einer benutzerde-
finierten Map-Funktion in Schlüssel-Wert-Paare zu transformieren. Da die
einzelnen Berechnungen voneinander unabhängig sind, können sie verteilt
durchgeführt werden. Die Zwischenergebnisse werden im Anschluss an-
hand ihres Schlüssels gruppiert und partitioniert, sodass sie zur weiteren
Verarbeitung wieder auf den vorhandenen Rechnerknoten verteilt vorlie-
gen. Eine benutzerdefinierte Reduce-Funktion nimmt nacheinander einen
dieser Schlüssel sowie die Liste der zu diesem Schlüssel gehörenden Werte
entgegen und berechnet anhand dieser das Endergebnis. Die Ausführung
der Reduce-Funktion erfolgt ebenfalls wieder verteilt.


map : Datensatz (Schlu¨ssel W ert)
→ × ∗
reduce : Schlu¨ssel W ert Schlu¨ssel W ert × ∗ → ×


Die hier gezeigte Definition der Map- und Reduce-Funktion soll lediglich
dem groben Verständnis dienen. Verschiedene MapReduce-Implementie-
rungen weichen teilweise stark von diesem Modell ab. Manche setzen vor-
aus, dass bereits die Eingabedaten als Schlüssel-Wert-Paare vorliegen. Auch
ist es möglich, dass die Reduce-Funktion mehrere Ausgaben liefern kann.


In Hadoop erfolgt die Ausführung eines MapReduce-Jobs als eine Men-
ge von Tasks. Auf jedem Rechner wenden ein oder mehrere Map-Tasks die
Map-Funktion auf einer Menge von Eingabedatensätzen an. Beim Auftre-
ten eines Fehlers oder beim Ausfall eines Rechners wird der betreffende
Task neu gestartet. Eventuell vorhandene Zwischenergebnisse werden ver-
worfen, um ein Exacly-Once-Verhalten und damit die Korrektheit des End-
ergebnisses sowie eine deterministische Ausführung zu gewährleisten. Nach
Abschluss der Map-Phase erfolgt äquivalent dazu die Ausführung der Re-
duce-Tasks.


Vorteile von MapReduce sind neben dem einfachen Programmiermo-
dell, der Mächtigkeit und der Fehlertoleranz auch die gute Skalierbarkeit.
MapReduce wird häufig für Text- und Log-Analysen eingesetzt sowie zur
Ausführung von Graph-Algorithmen. Nachteile sind die Einschränkungen,
die das starre Programmiermodell mitbringt. Beispielsweise ist für einige
Algorithmen, die nicht als ein einziger MapReduce-Job ausgedrückt wer-
den können, die Definition von MapReduce-Ketten nötig. Ein erster Job er-
ledigt einen ersten Schritt und schreibt die Ergebnisse auf die Festplatte,
die anschließend als Eingabe für einen zweiten Job dienen. Dieses Schrei-
ben und Lesen kann zu einer sehr inperformanten Ausführung führen. Das
gleiche Problem tritt bei iterativen Berechnungen auf. Dort wird das Ergeb-
nis jeder Iteration auf der Festplatte gespeichert, bevor es in der nächsten
Iteration gelesen werden kann.


Manche Datenbanksysteme - wie zum Beispiel MongoDB - bieten eine
eingebaute MapReduce-Funktionalität, um benutzerdefinierte Berechnun-
gen verteilt auszuführen. Der allgemeingültige und elegantere Weg ist es
jedoch, Apache Hadoop in Verbindung mit den für ein Datenbanksystem


spezifischen Ein- und Ausgabeformaten zu verwenden, wie zum Beispiel
dem „TableInputFormat“ für HBase oder dem „MongoInputFormat“, wel-
ches ein Teil des MongoDB Hadoop Connectors ist.


Apache Spark Die Idee hinter Spark [Zah+10] ist es - genau wie beim
MapReduce-Framework - eine Plattform zu bieten, um benutzerdefinierte
Methoden auf großen Datenmengen verteilt auszuführen. Allerdings ver-
zichtet Spark auf ein starres Programmiermodell und bietet stattdessen die
Möglichkeit, eine Reihe von Operationen nach Belieben zu definieren. Da-
zu gehören neben den Funktionen Map und Reduce auch Filter, Joins und
Gruppierungen. Die Datenzugriffe und auch die Operationen liefern so-
genannte Resilient Distributed Datasets (RDDs). Für MongoDB bietet bei-
spielsweise der MongoDB Hadoop Connector den Zugriff auf eine Kollek-
tion als RDD. Werden nun auf diesem RDD Operationen wie Filter oder
Map ausgeführt, sorgt das Spark Framework dafür, dass diese Berechnun-
gen so weit es geht direkt auf den Rechnern erfolgen, auf denen die Da-
ten gespeichert sind. Genau wie bei MapReduce wird so der Datentrans-
port minimiert. Aufgrund des flexiblen Programmiermodells werden die
in MapReduce erwähnten Schreibvorgänge für Zwischenergebnisse ver-
mieden. Stattdessen führt Spark weitestgehend alle Berechnungen im Ar-
beitsspeicher aus und schreibt erst das Endergebnis in die gewünschte Aus-
gabedatei oder in eine Datenbank. Auch bei iterativen Berechnungen behält
Spark die Zwischenergebnisse im RAM. Genau wie Hadoop ist Spark feh-
lertolerant und garantiert eine Exactly-Once-Ausführung für alle Berech-
nungen.


In Spark gibt es zwei Arten von Operationen, nämlich Transformationen
und Aktionen. Transformationen unterscheiden sich wiederum in solche mit
nahen Abhängigkeiten und weiten Abhängigkeiten. Erstere können direkt hin-
tereinandergeschaltet werden, z. B. eine einfache Datensatztransformation
mittels einer Map-Funktion und ein Filter. Weite Abhänigkeiten erfordern
eine Berechnung von Zwischenergebnissen. Eine Reduce-Funktion benö-
tigt beispielsweise die vollständige Liste an Werten zu einem Schlüssel, be-
vor die Berechnung starten kann. Beim Start eines Spark-Jobs werden die
Operationenfolgen analysiert und optimiert. Dabei erzeugt Spark so vie-
le Ausführungsphasen (Stages), wie es weite Abhängigkeiten gibt, plus ei-
ne weitere. Ein typischer MapReduce-Job würde also auch bei Spark aus
zwei Phasen bestehen. Unter Aktionen versteht man Operationen welche
tatsächlich Berechnungen ausführen. Denn allein durch die Ausführung ei-
ner Transformation werden keine Daten gelesen oder verarbeitet. Dies pas-
siert erst lazy, sobald eine Aktion erfolgt. Typische Aktionen sind collect
oder cache. Erstere wandelt ein RDD in eine Java-, Scala oder Python-
Kollektion, sie hebt also die Verteilung auf und materialisiert die Ergeb-
nisse. Die cache-Operation liefert wie gehabt ein RDD, erzwingt aller-
dings das Ausführen vorangegangener Transformationen. Dies ermöglicht
ein mehrfaches Wiederverwenden von Berechnungszwischenergebnissen.


Spark Streaming [Zah+12] erweitert Spark um die Möglichkeit, Daten-
ströme in Echtzeit zu analysieren. Im Gegensatz zur klassischen Stapelver-
arbeitung von gespeicherten Datensätzen sind Datenströme kontinuierlich,
sodass die Berechnung theoretisch nie abgeschlossen ist. Stattdessen liegen
die Ergebnisse von Datenstromanalysen auch wieder als Datenstrom vor.


Analog zu RDDs werden bei Spark Streaming die gleichen Arten von Ope-
rationen (Map, Reduce, Join, . . . ) auf diskretisierten Strömen (DStreams)
ausgeführt. Der eingehende Datenstrom wird zunächst in Pakete („Bat-
ches“) mit einer Größe von wenigen Sekunden zerlegt, auf denen wie auf
RDDs gearbeitet werden kann. Die Ergebnis-Pakete werden als Datenstrom
ausgegeben oder in eine Datenbank oder Datei gespeichert. Typische Da-
tenstromanfragen sind Fenster-basiert. Das heißt, auf einer Sequenz von
RDDs wird ein Fenster definiert, welches eine bestimmte Größe hat und in
bestimmten Intervallen aktualisiert wird. Wird Spark Streaming beispiels-
weise zur Analyse von Webserver-Logdateien eingesetzt, könnte eine An-
frage alle fünf Minuten die Anzahl der Besuche pro Webseite innerhalb der
letzten halben Stunde ausgeben. Hier wäre die Fenstergröße dreißig Minu-
ten und das Aktualisierungsintervall fünf Minuten.


Apache Flink Es existieren weitere Datenverarbeitungsframeworks, die
eine ähnliche Funktionalität wie die oben vorgestellten Programme bieten.
Apache Flink [Apa15b] ist speziell für die Analyse von Datenströmen kon-
zipiert, bietet aber auch Möglichkeiten zur Stapelverarbeitung. Die Verwen-
dung ist ähnlich zu der von Apache Spark, allerdings werden Ströme nicht
als Sequenzen von RDDs mit einer gewissen zeitlichen Größe angesehen,
sondern als tatsächliche kontinuierliche Datenstrom-Objekte. Dadurch eig-
net sich Flink besonders für Echtzeit-Analysen.


2.3 Inkrementelle Wartbarkeit


Die zu Beginn dieses Kapitels vorgestellten API-Methoden wie PUT und
GET bieten OLTP-Anwendungen einen Datenzugriff innerhalb weniger Mil-
lisekunden. OLTP steht für Online-Transaction-Processing, also für die Echt-
zeitverarbeitung von Transaktionen. Da Zugriffe in NoSQL-Datenbanken
meist ID-basiert sind und Transaktionen in dem Sinne, dass sie eine ato-
mare Folge von Datenbankaktionen darstellen, von den meisten Systemen
nicht unterstützt werden, sind Datenbankzugriffe in OLTP-Anwendungen
sehr schnell und es können sehr viele davon parallel stattfinden.


Im zweiten Abschnitt dieses Kapitels wurden mehrere Datenverarbei-
tungsframeworks vorgestellt, die für die Zwecke des Online-Analytical-Pro-
cessings (OLAP) eingesetzt werden. OLAP-Anwendungen dienen zur Da-
tenanalyse und bestehen typischerweise aus Prozessen mit langer Lauf-
zeit, die ihre Ergebnisse in eine Zieldatenbank oder in eine Datei schreiben.
Da die Ergebnisse allerdings nur auf dem Stand der Daten zum Zeitpunkt
der Analyse sind und da sich die Basisdaten regelmäßig ändern, müssen
die Berechnungen typischerweise oft wiederholt werden. In diesem Ab-
schnitt geht es darum, OLAP-Anwendungen inkrementell wartbar zu ma-
chen. Unter inkrementeller Wartbarkeit versteht man die Fähigkeit, dass
eine Anwendung zur erneuten Berechnung eines Ergebnisses das vorheri-
ge Ergebnis als Basis nehmen kann und die Änderungen an den Basisdaten
darauf anwendet.


Soll eine Berechnung inkrementell wartbar sein, müssen die folgenden
Fragen geklärt werden:


1. Was hat sich in den Basisdaten geändert?


2. Welche Werte im vorherigen Ergebnis müssen geändert werden?


3. Wie wirken sich die Änderungen auf das vorherige Ergebnis aus?


Die Komponente zur Erfassung der Änderung (Punkt 1) ist die sogenann-
te Änderungserfassung (Change-Data-Capture, CDC) [KR11]. Zuerst stellen
wir verschiedene Change-Data-Capture-Verfahren vor, die es ermöglichen,
die Änderungen in den Basisdaten seit der vorherigen Berechnung heraus-
zufinden. Anschließend beschreiben wir Ideen zur Einbringung der Ände-
rungen auf das vorherige Ergebnis (Punkte 2 und 3).


Verfahren zur Änderungserfassung


Verschiedene Änderungserfassungsverfahren unterscheiden sich zum einen
bezüglich ihrer Einsetzbarkeit für die betrachteten Datenbanksysteme. Des
Weiteren müssen je nach Verfahren unterschiedliche Mengen an Daten ana-
lysiert werden, was zu teilweise sehr deutlichen Geschwindigkeitsunter-
schieden führen kann. Aber auch hier gibt es keinen klaren Sieger. Manche
Verfahren eignen sich gut, wenn die Änderungen seit der letzten Durch-
führung gering sind, andere, wenn sich viel geändert hat. Wenn wir von
Änderungen sprechen, teilen wir diese in zwei Arten auf, und zwar Einfü-
gungen und Löschungen. Sei B die Datenbasis, auf der die Berechnung er-
folgt. B(t 1) bezeichnet den aktuellen Zustand und B(t 0) den Zustand der
Basisdaten zum Zeitpunkt t 0 der vorherigen Berechnung.


Einfügungen ∆ =B(t 1) B(t 0)
−
Löschungen =B(t 0) B(t 1) ∇ −


Einfügungen sind also diejenigen Datensätze, die seit der vorherigen Be-
rechnung neu hinzugekommen sind, Löschungen diejenigen, die seitdem
entfernt wurden. Findet innerhalb eines existierenden Datensatzes eine Ver-
änderung statt, kann diese als Löschung mit anschließender Einfügung ge-
sehen werden. Bei der nun folgenden Analyse der verschiedenen Ände-
rungserfassungsverfahren wird auffallen, dass Löschungen nicht von allen
Verfahren unterstützt werden.


Audit-Columns Der Begriff stammt aus Data-Warehouse-Systemen, bei
denen in den zu überwachenden Tabellen speziell für das CDC vorgese-
hene Spalten hinzugefügt werden. Im allgemeinen Fall versteht man unter
einer Audit-Column ein Attribut pro Datensatz, anhand dessen sich erken-
nen lässt, ob sich ein Datensatz geändert hat. In der Regel beinhaltet das At-
tribut den Zeitstempel der letzten Änderung oder eine Versionsnummer. Es
ist auch denkbar, dass die Audit-Column lediglich die boolesche Informati-
on beinhaltet, ob sich der Datensatz seit der vorherigen Berechnung ändert
hat oder nicht. In letzterem Falle muss das Transformationssystem nach ei-
ner Berechnung diesen Wert zurücksetzen, während Zeitstempel und Ver-
sionsnummern in den Basisdaten erhalten bleiben können.


Das relationale Datenbankmanagementsystem IBM DB$$$2$$$ für z/OS bietet
eine native Unterstützung für Audit-Columns. Beim Anlegen der Tabelle
wird die betreffende Spalte mit der Option ON UPDATE AS ROW CHANGE


TIMESTAMP versehen, sodass das Datenbanksystem automatisch den Zeit-
stempelwert aktuell hält. In MongoDB werden beim Einsatz automatisch
erzeugter Objekt-IDs die ersten vier Bytes dieser ID für den Zeitstempel
der Einfügung verwendet. Nachträgliche Änderungen am Dokument ver-
ändern allerdings nicht die Objekt-ID und damit auch nicht den Zeitstem-
pel. Da auf dem ID-Attribut stets ein B+-Baum-Index existiert, sind mittels
Präfixanfragen effiziente Suchen nach neu eingefügten Dokumenten mög-
lich. Im Falle von DB$$$2$$$ lassen sich mit einer Selektion auf der Audit-Column
alle Zeilen finden, die sich seit der letzten Berechnung geändert haben. Eine
Unterscheidung, ob es sich um einen komplett neuen oder um einen verän-
derten Datensatz handelt, ist in beiden genannten Ansätzen nicht möglich.
In MongoDB werden auf diese Art nur neu eingefügte Dokumente gefun-
den. In jedem Fall ist das Erkennen von Löschungen ausgeschlossen. Um
dieses Problem zu lösen, müssen Löschungen verboten werden und statt-
dessen eine Audit-Column mit den Zeitpunkt der logischen Löschung ein-
geführt werden.


Mit Audit-Columns lässt sich zwar feststellen, dass sich etwas am Da-
tensatz geändert hat, aber nicht was. Das Analyseprogramm hat nur Zu-
griff auf den neuen Stand der Daten, nicht aber auf den Stand zum Zeit-
punkt der vorherigen Berechnung. Das liegt daran, dass beim Einsatz von
Audit-Columns keine automatische Historie vorheriger Attributwerte an-
gelegt wird.


geleDgat swfiorldg.ende Beispiel zeigt mehrere Varianten, wie Audit-Columns ein-
gesetzt werden können, um Änderungen an Datensätzen zu erfassen:


{ _id: 1, name: "Ute", inserted: 1451606400 }
{ _id: 2, name: "Jörg", inserted: 1451606400,
updated: 1451607333 }
{ _id: 3, name: "Lisa", modified: true }
{ _id: 4, deleted: 1451607444 }


{ _id: ObjectId("5650d1906b66d66a5dbfda29"), ...}


Zeitstempel-basiertes CDC mittels Multi-Versionierung Verwaltet ein
Datenbanksystem eine Historie von Attributwerten und bietet darauf Zu-
griffsmöglichkeiten, kann diese zur Erfassung der Änderungen verwendet
werden. Mittels Multi-Versionierung lässt sich leicht herausfinden, welche
Datensätze seit der vorherigen Berechnung neu eingefügt wurden, wel-
che sich geändert haben und an welchen Datensätzen sich nichts geändert
hat. Des Weiteren hat man Zugriff auf die vorherigen Werte. Damit lässt
sich beispielsweise berechnen, um wie viel sich ein nummerischer Wert
erhöht oder verringert hat. Lediglich Löschungen können mittels Multi-
Versionierung nur schwierig erkannt werden. Sollen diese ebenfalls un-
terstützt werden, empfiehlt es sich, statt dem endgültigen Löschen von
Datensätzen (samt ihrer vollständigen Historie) lediglich deren Attribute
zu entfernen oder die Attributwerte auf null zu setzen. Einige NoSQL-
Datenbanksysteme wie HBase bieten eine eingebaute Multi-Versionierung,
die allerdings meist erst explizit aktiviert werden muss. Dazu gibt man die
Anzahl n der zu verwaltenden Versionen an. Im Extremfall kann dies bei
einer Datenbank der Größe N zu einem Speicheraufwand von O(N n) ·
führen. Bei HBase wird zu jedem Attribut eine Historie von Datenwerten


zusammen mit den Zeitstempeln des Schreibens gespeichert. Im Allgemei-
nen könnte man aber auch eine laufende Versionsnummer verwenden. Für
eine zuverlässige Änderungserfassung ist allerdings die Variante mit Zeit-
stempeln klar im Vorteil, da basierend auf diesen leicht festgestellt werden
kann, ob eine Version vor oder nach der Durchführung der vorherigen Be-
rechnung erstellt wurde.


Andere Ansätze verfolgen die Idee, dass sich nicht das Datenbankma-
nagementsystem sondern die Anwendung um die Versionierung kümmert.
Da der Zugriff bei NoSQL-Datenbanken meist über die ID erfolgt und vie-
le Systeme zur Indizierung der IDs B+-Bäume mit einer Unterstützung für
Präfix-Suchen verwenden, ist ein gängiger Ansatz, eine Versionsnummer
oder einen Zeitstempel als Suffix an die ID anzuhängen [Gra13]. Statt der
ID p17 könnte diese dann etwa p17:1451606400 lauten. Über eine Su-
che nach IDs mit dem Präfix p17: werden alle Versionen gefunden. Diese
manuelle Art Versionierung kann bei vielerlei NoSQL-Datenbanksystemen
eingesetzt werden. Es bietet sich an, eine Client-Erweiterung zu verwen-
den, um Datenbankmethoden (PUT, GET, . . . ) wie gehabt verwenden zu
können, um standardmäßig auf die aktuelle Version zuzugreifen und um
beim Einfügen und Ändern die Zeitstempel automatisch zu setzen [Fel+14;
CL13].


Log-basiertes CDC Eine Log-Datei ist eine vom Datenbankmanagement-
system gepflegte Datei, in welcher von Anwendungen durchgeführte Da-
tenbankänderungen protokolliert werden [HR83]. Sie kommt für verschie-
dene Zwecke zum Einsatz, in der Regel zur Gewährung der ACID-Eigen-
schaften. Mittels Transaktionslogs kann die Atomarität von Transaktionen
gewährleistet werden. Geht von einer Reihe von atomar auszuführenden
Operationen eine Operation schief oder beschließt die Anwendung den ma-
nuellen Abbruch einer Transaktion, kann die Log-Datei genutzt werden,
um bereits ausgeführte Änderungen rückgängig zu machen. Dazu ist es
vonnöten, dass die Log-Datei die Bytes der geänderten Blocke vor der Än-
derung enthält. Alternativ kann die Atomarität auch dadurch gewährleis-
tet werden, dass noch nicht freigegebene Änderungen lediglich in die Log-
Datei und noch nicht in die Datenbank geschrieben werden. Beim Abbruch
der Transaktion müssen dann nur die Einträge in der Log entfernt werden.
Erst beim Commit der Transaktion werden die Änderungen in die Daten-
bank eingebracht. Ein zusätzlicher Vorteil dieser Variante ist die Gewähr-
leistung der Isolation, da nicht freigegebene Änderungen noch nicht von
anderen Transaktionen gelesen werden. Diese Variante Transaktionen zu
unterstützen wird beispielsweise vom Graphdatenbanksystem Neo4J auf
eine ähnlich zu der hier beschriebenen Art eingesetzt. Der Key-Value-Store
Redis hingegen nutzt Log-Dateien, um die Dauerhaftigkeit abgeschlosse-
ner Transaktionen zu gewährleisten. Dies ist vonnöten, da Redis eine In-
Memory-Datenbank ist. Änderungen erfolgen also nur im Hauptspeicher
und werden je nach Konfiguration lediglich im Zeitintervall einiger Sekun-
den auf die Festplatte übertragen. Um zwischenzeitige Schreiboperationen
bei einem Stromausfall nicht zu verlieren, kommt die Log-Datei zum Ein-
satz, welche persistent gespeichert ist. Aufgrund des sequenziellen und
asynchronen Schreibens leidet die Performanz des Systems nicht so stark
darunter wie beim tatsächlichen persistenten Einbringen jeder Änderung


beim Abschluss einer Transaktion. Auch in MongoDB wird ein sogenann-
tes Journal eingesetzt, um Anwendungen die Bestätigung der erfolgrei-
chen Ausführung einer Operation zu geben, noch bevor diese persistent
geschrieben, wohl aber geloggt, wurden.


Ein anderer Anwendungsfall für Log-Dateien kommt bei der Datenrepli-
kation zum Einsatz. Der Grundgedanke von Replikation ist, dass jede Än-
derung, die auf einem Rechner (Replika) ausgeführt wird, ebenfalls auch
auf die anderen Replikas übertragen wird, um dort den kompletten Da-
tenbestand quasi in Echtzeit zu spiegeln. Aufgrund von Netzwerklaten-
zen, temporären Ausfällen und anderen Verarbeitungsverzögerungen ist
das gespiegelte Abbild jedoch nur „beinahe live“. Ist ein Replika aufgrund
schwererer Probleme oder wegen eines Ausfalls für eine längere Zeit nicht
erreichbar, müssen beim Wiederanlauf alle verpassten Operationen nach-
geholt werden. Hierbei kommt die Log-Datei zum Einsatz. Diese enthält
entweder Logsequenznummern oder Zeitstempel, mit denen sich feststel-
len lässt, welche Änderungen noch auszuführen sind.


Log-Dateien können im Wesentlichen auf zwei Arten aufgebaut sein.
Die erste Möglichkeit ist logisches Logging oder auch Statement-basiertes Log-
ging. Dabei beinhaltet jeder Log-Eintrag eine konkrete Datenbankoperati-
on, beispielsweise ein Einfügen, Löschen oder Ändern. Leseoperationen
werden nicht geloggt, da sie keine Auswirkungen auf den Datenbestand
haben. Die alternative Methode ist physisches Logging oder auch binäres Log-
ging. Hierbei wird protokolliert, welche Bytes in welchen Blöcken inwiefern
geändert werden müssen. Zwar gibt es noch weitere Log-Typen - wie das
physiologische Logging -, auf die aber hier nicht weiter eingegangen wird.
Stattdessen betrachten wir die Vorteile und Nachteile sowie die Möglichkei-
ten, wie Log-Dateien zu Zwecken der Änderungserfassung verwendet wer-
den können. Statement-basiertes Logging kommt beispielsweise bei der Re-
plikation in MongoDB zum Einsatz, da diese auch dann funktioniert, wenn
auf unterschiedlichen Replikas verschiedene MongoDB-Versionen oder un-
terschiedliche Storage-Engines verwendet werden. Der Nachteil ist, dass
aus den Log-Einträgen keine Information entnommen werden kann, wie
eine Operationen wieder rückgängig gemacht werden kann. Die Log-Datei
kann also nur vorwärts und nicht rückwärts abgespielt werden. Bei der Re-
plikation ist ohnehin nur ersteres vonnöten.


Für die Änderungserfassung bringt Statement-basiertes Logging große
Nachteile mit sich. Mit der Log-Datei lässt sich zwar feststellen, welche Än-
derungen an der Datenbank seit einer vorherigen Analyse ausgeführt wur-
den, allerdings ist der Basisdaten-Zustand zum Zeitpunkt der vorherigen
Analyse nicht mehr abrufbar. Einfach gesagt werden also nur Einfügun-
gen und keine Änderungen oder Löschungen unterstützt. Erst durch auf-
wändige Methoden, wie dem Durchsuchen der Log nach vorherigen Ope-
rationen auf einem gegebenen Datensatz oder über eine Kombination mit
Snapshot-basiertem Logging (siehe nächster Absatz) lässt sich auch Log-
basiertes CDC zuverlässig verwenden.


Das folgende Beispiel zeigt einen Eintrag der in MongoDB zur Repli-
kation geführten Operationen-Log. Er beinhaltet unter anderem einen Zeit-
stempel (ts), die Art der Operation (i = Einfügung), die betroffene Da-
tenbank und Kollektion (ns = Namespace) sowie das neue Objekt (o). Bei
Änderungen und Löschungen befinden sich die IDs der zu ändernden bzw.
zu löschenden Dokumente in den Log-Einträgen.


{ ts : Timestamp(1448269379, 1), ..., op: "i",
ns: "test.personen", o: { _id: 1, name: "Ute" } }


Snapshot-basiertes CDC Der Grundgedanke dieser Art der Änderungs-
erfassung ist sehr einfach: Zum Zeitpunkt jeder Analyse wird ein vollstän-
diges Abbild der Basisdaten, ein sogenannter Snapshot oder Schnappschuss,
gespeichert. Bei einer späteren Analyse kann der aktuelle Datenbankzu-
stand mit dem vorherigen verglichen werden [Lin+86; LGM96]. Aufwändig
daran ist nicht nur das Anlegen dieser Abbilder sondern auch der Vergleich
zweier Snapshots. Allerdings bringt Snapshot-basiertes CDC auch Vorteile
mit sich. Ändern sich einzelne Datensätze auf einer Datenbasis enorm oft,
oder wird eben so viel gelöscht wie eingefügt, würden bei Multi-Versionie-
rung oder Log-Dateien große Datenmengen anfallen, da sie alle Zwischen-
zustände beinhalten. Bei Snapshot-basiertem CDC bleiben diese Zwischen-
zustände versteckt und es geht lediglich um den Vorher-Nachher-Vergleich.
Während bei den zuvor vorgestellten Alternativen entweder das Daten-
bankmanagementsystem spezielle Funktionalität mitbringen muss oder An-
wendungen angepasst werden müssen, ist diese Variante des CDC prinzi-
piell immer möglich. Auch unterstützt diese Variante alle Arten von Än-
derungen: Einfügungen, Löschungen sowie die Information, welche Attri-
butwerte sich wie geändert haben. Herausforderungen liegen jedoch bei
der Erstellung eines konsistenten Snapshots. Da der Erstellungsvorgang bei
großen Datenmengen mehrere Minuten in Anspruch nehmen kann und in
dieser Zeit weiterhin Änderungen durchgeführt werden, dürfen nicht Teile
des Snapshots alte und andere Teile neue Zustände haben. Eine Möglich-
keit zur Erreichung der Konsistenz ist beispielsweise die Verwendung von
Multi-Versionierung.


Trigger-basiertes CDC Ein Datenbanktrigger ist eine aktive Komponente,
welche benutzerdefinierte Operationen ausführt, sobald Änderungskom-
mandos an die Datenbank gestellt werden. Unterstützt ein System Trigger,
können diese zur Änderungserfassung eingesetzt werden. Eine Möglich-
keit ist, dass ein Trigger bei jeder Einfügung, Änderung und Löschung
„feuert“ und die Änderungen sowie den vorherigen Datenzustand in ei-
ne Warteschlange speichert. Diese kann zum Zeitpunkt von Berechnungen
verwendet werden, um die Auswirkungen seit der vorherigen Berechnung
zu bestimmen. Dieses Verfahren ist analog zum Log-basierten CDC, es er-
möglicht allerdings ein individuell anpassbares Loggen von Operationen
und Werte-Historien. Wenn das Datenbankmanagementsystem keine Log-
Dateien führt oder wenn Log-Dateien keine Redo-Informationen beinhal-
ten, kann dies durch den Einsatz von manuellem Logging mittels Triggern
kompensiert werden.


kompensiert werden.
Ein alternativer Ansatz von Triggern ist die unmittelbare Ausführung
von Berechnungen. Während bei den vorherigen CDC-Ansätzen sowie bei
einer vollständigen Neuberechnung Analysen üblicherweise in gewissen
Zeitabständen (z. B. einmal täglich) ausgeführt werden und CDC dazu ein-
gesetzt wird, die Änderungen seit der vorherigen Berechnung zu ermitteln,
ist es mit Triggern möglich, Änderungen auf das Ergebnis anzuwenden, so-
bald sie auf den Basisdaten erfolgen. Abgesehen von der Berechnungsdau-
er und anderen Verzögerungen, ist das Ergebnis bei dieser Variante immer


„live“ und auf dem neusten Stand. In SQL ist ein äquivalentes Datenbank-
objekt eine materialisierte Sicht mit der Option REFRESH IMMEDIATE. So-
bald sich die Daten der in der FROM-Klausel stehenden Tabellen ändern,
werden diese Änderungen direkt auf das materialisierte Ergebnis der Sicht
angewandt.


Inkrementelle Aggregationen


Datentransformationen, die lediglich Selektionen oder Projektionen durch-
führen sind leicht inkrementell ausführbar. Dazu wird bei neu eingefügten
Datensätzen geschaut, ob sie das Selektionsprädikat erfüllen und dement-
sprechend werden sie zur Transformation betrachtet oder nicht. Bei Lö-
schungen erfolgt auf gleiche Weise das Entfernen der Sätze. Lediglich bei
Änderungen muss darauf geschaut werden, ob Prädikate vorher erfüllt wa-
ren und immer noch erfüllt sind. Sieht man Änderungen jedoch als Lö-
schungen mit anschließendem Neueinfügen, ist auch hier die Lösung ein-
fach.


Komplexe Berechnungen sind jedoch diejenigen, in denen Daten grup-
piert und aggregiert werden. Bei der Berechnung einer Summe wird das bei
der vorherigen Analyse berechnete Ergebnis als Basis genommen und die
betreffenden Werte der eingefügten Datensätze darauf aufaddiert. Gelösch-
te Datensätze sorgen für Subtraktionen und Änderungen können auch hier
wieder als eine Kombination aus beidem angesehen werden.


wieder als eine Kombination aus beidem angesehen werden.
Betrachten wir das folgende Beispiel, in dem die Gehaltsumme pro Fir-
ma berechnet werden soll. Zu Beginn sehe der Datenbestand wie folgt aus:


{ _id: 1, name: "Ute", gehalt: 38000, firma: "IBM" }
{ _id: 2, name: "Jörg", gehalt: 37000, firma: "SAP" }
{ _id: 3, name: "Lisa", gehalt: 42000, firma: "IBM" }


Das Ergebnis ist also dieses:


{ _id: 'IBM', summe: 80000 }


{ _id: 'SAP', summe: 37000 }


Betrachten wir nun folgende Einfügung, Löschung und Änderung:


db.personen.insert({ _id: 4, name: 'Rita',
gehalt: 50000, firma: 'IBM' })
db.personen.remove({ _id: 1 })
db.personen.update({ _id: 3} {$set: { firma: 'SAP' }}


Die Gehälter der einzelnen Firmen ändern sich also folgendermaßen:


{ _id: 'IBM', summe: 80000+50000-38000-42000=50000}
{ _id: 'SAP', summe: 37000+42000=79000 }


Es fällt auf, dass die Zahl 42000, die zur Berechnung der neuen Summen
benötigt wird, nicht in der Liste der Änderungsoperationen zu finden ist.
Es ist also auch nötig, unveränderte Attribute zu betrachten.


Dieses Unterkapitel soll lediglich einen kleinen Einblick in die Heraus-
forderungen bei inkrementellen Berechnungen liefern. Im späteren Verlauf
der Arbeit wird auf diese genauer eingegangen. Bei der Verwendung von
anderen Aggregatfunktionenen als der Summenfunktion treten noch wei-
tere Probleme auf:


AVG Die Berechnung eines Durchschnittes ist nicht möglich, wenn ledig-
lich ein zuvor berechneter Durchschnittswert vorliegt. Dies liegt daran, dass
die Durchschnittsfunktion nicht assoziativ ist:


AVG(AVG(a, b), c) = AVG(a, AVG(b, c)) = AVG(a, b, c) 6 6


Später in dieser Arbeit werden wir sehen, dass die Assoziativität für die
inkrementelle Berechnung erfüllt sein muss. Bei der Durchschnittsfunktion
lässt sich diese durch die Speicherung von Hilfsdaten, nämlich der Anzahl
der aggregierten Elemente, erreichen.


MIN / MAX Die Minimum- und Maximum-Funktionen sind ein Beispiel
für irreversible Aggregatfunktionen. Sie unterstützen somit keine Löschun-
gen. Wird der aktuell kleinste (bzw. größte) Wert einer Menge entfernt, ist
die Bestimmung des Minimums (Maximums) unmöglich. Hier kann ein
MIN/MAX-Cache [Cha00] Abhilfe schaffen, in dem die n kleinsten und
größten Werte gespeichert werden, um zumindest die Unterstützung von
n Löschungen zu bieten.


COUNT Nehmen wir an, es soll eine Zählung von Werten (1, 7, 9) vorge-
nommen werden. Das Ergebnis ist 3. Soll nun die Zahl 9 eingefügt werden,
ist das Ergebnis nicht COUNT(3, 9) = 2 sondern 3+COUNT(9) = 4. Beim Zäh-
len darf also nicht der Fehler gemacht werden, dass eine vorherige Zählung
lediglich als ein einziges Element interpretiert wird. Als Lösung bietet sich
an, die Zählfunktion als Summe von Einsen SUM(1) zu interpretieren.


Sonstige Bei nicht-kommutativen Aggregatfunktionen wie IMPLODE (zur
Konkatenation von Zeichenketten) kann bei der inkrementellen Berechnung
eine Ordnung zerstört werden. Ein Beispiel: Es soll eine Komma-getrennte
Liste aller Namen von Personen pro Firma erstellt werden, die nach Ge-
hältern geordnet ist. Ändert sich das Gehalt einer Person, ist unklar, ob und
wie der Name in der Liste verschoben werden muss. Darüber hinaus gibt es
Aggregafunktionen, in der eine Duplikateliminierung erfolgt. In SQL wird
beispielsweise COUNT(DISTINCT firma) eingesetzt, um die Anzahl der
Firmen zu ermitteln und dabei jede Firma nur einmal zu zählen. Bei einer
Löschung tritt ähnlich wie bei MIN und MAX das Problem auf, dass man
nicht mehr weiß, ob die letzte Instanz eines Wertes gelöscht wurde oder ob
der Wert noch in anderen Datensätzen präsent ist.


Änderungseinbringung


Bei inkrementellen Berechnungen muss in der Regel auf zwei Datenquel-
len lesend zugegriffen werden. Zum einen auf die eigentlichen Basisdaten,
auf denen auch die Änderungserfassung erfolgt, zum anderen aber auch
auf das Ergebnis der vorherigen Berechnung. Das Ziel der inkrementellen
Berechnung soll sein, dass das Resultat äquivalent zu dem einer vollstän-
digen Berechnung ist. Betrachten wir die Basisdatensätze a 1, . . . , aj, die be-
reits bei einer vorherigen Berechnung das Ergebnis f(a 1, . . . , aj) erzeugt ha-
ben. Nach der Löschung von ai, . . . , aj und der Einfügung von aj+1, . . . , a k
gilt die inkrementelle Berechnung unter der Operation als korrekt, genau ◦
dann wenn gilt:


f(a 1, . . . , aj) f∗(ai, . . . , aj) f(aj+1, . . . , a k) = f(a 1, . . . , ai−1, aj+1, . . . , a k) ◦ ◦


Es fällt auf, dass eine Funktion f∗ vonnöten ist, um eine zu f inverse Be-
rechnung auszuführen, also um die Auswirkungen einer Berechnung wie-
der zu kompensieren. Es muss gelten f f∗ = e, wobei e das neutrale Ele- ◦
ment bezüglich ist. Des Weiteren ist Assoziativität und Kommutativität
von vonnöten,◦ sodass , f∗ und e auf dem Wertebereich von f eine abel- ◦ ◦
sche Gruppe bilden. Die genaue Definition der Operationen hängt von der
Art der Berechnung ab. Einige Beispiele:


• Keine Aggregation: Die Berechnung bestehe lediglich aus einer Projek-
tion. Aus jedem Personendatensatz aj soll das Geburtsjahr yj = f(aj)
extrahiert werden. Da keine Aggregation erfolgt, ist das Ergebnis eine
Multimenge von Geburtsjahren y 1, . . . , yj = f(a 1, . . . , aj). Bei der { }
Operation handelt es sich um die Multimengen-Vereinigung , die
Funktion f◦∗ fungiert wie f, jedoch mit negativen Häufigkeiten⊎ . Das
neutrale Element ist die leere Multimenge: f f∗ = = e. ◦ ∅


FOOTNOTE:• S uu mm dm ieen Abi dld du itn ig o: nIs +t uf n(a d1, e. s. g. i, la tj f) ∗= =P1i∈a f1, ....,aj i, handelt es sich bei ◦ − ·


• Z mä ih tl ie sn t: af u( ca h1, h. i. e. r, aj d) ie= APddi∈ ita i1 o,. n..,a uj n1 dis ft ∗e =in Sp 1ez fia . lfall der Summe, so- ◦ − ·


Weitere Funktionen wie die Durchschnittsberechnung oder die Summen-
bildung im Anschluss an eine Projektion unter der Bildung von Gruppie-
rungen stellen Spezialformen der drei genannten Berechnungen dar und
können durch ihre Kombination dargestellt werden. Dabei gelten die im
vorherigen Abschnitt genannten Einschränkungen. Hier ist nochmals zu
erwähnen, dass es zur Berechnung von Minima und Maxima nicht ohne
Weiteres möglich ist, f∗ zu definieren.


Bei der eigentlichen Berechnung eines Transformationsergebnisses un-
ter Verwendung der sich seit der letzten Berechnung geänderten Basisda-
ten sowie des vorherigen Ergebnisses gibt es zwei Alternativen, wann das
vorherige Ergebnis gelesen wird. Einmal vor dem Beginn der eigentlichen
Berechnung, oder alternativ kurz vor dem Schreiben des Ergebnisses. Bei
ersterem handelt es sich um die sogenannte Overwrite-Installation, bei zwei-
terem um die Increment-Installation [Jör+11b]. Im Folgenden erläutern wir
die Alternativen kurz und weisen auf deren Vor- und Nachteile sowie de-
ren Einschränkungen hin.


Overwrite-Installation Bei der Overwrite-Installation wird das Ergebnis
der vorherigen Berechnung vollständig eingelesen und in eine geeignete
Form gebracht, sodass es direkt mit den eigentlichen Basisdaten aggregiert
werden kann. Nehmen wir als Beispiel die Berechnung der Gehaltsumme
pro Firma. Aus dem Ergebnisdatensatz (SAP, 1500000) wird ein anony-
mer Personen-Eingabedatensatz erzeugt mit der Firma SAP und dem Ge-
halt 1500000. Zu Beginn der Berechnung - im gegebenen Beispiel also vor
der Summenbildung - liegen alle zur Ermittlung des Endergebnisses not-
wendigen Daten vor. Das Endergebnis ist vollständig und beinhaltet alle
Endwerte, auch solche, die sich nicht geändert haben. Die Variante trägt
den Namen Overwrite, weil üblicherweise das vorherige Ergebnis vollstän-
dig überschrieben wird. Alternativ könnte man es aber auch in ein sepa-
rates Ausgabeziel schreiben lassen. Ein Nachteil der Overwrite-Installation
ist, dass das komplette vorherige Ergebnis eingelesen werden muss, selbst
wenn daran nur sehr wenige Änderungen vorgenommen werden. Der Vor-
teil ist die universelle Anwendbarkeit und eine hohe Performanz, wenn es
viele Änderungen gibt [Sch+14].


Increment-Installation Wie der Name schon sagt, beinhaltet diese Vari-
ante eine Inkrement-Operation. Sie wird meist für die Berechnung von Sum-
me und Anzahl verwendet. Theoretisch ist das Verfahren aber auch auf
andere Analysen übertragbar. Die Berechnung läuft unter dieser Varian-
te so ab, dass sie nur auf den Änderungen in den Basisdaten basiert. Das
Ergebnis der Berechnung ist also nicht final, sondern stellt nur eine abso-
lute Änderung dar, etwa (SAP, 17000), weil entweder das Gehalt einer −
bei SAP arbeitenden Person gekürzt wurde oder weil eine ebensolche Per-
son die Firma gewechselt hat oder ganz gelöscht wurde. Dieses Ergebnis
darf natürlich nicht wie bei der Overwrite-Installation einfach die Werte
in der Datenbank überschreiben, sondern es muss eine Art „Plus-Gleich“
(+=) durchgeführt werden. Das Schreiben läuft also so ab, dass zunächst
der vorherige Ergebniswert gelesen werden muss, dann werden die bei-
den vorliegenden Zahlen addiert und schließlich erfolgt das Zurückschrei-
ben der Summe. Der Vorteil dieser Variante ist, dass nur diejenigen Ergeb-
nisdatensätze gelesen werden müssen, bei denen eine Änderung zu erfol-
gen hat. Alle anderen bleiben unberührt. Der Nachteil dieser Variante ist,
dass die Inkrement-Operation meist sehr kostspielig ist. Während bei der
Overwrite-Installation das vorherige Ergebnis sequenziell auf einen Schlag
schnell gelesen und zum Ende wieder auf sequenzielle Art als Gesamtes
ebenso schnell geschrieben werden kann, handelt es sich bei der Inkrement-
Operation um einen wahlfreien Zugriff auf die Daten. Der zu ändernde
Datensatz muss also zunächst gefunden, dann gelesen und letztendlich zu-
rückgeschrieben werden. Bei sehr vielen Änderungen kann dieser Prozess
deutlich langsamer sein als bei der Overwrite-Installation oder als bei einer
vollständigen Neuberechnung.


Bei allen Varianten der inkrementellen Berechnung ist natürlich zu be-
achten, dass sie nur durchgeführt werden können, wenn auf vorherige End-
ergebnisse lesend zugegriffen werden kann. Erfolgt das Schreiben der Da-
ten in ein Ziel ohne Leseberechtigung oder in ein Write-only-Datenziel, sind
inkrementelle Berechnungen nicht durchführbar. Ein Beispiel dazu ist eine
Analyse, deren Ergebnisse per E-Mail versendet werden (es existiert kein
Gesendet-Ordner) oder deren Ergebnisse mittels Text-to-Speech aus einem
Lautsprecher ertönen.


2.4 Datenintegration


In diesem Abschnitt geht es um Vorgänge, bei denen Daten aus mehreren
Quellsystemen in einem gemeinsamen Zielsystem integriert werden sollen.
Integration bedeutet in diesem Falle, dass es nicht damit getan ist, die Da-
ten lediglich eins zu eins zu kopieren, sondern dass zur Gewährung der In-
tegrität komplexere Schritte nötig sind. Im Allgemeinen geht es dabei um
die Überwindung von Heterogenität [LN07]. Werden unterschiedliche Soft-
waresysteme zur Datenspeicherung im Quell- und Zielsystem eingesetzt,
unterscheiden diese sich typischerweise in ihren Zugriffsmethoden und in
ihren Datenmodellen. Geeignete Datenbank-Middleware kann dazu einge-
setzt werden, diese sogenannte technische Heterogenität und die Datenmodell-
Heterogenität zu überwinden. Anderen Formen von Heterogenität resultie-
ren aus der Autonomie beim Schema-Design. Verschiedene Datenbankent-
wickler modellieren die gleichen Reale-Welt-Konzepte mit anderen Daten-
modellkonzepten oder unter der Verwendung unterschiedlicher Termino-
logien und Datentypen. Im Folgenden gehen wir auf insgesamt sechs Arten
von Heterogenität ein [LN07, Kapitel 3]. Weiter unten werden zwei Mög-
lichkeiten zur Datenintegration vorgestellt und diskutiert, nämlich die ma-
terialisierte und die virtuelle Integration.


Formen von Heterogenität


Technische Heterogenität Betrachtet man zwei verschiedene Datenbank-
managementsysteme, bieten diese in der Regel zwei unterschiedliche Zu-
griffsschnittstellen (APIs). Dazu zählen Methoden zum Verbindungsauf-
bau, zum Lesen und Schreiben von Daten sowie zur Steuerung von Trans-
aktionen. In relationalen Datenbanken kommen zur Überwindung der tech-
nischen Heterogenität Datenbank-Gateways wie zum Beispiel JDBC (Java Da-
tabase Connectivity) zum Einsatz. Diese machen es Entwicklern möglich,
mit gewohnten Methoden auf ein fremdes Datenbankmangementsystem
zuzugreifen, ohne erst eine neue API zu verstehen. Des Weiteren kann in ei-
ner Anwendung schnell von einem System auf ein anderes gewechselt wer-
den, indem lediglich ein anderer Datenbanktreiber zum Einsatz kommt.
Bei NoSQL-Datenbanken kommen DB-Gateways aktuell in der Regel noch
nicht zum Einsatz. Das bedeutet, dass jede Interaktion mit der Datenbank


neu formuliert werden muss, wenn von einem System auf ein anderes ge-
wechselt wird. Bei Datenintegrationen ist die Überwindung technischer He-
terogenität meist einfach zu lösen, da dieser Prozess generisch für ein Da-
tenbankmanagementsystem erfolgen kann. Typischerweise erfolgt die Über-
windung mittels der Entwicklung von Wrappern. Ein Wrapper ist eine spe-
ziell für ein System entwickelte Komponente, welches zum Lesen und Schrei-
ben von Daten eingesetzt werden kann. Nach dem Lesen bringt der Wrap-
per die Daten in ein einheitliches internes Format, aus welchem vor dem
Schreiben wieder das Datenbank-spezifische Format erzeugt wird.


Datenmodell-Heterogenität Zwar gibt es viele verschiedene relationale
Datenbankmanagementsysteme, im Großen und Ganzen verwenden sie je-
doch das gleiche Datenmodell: Tabellen, Spalten, Zeilen, etc. Trotzdem gibt
es aber auch hier oft Unterschiede, zum Beispiel die Unterstützung komple-
xer Datentypen. Eine weiter ausgeprägte Form von Datenmodell-Heteroge-
nität liegt allerdings bei der Verwendung zweiter verschiedener Datenbank-
Klassen vor. Dazu betrachte man nur die Definitionen der verschiedenen
Datenmodelle in NoSQL-Datenbanken in Abschnitt 2.1. Bei der Dateninte-
gration kann die Überwindung der Datenmodell-Heterogenität leicht sein,
wenn das Datenmodell im Zielsystem ein Über-Modell des Quellsystem-
Datenmodells darstellt. Als Beispiel können wir die Migration von Doku-
menten in CouchDB in die Graphdatenbank Neo4J betrachten. Da Neo4J
ein JSON-ähnliches Format für die Knotenproperties verwendet, können
einfach alle JSON-Dokumente aus CouchDB als Knoten übernommen wer-
den. In der Rückrichtung ist die Datenintegration äußerst problematisch,
da Kanten - also Beziehungen zwischen Datensätzen - nicht in Dokumen-
tendatenbanken unterstützt werden. Für solche Zwecke muss man sich in-
dividuell eine Abbildung im Ziel-Datenmodell überlegen, beispielsweise
in diesem Fall über einen Array, der Kanten-Labels, -Properties und die IDs
der Zieldatensätze enthält.


Syntaktische Heterogenität Wenn es darum geht, Datenwerte zu spei-
chern, gibt es meist viele alternativen Formen. Ein Kalender-Datum kann
beispielsweise als eine mit Punkten getrennte Folge von Tag, Monat und
Jahr in Form eines Strings gespeichert werden oder aber in Form eines
Unix-Zeitstempels als ganze Zahl. Eine weitere Alternative wäre die Ver-
wendung eines speziellen Datentyps, etwa der SQL-Typ DATE. Andere Fäl-
le syntaktischer Heterogenität treten bei der Verwendung von verschiede-
nen Zeichenkodierungen, Umlaut-Repräsentationen, Zahlenformaten oder
Maßeinheiten auf. Im Allgemeinen lässt sich diese Form der Heterogenität
allerdings leicht mittels einfacher Umrechnungsfunktionen oder regulärer
Ausdrücke überwinden.


Strukturelle Heterogenität Ein Datenbankdesigner muss sich bei seiner
Arbeit stets die Frage stellen, welche Konzepte des Datenmodells für wel-
che Reale-Welt-Konzepte verwendet werden sollen. Verwaltet eine Daten-
bank die Produkte eines Webshops, könnte man in einer relationalen Daten-
bank die Produktkategorie mit dem Datenmodell-Konzept „Spaltenwert“
umsetzen. Eine Spalte in der Produktdatenbank enthielte also in diesem
Fall den Namen der Kategorie, in der sich das Produkt befindet, z. B. "buch".
Eine Alternative wäre die Verwendung des Konzepts „Tabelle“. In diesem
Fall würden alle Produkte der Buch-Kategorie in eine Tabelle buch gespei-
chert. Man beachte, dass in diesem Beispiel die strukturelle Heterogenität
so sehr ausgeprägt ist, dass ein Daten-Metadaten-Konflikt auftritt. In der
ersten Alternative wurde die Kategorie auf Datenebene, in der zweiten auf
Metadatenebene umgesetzt. Um ein Produkt mit einer neuartigen Katego-
rie hinzuzufügen, ist im einen Fall ein einfaches INSERT, im anderen Fall
ein zusätzliches CREATE TABLE vonnöten. In NoSQL-Datenbanken gibt es
aufgrund der Schema-Flexibilität und der Unterstützung komplexer Typen
weitere Ursachen für strukturelle Heterogenität. In MongoDB könnte bei-
spielsweise jede Kategorie ein Dokument sein, welches einen Array aller in
ihr enthaltenen Produkte beinhaltet. Zur Überwindung von struktureller
Heterogenität sind Methoden oder Sprachen notwendig, die es erlauben,
Metadaten in Daten und Daten in Metadaten umzuwandeln.


Schematische Heterogenität Verwendet man zur Umsetzung ein und des
selben Reale-Welt-Konzepts in zwei Implementierungen das gleiche Daten-
modell-Konzept, können trotzdem zwei heterogene Schemata vorliegen. In
diesem Fall sprechen wir von schematischer Heterogenität. Nehmen wir
an, ein Datenbankdesigner entscheidet sich dafür, den Verlagsort eines Bu-
ches in der Produktdatenbank als Spaltenwert zu speichern. In einem Fall
kann dies eine Spalte der Produkttabelle sein, im anderen Fall eine Spal-
te in einer Verlagstabelle, welche über eine Fremdschlüsselbeziehung mit
der Produkttabelle verbunden ist. Letztere Modellierung stellt eine Nor-
malisierung der ersteren dar und vermeidet Redundanzen und Anomalien.
In beiden Fällen ist der Verlagsort auf Datenebene gespeichert. Die Abbil-
dung des einen Schemas in das andere ist mittels einer Verbundoperation
bzw. umgekehrt mit einer Selektion und Projektion möglich. Im Allgemei-
nen kann die schematische Heterogenität meist mit einfachen Operatoren
überwunden werden.


Semantische Heterogenität Während die bisher vorgestellten Formen der
Heterogenität damit zu tun hatten, wie die Konzepte der realen Welt in ei-
nem speziellen Datenbanksystem umgesetzt werden, ist die semantische
Heterogenität eine Folge der Autonomie bei der Verwendung von Namen
und Terminologien. Auf Metadatenebene gilt es beispielsweise zu entschei-
den, ob Attribute deutsche oder englische Namen erhalten, ob Abkürzun-
gen verwendet werden oder ob sie lediglich durchnummeriert werden. Letz-
tere Alternative ist äußerst unelegant, aber erstere Schemata können mittels
Wörterbücher und Thesauri ineinander überführt werden. Homonyme und
Synonyme sorgen dabei allerdings oft für Unklarheiten. Im Allgemeinen
kann man sagen, dass die semantische Heterogenität die am schwierigsten
zu überwindende Form darstellt, da vor allem in automatischen Prozes-
sen schwierig festgestellt werden kann, was sich ein Datenbankdesigner


mit der Vergabe eines Namens gedacht hat. Für die Überwindung wer-
den Schema-Matching-Werkzeuge [MBR01; RB01] eingesetzt, welche Kor-
respondenzen zweier heterogener Schemata erkennen. Ein solches Werk-
zeug betrachtet dabei die Namen der Schemaelemente, die verwendeten
Datentypen und die Struktur, wie die Schemaelemente untereinander an-
geordnet sind. Zusammengesetzte und hybride Schema-Matcher schauen
dabei nicht nur auf ein Kriterium sondern kombinieren verschiedene An-
sätze. Zusätzlich ist eine Untersuchung über das Schema hinweg, also auf
Instanzebene möglich. Mit dieser Methode wäre es sogar möglich, eine Kor-
respondenz zwischen zwei Attributen mit kryptischen Namen herauszu-
finden, wenn sie ähnliche Werte, z. B. Produktkategorien oder Personenna-
men, enthalten.


Materialisierte und virtuelle Integration


Die weiter oben beschriebenen periodisch ausgeführten Datentransforma-
tionen und Analysen mit den dazugehörigen Änderungserfassungsmetho-
den entsprechen der materialisierten Integration [LN07, Kapitel 4]. Materiali-
siert bedeutet, dass das Ergebnis tatsächlich in einer Datenbank gespeichert
wird. Die Alternative dazu ist die virtuelle Integration, bei der das Ergebnis
lediglich eine Sicht auf die Basisdaten darstellt. Handelt es sich bei der Inte-
gration um eine Aufgabe, die nur ein einziges mal ausgeführt werden muss,
beispielsweise die Übernahme von Kundendaten in ein neues System, ver-
wendet man dazu die materialisierte Integration. Ändern sich jedoch die
Basisdaten ständig und es ist vonnöten, dass die Änderungen im Zielsys-
tem sofort sichtbar sind, kann eine virtuelle Integration sinnvoll sein.


In einem Data-Warehouse findet man die materialisierte Integrationen
in der Form von ETL-Jobs wieder. Die drei Phasen des Extract-Transform-
Load bestehen daraus, Daten aus einem Basissystem zu extrahieren, diese
so zu transformieren, dass sie sich für typische Datenanalyseaufgaben eig-
nen, und das Ergebnis schließlich ins Datenwarenhaus zu laden. Temporä-
re Zwischenergebnisse befinden sich in der Staging-Area, auf welche in al-
len drei ETL-Phasen lesend und schreibend zugegriffen werden kann. Die
Eingabedaten kommen aus verschiedenen Quellen, bei einem Online-Shop
beispielsweise Produktdaten, Einkaufsstatistiken und Webseiten-Besucher-
Logs. Komplexe Analysen, etwa um Einkaufsverhalten vorherzusagen oder
Berichte zu erzeugen, dauern oft mehrere Minuten oder Stunden und wer-
den deshalb üblicherweise nicht direkt auf den Basisdaten ausgeführt. Statt-
dessen erfolgt in periodischen Zeitintervallen die Integration der Daten in
das Datenwarenhaus, auf welches anschließend zu Analysezwecken nur
lesend zugriffen wird. Da im Transform-Schritt die Daten bereits vorag-
gregiert und mit Join-Operationen denormalisiert werden können, sind die
letztendlichen Analysen deutlich effizienter, als wenn man sie direkt auf
den Basisdaten ausführen würde. Des Weiteren beeinflussen sie die Ver-
fügbarkeit und Performanz des eigentlichen Systems nicht. Bei direkten
Datenanalysen würden lang-gehaltene Sperren zu einer drastischen Ver-
langsamung parallel laufender Transaktionen führen. Die Ausführung des
ETL, also des Vorgangs, der nötig ist, um die Daten ins Datenwarenhaus zu
bekommen, dauert allerdings je nach Datenmenge, Wahl des Änderungs-
erfassungsverfahrens und Ausmaß an Änderungen seit einer vorherigen


Ausführung eine gewisse Zeit. Um die Daten im Datenwarenhaus aktuell
zu halten sind wiederholte Ausführungen vonnöten.


Bei virtuellen Integrationen liegen die Daten stets im aktuellen Zustand
vor. Stale Data, also veraltete Datenwerte, können aufgrund der direkten
Weiterleitung der Leseaktionen zu den Basisdaten nicht auftreten. Des Wei-
teren sind keine ETL-Jobs vonnöten, die über den kompletten Datenbe-
stand iterieren, und es wird kein zusätzlicher Speicherbedarf benötigt. Eine
virtuelle Integration liegt als eine Art „Kochrezept“ vor. Wenn eine Anwen-
dung einen Datensatz im integrierten Schema anfordert, wird das Rezept
gemäß der Anfrage angepasst und schließlich direkt auf den Basisdaten
ausgeführt. Bereits durch einfache Optimierungen wie die Ausführung ei-
nes Predicate-Pushdows, also der Filterung der Daten direkt an der Quelle,
können einfache virtuelle Transformationen sehr effizient ausgeführt wer-
den. Allerdings sind komplexen Analysen, in denen große Datenmengen
gelesen und aggregiert werden, bei der virtuellen Integration deutlich lang-
samer als bei der materialisierten.


Transformationssprachen für N3oSQL


Anders als bei relationalen Datenbanken gibt es in der Welt heterogener
NoSQL-Datenbanken keine einheitliche Anfragesprache. Die im vorheri-
gen Kapitel vorgestellten Programme und Frameworks machen es mög-
lich, komplexe Analysen effizient und verteilt auszuführen, jedoch ist die
Verwendung meist aufwendig und mit einer gewissen Einarbeitungszeit
verbunden. Die Alternative zum Schreiben einer Analyse in einer Program-
miersprache wie Java ist die Verwendung einer speziellen Anfrage- und
Transformationssprache, wie sie in diesem Kapitel vorgestellt werden.


3.1 SQL-basierte Sprachen


Die Structured Query Language SQL ist mächtig und wird von einer großen
Benutzergruppe beherrscht. Auch in der Programmierung unerfahrene An-
wender sind oft im Formulieren von SQL-Anfragen auf relationalen Daten-
banken geübt. Deshalb verfolgen viele Ansätze die Idee, die Sprache SQL
als Basis zu nehmen und sie so zu erweitern, dass sie zur Definition von
Anfragen auf NoSQL-Datenbanken genutzt werden kann. Dadurch wird
es möglich, komplexe Anfragen in kurzen und leicht verständlichen SQL-
Statements zu formulieren, ohne dass sich der Anwender um die Optimie-
rung und die verteilte Ausführung mittels eines Datenverarbeitungsframe-
works kümmern muss. Auf der anderen Seite ist die Sprache SQL jedoch
stark für relationale Datenbanken optimiert. Es gibt also keine einfache Un-
terstützung für flexible Schemata oder komplexe Datentypen. Die im Fol-
genden vorgestellten Ansätze versuchen dieses Manko teilweise durch die
Erweiterung der Sprache zu kompensieren.


Hive Apache Hive [Thu+09] ist eine Data-Warehouse-Erweiterung für Ha-
doop [Apaa]. Ursprünglich wurde sie von Facebook entwickelt und im Jah-
re 2008 der Open-Source-Gemeinde zur Verfügung gestellt [Hiv]. Die Spra-
che HiveQL, die den SQL’92-Standard zwar nicht vollständig erfüllt, sich
aber sehr stark daran orientiert und ihn teilweise auch erweitert, ermög-
licht es, relationale Tabellen anzulegen, welche in dem verteilten Dateisys-
tem HDFS (Hadoop Distributed File System) gespeichert werden. Die SQL-
ähnlichen Anfragen werden in der ursprünglichen Hive-Implementierung
nach einer Optimierung in eine Folge von MapReduce-Jobs umgewandelt
und schließlich mittels des Hadoop MapReduce Frameworks ausgeführt.
Einfache Selektionsanfragen resultieren in einem einzigen Map-Job. An-
fragen, in denen zwei Tabellen über eine Joinoperation miteinander ver-
bunden wurden, werden zu einem Job mit einer Map- und einer Reduce-
Phase. Kommen zusätzliche Gruppierungen, Aggregationen, Sortierungen
oder weitere Verbünde hinzu, kann die HiveQL-Anfrage nicht mehr als
ein einziger MapReduce-Job durchgeführt werden, sondern als eine Ver-
kettung mehrerer solcher. Durch die Unterstützung sogenannter Engines
ist es möglich, TEZ [Sah+15] oder Spark [Zah+10] anstelle von MapReduce
einzusetzen. Zur Speicherung der Tabellendaten unterstützt Hive mehrere
Dateiformate, unter anderem CSV-Textdateien sowie das optimierte ORC-
Format (Optimized Row Columnar [Orc]). In letzterem werden beispielswei-
se Tabellendaten in sogenannte Stripes aufgeteilt und innerhalb jedes Stri-
pes werden die kleinsten und größten Werte zu jeder Spalte vermerkt. Dies
ermöglicht ein effizienteres Durchsuchen. Zusätzlich zu den nativen Spei-
cherformaten werden in Hive auch externe Tabellen unterstützt, also Tabel-
len, deren Daten in einem fremden System liegen. Dies kann zum Beispiele
eine HBase-Tabelle sein, was es möglich macht, SQL-Anfragen an existie-
rende HBase-Tabellen zu stellen [Hiv]. Allerdings muss die HBase-Tabelle
ein festes Schema besitzen, welches man beim Erstellen der Hive-Tabelle
zusammen mit den Datentypen der einzelnen Spalten definieren muss.


Impala Da Hive ursprünglich für die Ausführung mittels MapReduce
entwickelt wurde und in Verbindung mit HBase sehr inperformant war,
entstanden weitere Systeme, die ebenfalls den Gedanken verfolgten, Da-
tenmengen verteilt zu speichern und per SQL abrufbar zu machen. Die von
der Firma Cloudera entwickelte Datenbank-Engine Apache Impala [Kor+16]
baut – genau wie Hive – auf den Hadoop-Technologien auf, also dem ver-
teilten Dateisystem HDFS sowie dem Wide-Column Store HBase. Aller-
dings werden die an Impala gestellten SQL-Anfragen nicht zuerst in schwer-
gewichtige MapReduce-Anfragen gewandelt, sondern direkt von Impala-
Prozessen, die auf jedem Rechnerknoten installiert sind, verarbeitet. Impa-
la unterstützt nahezu alle Eigenschaften des SQL’92-Standards sowie eini-
ge analytische Erweiterungen aus SQL’2003 und benutzerdefinierte exter-
ne Funktionen. Da Impala seine Daten üblicherweise im HDFS speichert,
ist es hauptsächlich dazu gedacht, einmal geladene große Datenmengen zu
analysieren, und nicht um Änderungen oder Löschungen auf diesen aus-
zuführen.


