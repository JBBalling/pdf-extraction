Fakultät für Mathematik und Wirtschaftswissenschaften
Institut für Numerische Mathematik


Segmentierung und Bewertung
korrosionskritischer Konstruktionen einer
Gesamtfahrzeugstruktur mittels Methoden
der künstlichen Intelligenz


Dissertationsschrift zur Erlangung des Doktorgrades Dr.rer.nat.
der Fakultät Mathematik und Wirtschaftswissenschaften
der Universität Ulm


Vorgelegt von:


Ludwig Waibel
aus Memmingen


Prof. Dr. Stefan Funken
Prof. Dr. Stefan Funken
Prof. Dr. Friedhelm Schwenker
Dr. Andreas Mittelbach
30.06.2022


Amtierender Dekan:
1. Gutachter:
2. Gutachter:
Betreuer Mercedes-Benz AG:
Tag der Promotion:


(Teil-)Ergebnisse zur Kanten- und Flanschsegmentierung sind in L. Waibel, A.
Mittelbach und S. Funken. „Semantic segmentation of corrosive critical designs
in body-in-white structures for corrosion simulation“. In: Materials and Corrosion
72.5 (2021). doi: 10.1002/maco.202012134 veröffentlicht.


(Teil-)Ergebnisse zur Kantenbewertung sind in L. Waibel et al. „Edge delamina-
tion width prediction of 3D body-in-white part by finite element-based corrosion
simulation and neural networks“. In: Materials and Corrosion 73.1 (2022). doi:
10.1002/maco.202112637 veröffentlicht.


Einleitung


Der Prozess der Korrosion beschreibt die allmähliche Zersetzung von metallischen
Materialien durch elektrochemische Reaktionen mit der Umgebung. Dabei verur-
sacht die Umwandlung der Materialien allein in den USA einen direkten jährli-
chen Schaden von 3.1% des Bruttoinlandsprodukts, wobei der Transportsektor für
mehr als ein fünftel verantwortlich ist. [5, 108] Neben ökonomischen Gesichtspunk-
ten ist eine Absicherung hinsichtlich Korrosion aus Gründen der Nachhaltigkeit,
Sicherheit, Qualität, Garantieanforderung und Ästhetik auch im modernen Auto-
mobilbau der Mercedes-Benz AG unerlässlich.


Um in Zeiten immer kürzerer Produktzyklen und zunehmenden Kostendrucks
einen langlebigen und qualitativ hochwertigen Korrosionsschutz entwickeln zu kön-
nen, werden vermehrt digitale Modelle eingesetzt, die die Experten unterstützen.
Insbesondere Methoden aus dem Bereich der künstlichen Intelligenz (KI) stellen
einen vielversprechenden Weg dar, um die damit verbundenen Herausforderungen
zu bewältigen, da diese sich durch ein menschenähnliches Vorgehen auszeichnen.
So wird beispielsweise durch ein sogenanntes Neuronales Netz (NN) die Funkti-
onsweise eines Gehirns modelliert. Eine weitere Parallele zeigt sich durch die Art
und Weise des Wissenserwerbs, denn sowohl die menschliche als auch die künstli-
che Intelligenz benötigen ein gewisses Training, um die gewünschten Resultate zu
erzielen. Das heißt, dass anhand von Beispielen, die auch zum Teil inkonsistent
sein können, ein Vorgehen gelernt und auf neue unbekannte Instanzen angewandt
wird. Mittlerweile erreichen problemspezifische Methoden der KI, beispielsweise
in der Bilderkennung [165], Fehlerraten auf menschlichem Niveau.


Diese Methoden versprechen somit die Realisierung einer vollautomatisierten, di-
gitalen, zuverlässigen und schnellen Umsetzung einer Vielzahl an korrosionsbe-
zogener Aufgabenstellungen, wodurch die Arbeit der Korrosionsschutzexperten
unterstützt, beschleunigt, standardisiert und objektiviert wird, was wiederum zur
Sicherung der internationalen Wettbewerbsfähigkeit der Mercedes-Benz AG bei-
trägt. Aus diesem Grund beschäftigt sich die vorliegende Promotionsarbeit mit
dem Einsatz von KI Methoden im Kontext des Korrosionsschutzes.


Korrosionskritische Konstruktionen


Bei der Mercedes-Benz AG werden momentan zur korrosiven Absicherung Fahr-
zeugprototypen einerseits Felderprobungen auf der Straße und andererseits ag-
gressiven Testumgebungen in speziellen Klimakammern unterzogen. Die Korrosi-
onsschutzexperten identifizieren vor, während und nach den Tests kritische Bau-
teilbereiche und definieren gegebenenfalls geeignete Korrosionsschutzmaßnahmen.
Die durchgeführten Tests zeigen, dass gewisse Bauteilbereiche und spezielle Kon-
struktionen wesentlich häufiger von Korrosion betroffen sind als andere.


Abbildung 0.1: Kanten- (rot) und Flanschsegmentierung (gelb) eines GLAs.


Diese korrosionskritischen Konstruktionen bestehen aus Bauteilkanten und Flan-
schen, wobei ein Flansch eine nicht zerstörungsfrei lösbare Verbindung zwischen
Rohbauteilen bezeichnet. Diese Konstruktionen werden aktuell aufwendig von den
Korrosionsschutzexperten manuell herausgefiltert und analysiert, weshalb zur Un-
terstützung eine neuartige Kanten- und Flanschsegmentierung für Fahrzeugkaros-
serien entwickelt wird. Die Segmentierung erlaubt eine Reduktion der Gesamt-
daten auf korrosionskritische Konstruktionen, was den bisherigen Analyseprozess
beschleunigt. Beispielsweise wird so die zu prüfende Oberfläche eines GLAs (Ab-
bildung 0.1) um circa dreiviertel reduziert.


Zur Kanten- und Flanschsegmentierung erweisen sich Methoden der KI als be-
sonders geeignet, nachdem korrosionskritische Konstruktionen nicht eindeutig de-
finiert sind und diese Algorithmen auch anhand inkonsistenter Daten lernen kön-
nen. Aus diesem Grund werden insbesondere KI Methoden zur Umsetzung der
Kanten- und Flanschsegmentierung in dieser Arbeit untersucht.


Korrosionsrisiko


Die beschriebene Hardware basierte Absicherung der Rohkarosserie hinsichtlich
Korrosion erfordert das physische Vorhandensein der Prototypen, langwierige und
kostspielige Tests sowie entsprechendes Equipment und geschultes Personal. Zur
Reduktion der Kosten- und Zeitfaktoren wird im Zuge der Digitalisierung verstärkt
auf qualitative und quantitative Simulationsergebnisse zurückgegriffen. In den Ar-
beiten [18, 65, 101, 212] werden dazu Kanten- und Flansch-Korrosionssimulationen
auf Basis der Finite Elemente Methode (FEM) beschrieben, die das Korrosions-
verhalten über einen längeren Zeitraum stabil auf kleinen zweidimensionalen Geo-
metrien in Abhängigkeit der räumlichen Lage und verschiedenen Klimaten model-
lieren. Die Simulationsergebnisse werden durch entsprechende Korrosionstests an
Probeblechen validiert.


Eine Korrosionssimulation einer gesamten Fahrzeugkarosserie ist durch die naive
Skalierung dieser Modelle allerdings nicht möglich, da dies rein rechnerisch einen
Speicherbedarf in der Größenordnung von mehreren Exabyte (= 1018 B = 109 GB)
ergibt. Dieser immense Speicherbedarf und die damit verbundenen hohen Rechen-
zeiten führen zu einer nicht durchführbaren, bzw. unpraktikablen Simulation.


Durch die entwickelte Kantensegmentierung für Fahrzeugkarosserien wird der zu
untersuchende Bereich der Bauteile auf Stellen mit potentiellem Korrosionsrisiko
reduziert. Diese Datenreduktion ermöglicht die Bestimmung eines Korrosionsrisi-
kos der identifizierten Bauteilkanten durch die FEM Simulationen. Da diese auf
Basis der Korrosionserscheinungen an Prüfblechen entwickelt werden, wird die
Anwendung an einem generischen Bauteil validiert. (Abbildung 0.2, Punkt 1 und
2) Um die Bestimmung des Korrosionsrisikos der Bauteilkanten zu beschleunigen,
werden verschiedene Arten von NNs mit den Simulationsergebnissen trainiert.
(Abbildung 0.2, Punkt 3) Das trainierte NN wird dann auf das Rohbauteil an-
gewendet, wodurch sich eine enorme Zeiteinsprung verglichen mit der Simulation
ergibt, da keine Simulationen durchgefürt werden, sondern nur zwischen Simula-
tionsergebnissen interpoliert wird. (Abbildung 0.2, Punkt 4)


Zur Bestimmung der Kantenkorrosion einer Bauteilkonsole ergeben sich beispiels-
weise folgende Kennwerte. Die naive Skalierung der Kanten-Korrosionssimulation
benötigt rein rechnerisch einen Speicherbedarf von 495.9 1012 B bei entsprechend
· hoher Berechnungszeit und ist somit für die industrielle Anwendung nicht ein-
setzbar. Wird zunächst die in dieser Arbeit entwickelte Kantensegmentierung ein-
gesetzt, so ergeben sich einerseits 32 h für die Anwendung der FEM basierten
Korrosionssimulation und andererseits 1.5 s für die Korrosionsprognose durch ein
trainiertes NN.1


1Auf einer Workstation mit Intel i7-8700K, 64GB RAM und NVIDIA Quadro P2000.


Abbildung 0.2: Auswertung des experimentellen, simulierten und vorhergesag-
ten Korrosionsrisikos. Durchgeführte Auswertungen sind dick ge-
druckt.$$$1$$$


Zielsetzung


Aus den obengenannten Punkten leitet sich folgende Forschungsfrage ab, die im
Verlauf der Promotionsarbeit beantwortet wird:


Wie können korrosionskritische Konstruktionen einer Gesamtfahrzeugkarosserie
mittels Methoden der KI segmentiert und Korrosionsrisiken prognostiziert wer-
den?


Die Ziele dieser Dissertation sind dabei insbesondere:


i) Entwicklung von Methoden zur effizienten und exakten Segmentierung von
korrosionskritischen Konstruktionen auf Basis der Geometriedaten


ii) Entwicklung einer schnellen und genauen Korrosionsprognose unter Berück-
sichtigung der Bauteilgeometrie, der räumlichen Lage und verschiedener Kli-
maeinflüsse


iii) Implementierung dieser Methoden in geeignete KI-Anwendungen


1Modifiziert nach [203] mit freundlicher Genehmigung von John Wiley & Sons Inc.


Einleitung


Gliederung


Im ersten Kapitel wird der Begriff der KI definiert und auf Standardmethoden
sowie deren Bewertung und Optimierung eingegangen. Anschließend werden be-
kannte Methoden zur dreidimensionalen Geometrieanalyse und die Grundlagen
der Korrosionstheorie zusammengefasst.


Das zweite Kapitel bildet den Schwerpunkt der Arbeit und behandelt die Kanten-
und Flanschsegmentierung von Gesamtfahrzeugstrukturen. Um eine effiziente und
korrekte Segmentierung gewährleisten zu können, werden einige Anforderungen
an die einzelnen Geometrien bzw. den gesamten Datensatz gestellt und gegebe-
nenfalls Fehler korrigiert. Des Weiteren werden Kanten- und Flansch-spezifische
Eigenschaften beschrieben, die aus den Geometriedaten extrahiert werden. Die-
se dienen als Ausgangspunkt für deterministische und KI basierte Analysen, die
entsprechend ausgewertet, optimiert und miteinander verglichen werden. Abschlie-
ßend wird dargestellt, wie die entwickelten Methoden zur Segmentierung von Ge-
samtfahrzeugstrukturen verwendet werden.


Kapitel drei behandelt die Bestimmung des Korrosionsrisikos an den zuvor identifi-
zierten korrosionskritischen Konstruktionen. Dazu wird zunächst ein Validierungs-
schema beschrieben, sodass sichergestellt wird, dass die von der Mercedes-Benz
AG entwickelten Kanten-Korrosionssimulationen von Prüfblechen auf Rohbauteile
übertragen werden können. Im nächsten Schritt wird erläutert, wie verschiedene
NN Typen mit den Simulationsergebnissen trainiert und anschließend zur Pro-
gnose der Kantenkorrosion von Rohbauteilen verwendet werden. Außerdem wird
der Einsatz von KI Methoden zur Reproduktion der Simulationsergebnisse der
Flanschkorrosion dargelegt.


Das vierte Kapitel geht auf Kennwerte des Datenimports, der Korrektur sowie
den verwendeten markierten Datensatz zum Training der unterschiedlichen Ana-
lysemethoden ein. Darüber hinaus werden die Prognosegüten der verschiedenen
deterministischen und KI basierten Methoden zur Kanten- und Flanschsegmen-
tierung miteinander verglichen. Nachdem sich die Kanten-Korrosionssimulationen
auf Rohbauteile übertragen lassen und die Simulationsresultate durch die verschie-
denen NN Typen mit geringem Fehler reproduziert werden, ist eine sehr schnelle
Korrosionsprognose realisierbar. Auch die Simulationsergebnisse der Flanschkor-
rosion können mit Methoden der KI mit entsprechenden Trainingsdaten wieder-
gegeben werden.


1 Aktueller Kenntnisstand


1.1 Künstliche Intelligenz


1.1.1 Arten künstlicher Intelligenz


Unter Methoden der künstlichen Intelligenz (KI) werden Systeme verstanden, die
menschlich oder rational denken bzw. handeln. [166] Im Gegensatz zu determinis-
tischen Ansätzen, die Probleme durch eine Liste formaler, mathematischer Regeln
beschreiben, lernen KI Algorithmen aus Erfahrungen bzw. Daten. Dazu werden
einfache Lösungskonzepte hierarchisch zusammengeschlossen, um komplexe Pro-
bleme zu lösen. Wissensbasierte KI leitet basierend auf simplen Wissen komplexere
Aussagen logisch ab. Wird hingegen das Wissen selbständig von der KI durch das
Extrahieren von Mustern aus den Daten erlangt, so wird diese den Machine Lear-
ning (ML) Ansätzen zugeordnet. Die klassischen ML Verfahren analysieren dabei
manuell ausgewählte bzw. extrahierte Daten, die auch Features genannt werden.
Im Unterschied dazu lernen Deep Learning (DL) Methoden Features selbstständig
aus Rohdaten zu aggregieren. Insofern bilden DL Methoden eine Untermenge der
ML Ansätze, die wiederum eine Untermenge der KI sind. [71]


Des Weiteren wird zwischen überwachten (engl. supervised), semi-überwachten
und unüberwachten Methoden unterschieden. Für überwachte Methoden werden
den Features Klassen bzw. Werte zugeordnet, je nachdem ob diese zur Klassifika-
tion oder zur Regression verwendet werden. [13, 191]


Semi-supervised Verfahren [223] benutzen viele ungelabelte Instanzen zusammen
mit wenigen gelabelten und können deshalb als supervised Verfahren mit kleiner
Trainingsmenge betrachtet werden. [207]


Unsupervised Methoden arbeiten ohne vorgegebene Klassen und fassen beispiels-
weise ähnliche Features zu Clustern zusammen. [13, 166]


1.1.2 Features, Labels und Regressionswerte


1.1.2.1 Notationen


Die Modellparameter ω klassischer überwachter ML Methoden zur Klassifikation
und Regression werden auf Basis der Trainingsmenge bestehend aus den Feature-
daten RnT×nf zusammen mit den Labels ZnT oder den Regressionswerten
F ∈ L ∈ RnT ermittelt bzw. trainiert. Dabei bezeichnet n f die Anzahl der verschie-
R ∈ denen Features und n die Anzahl der Trainingsinstanzen. Jede Trainingsinstanz, T
die den Featurevektor und das Label (f , (cid:96) ) , (cid:96) Z bzw. den Regressionswert j j
∈ (f , r ) , r R, j = 1, . . . , n umfasst, wird in die Anpassung des Modells in- j j T
∈ volviert. Das trainierte Modell wird dann genutzt, um für neue unbekannte Tes-
tinstanzen f das Label l bzw. den Wert r , i = 1, . . . , n zu bestimmen. Dabei i i i A
bezeichnet n die Anzahl der Testinstanzen. A


1.1.2.2 Feature Normalisierung


Um die Leistung eines Modells zu steigern ist es oftmals von Vorteil die Features
vor der Analyse zu normalisieren. Dadurch wird sichergestellt, dass die Einfluss-
größe jedes Features auf die zu optimierende Zielfunktion homogen ist. [13, 191]
Der Erwartungswert µ und die Varianz σ2 der n -elementigen Trainingsmenge k k T
des k-ten Features werden durch


berechnet. Standardisierte Features werden mittels


bestimmt, wobei die linear-transformierten Elemente f˜ des Trainingssets einen k
Erwartungswert von 0 und eine Standardabweichung von 1 besitzen. [12, 191]
Eine nicht-lineare Skalierung ergibt sich durch die sogenannte Softmax-Normali-
sierung [191] mit einem vorinitialisierten positiven Skalierungsfaktor β > 0


wodurch die Werte f¯ im Intervall (0, 1) liegen. Durch diese Normalisierung wer- jk
den Werte, die stark vom Mittelwert abweichen, stärker beeinflusst. [191]
Der Mittelwert und die Varianz in Gleichung (1.1) werden nur über der Trainings-
menge ermittelt, da diese Eigenschaften im Allgemeinen für neue Daten unbekannt
sind. Dennoch werden auch die Features der Testdaten f über Gleichung (1.2) oder i
Gleichung (1.3) mit den bekannten Werten für µ und σ transformiert, damit alle k k
Features die gleiche Normalisierung erfahren. [13, 191]


1.1.2.3 Feature Reduktion


Steht eine große Anzahl n an verschiedenen Features zur Verfügung, die oft- f
mals voneinander abhängen, so kann es hilfreich sein diese vor der eigentlichen
Featureanalyse auf n < n unabhängige zu reduzieren. Durch die Hauptkompo- u f
nentenanalyse [88] wird der Datensatz auf sogenannte Hauptkomponenten redu-
ziert, sodass der Informationsgehalt, also die Varianz des ursprünglichen Datensat-
FOOTNOTE:zes bestmöglich durch die wenigen Hauptkomponenten u , . . . , u approximiert 1 nu
wird. Für den Fall, dass n = 1 gesetzt wird, so werden die originalen Featu- u
res f j Rnf , j = 1, . . . , n T , die hier als Spaltenvektor interpretiert werden auf
∈ ut f R projiziert. Die Varianz der projizierten Daten wird durch ut Σu mit der
1 j ∈ 1 1
Kovarianzmatrix


FOOTNOTE:berechnet. Zusammen mit der Nebenbedingung ut u = 1, zeigt sich, dass die 1 1
FOOTNOTE:projizierte Varianz maximiert wird, sobald Σu = λ u gilt. Die Varianz beträgt 1 1 1
FOOTNOTE:dann ut Σu = λ 0. Das heißt, dass für eine Projektion mit maximalem Infor- 1 1 1
≥ mationsgehalt die Hauptkomponente u dem Eigenvektor von Σ mit dem größten
1
Eigenwert entspricht. Falls n > 1 gewählt wird, so sind die Hauptkomponenten u
FOOTNOTE:u , u . . . , u durch die Eigenvektoren von Σ korrespondierend zu den größten 1 2 nu
Eigenwerten λ λ . . . λ 0 gegeben. [13] 1 ≥ 2 ≥ ≥ nu ≥


Vor der Anwendu≥ng d≥er Hau≥ptkom≥ponentenanalyse werden die Features über Glei-
chung (1.2) standardisiert, da die Varianz der Daten von der Skalierung der Fea-
tures abhängt. [97]


Die Anzahl n wird beispielsweise so gewählt, dass die kumulierte erklärte Vari- u
FOOTNOTE:anz (cid:80)nu λ / (cid:80)nf λ der n Hauptkomponenten einen vorgegebenen Anteil δ k=1 k k=1 k u u
übersteigt. [97]


1.1.3 Methoden der künstlichen Intelligenz


1.1.3.1 k -nächste Nachbarn nn


Fix und Hodges Jr. [58] publizieren eine nicht-parametrische Methode zur Klassi-
fizierung der Entität i durch die k -nächsten Nachbarn (KNN) im Featureraum nn
bzw. deren Labels. Dazu wird bezüglich einer Distanzmetrik d der Abstand des
unbekannten Featurevektors f = f , j 1, . . . , n zu den Trainingsdaten i j T
(cid:54) ∀ ∈ { } (f , (cid:96) ) , j = 1, . . . , n bestimmt. Die Distanzen werden dann aufsteigend sortiert, j j T
sodass d(f , f ) d(f , f ) . . . d(f , f ) gilt. Für ein binäres Klassifika- i j1
≤
i j2
≤ ≤
i jnT
tionsproblem wird k ungerade gewählt, damit die Mehrheitsentscheidung zur nn
Klassifizierung der Entität i durch


eindeutig ist. Dabei bezeichnet


Im Rahmen einer Regression wird der Mittelwert der k -nächsten Nachbarn im nn
Trainingsdatensatz (f , r ) , j = 1, . . . , n durch j j T


ermittelt. Alternativ kann der Mittelwert durch das Inverse der Distanzen gewich-
tet werden. [35, 55]


FOOTNOTE:Als Metrik werden unter anderem die euklidische d (x, y) = x y oder die 2
(cid:107) − (cid:107)2 Mahalanobis Distanz [128]


FOOTNOTE:mit Σ 1 als Inverses der Kovarianzmatrix verwendet. Die Mahalanobis Distanz −
berücksichtigt somit die unterschiedlichen Skalen der Features und deren Korre-
lation. [13, 137, 191]


1.1.3.2 Classification And Regression Trees


Breiman et al. [22] publizieren auf Basis der Algorithmen von Sonquist und Mor-
gan [179] und Messenger und Mandell [141] sogenannte Classification And Re-
gression Trees (CART), also Bäume zur Klassifikation und Regression. Im Fal-
le der Klassifikation werden die Feature-Label-Paare (f , (cid:96) ) , j = 1, . . . , n der j j T
Trainingsmenge rekursiv in einem Top-down-Ansatz analysiert. In Bezug auf eine
binäre Klassifikation werden für jeden Knoten des Baumes mit n Elementen
folgende Schritte durchgeführt: K K


i) Das Element j im Knoten wird dem Kindknoten zugewiesen, falls für l
K K das k-te Feature f δ , j für eine Schranke δ gilt. Andernfalls jk k k
≤ ∈ K werden die Elemente dem Kindknoten zugewiesen. Falls das Feature f r k
K kategorisch ist, so erfolgt die Zuteilung durch f , wobei eine echte jk
∈ A A Untermenge aller Kategorien in darstellt. Für ein geordnetes Feature er-
K geben sich insgesamt n 1 und für kategorische Features 2nK−1 1 mögliche
Aufteilungen. K− −


ii) Für alle möglichen Zerlegungen des Knotens wird jeweils ein Impurity
K Gain
n n


berechnet. Die Impurity I kann beispiKelsweise durcKh den Gini-Index I( ) =
FOOTNOTE:1 (cid:80)1 p2 bestimmt werden, wobei p = 1 (cid:80)nK 1((cid:96) = g, j K ) die
re− lativeg= H0 äug figkeit der Elemente mit Kg lassenK g inj=1 bej zeichnet. ∈ FaK lls ein
K Knoten nur Elemente der gleichen Klasse besitzt, so ist I ( ) = 0. pure pure K K


iii) Das FeatKure fˆ zusammen mit der Schranke δˆ , die den höchsteKn Impurity k k
Gain erzielen, werden verwendet, um den Knoten in die Kindknoten l
K K und aufzuspalten. Für I ( ) = 0 ist keine Aufteilung notwendig. r pure
K K Die Klassenzuweisung eines Knotens erfolgt durch einen Mehrheitsentscheid
bezüglich der Labels der Elemente im Knoten.


Im Gegensatz zur Verwendung von Stoppregeln zur Aufspaltung der Knoten,
wie in den Methoden von Sonquist und Morgan [179] und Messenger und Man-
dell [141], generiert CART zunächst einen Baum mit I ( ) = 0 für alle Blatt- pure
K knoten. Anschließend wird dieser Baum dann auf eine Größe beschnitten, die den
niedrigsten Fehler bezüglich einer Kreuzvalidierung aufweist. Dadurch wird die
Wahrscheinlichkeit einer Unter- bzw. Überanpassung des CART an die Trainings-
daten im Gegensatz zu den Algorithmen [141, 179] reduziert. [125] Die ermittelten
Entscheidungsregeln werden dann auf neue Testdaten angewandt.


Für eine Regression kann die Impurity durch die mittlere quadratische Abwei-
FOOTNOTE:chung (engl. Mean Squared Error (MSE)) I( ) = 1 (cid:80)nK (r r ) bestimmt
FOOTNOTE:werden, wobei dem Knoten der Mittelwert rK = 1
n
FOOTNOTE:(cid:80)K nKj= r1 deµ r− darj in enthalte-
nen Regressionswerten zugewK iesen wird. [22]
µ nK i=j j


Die Normalisierung der Features hat keinen Einfluss auf die Ergebnisse, da die
Schranken δ aus den Elementen im Knoten gewählt und somit entsprechend an-
gepasst werden.


1.1.3.3 Diskriminanzanalyse


Die Diskriminanzanalyse (DA) führt eine Klassifizierung des Featurevektors f i
mithilfe der Entscheidungsregel l = 1 P (l = 1 f ) > P (l = 0 f ) durch. Nach i i i i i
⇔ | | dem Satz von Bayes ist dies äquivalent zu


unter der Annahme, dass P (l = 1) = P (l = 0) gilt. Wenn die Dichten i i


einer multivariaten Normalverteilung entsprechen, ergibt sich aus Gleichung (1.10)
und der Anwendung des natürlichen Logarithmus folgende Bayes-Regel für die
Quadratische Diskriminanzanalyse (QDA)


FOOTNOTE:mit einem Schwellwert ω . Dabei werden Σ und µ auf den Trainingsdaten er- 0 k k
mittelt. [32, 49]


FOOTNOTE:Die Lineare Diskriminanzanalyse (LDA) resultiert aus der Annahme Σ = Σ = Σ, 0 1
FOOTNOTE:wodurch sich Gleichung (1.12) auf l = 1 ωtf > ω reduziert, mit ω = i i 0
FOOTNOTE:⇔ Σ 1 (µ µ ) und ω = 1ωt (µ + µ ) für P (l = 1) = P (l = 0). [49] − 0
−
1 0 2 0 1 i i
Die von Fisher [57] publizierte lineare Diskriminanzanalyse kommt ohne die Ein-
schränkung Σ = Σ = Σ aus und maximiert das Verhältnis der Varianz zwischen 0 1
und innerhalb der Klassen durch ω = (Σ 0 + Σ 1)−1 (µ 0 µ 1). [13, 49] Werden statt
− den eigentlichen Features die über Gleichung (1.2) standardisierten Features ver-
wendet, so hat dies keinen Einfluss auf die Klassifizierung. (Anhang A)


Eine Regularisierung der linearen bzw. der Fisher Diskriminanzanalyse kann durch


FOOTNOTE:realisiert werden, wobei (cid:0)Σ , Σ , . . . , Σ (cid:1)t die Diagonalelemente von Σ sind 1,1 2,2 nf,nf
und I die Einheitsmatrix ist. [61, 76, 181] nf


1.1.3.4 Support Vektor Maschine


Vapnik [194] veröffentlicht eine sogenannte Support Vektor Maschine (SVM), die
eine Hyperebene H = f Rnf ωtf+ω 0 zur Trennung zweier linear separierbaren
{ ∈ | } Mengen = f (cid:96) = g, j = 1, . . . , n , g = 1, 1 findet, die maximalen Abstand g j j T
L { | } − zu den Elementen in und besitzt. Das heißt, es wird das quadratische 1 1
Minimierungsproblem L− L


gelöst. Vektoren f , die (cid:96) (ωtf + ω ) = 1 erfüllen, werden Support Vektoren (SV) j j j 0
genannt. Diese sind ausreichend, um die optimale Wahl von ω als Linearkombina-
FOOTNOTE:tion ω = (cid:80)nT (cid:96) λ f = (cid:80) (cid:96) λ f mit λ 0 darzustellen, wobei λ > 0 nur für j=1 j j j SV j j j j
≥
j
SV gilt. Für neue Daten f ergibt sich dann das Label durch i


FOOTNOTE:für einen SV f und f . [32, 195] 1(cid:48)j ∈ L1 −(cid:48) 1k ∈ L−1


Diese linearen SVMs benötigen eine linear separierbare Trainingsmenge und liefern
auch nur eine lineare Entscheidungsfunktion, wie Abbildung 1.1 veranschaulicht.
Dadurch ergeben sich zwei Problemstellungen:


Falls die Trainingsdaten nicht ohne Fehler linear separiert werden können,
werden Schlupfvariablen ξ 0, j = 1, . . . , n in das Problem aus Glei- j T ≥


Abbildung 1.1: Entscheidungsebene H einer linearen SVM mit maximalem Ab-
stand zwischen den linear separierbaren Klassen und sowie 1 1
FOOTNOTE:den Support Vektoren f , f und f , f L− .L 1 1(cid:48)1 1(cid:48)2 ∈ L1 (cid:48) 11 (cid:48) 12 ∈ L−1


chung (1.14) eingeführt, sodass folgendes Minimierungsproblem


mit einer Konstanten C entsteht. [13, 32, 195]


ii) Finden einer nicht-linearen Entscheidungsfunktion:
Durch das Abbilden der nicht-linear separierbaren Inputs f j Rnf , j =
∈ 1, . . . , n in einen Raum Rm mithilfe einer passenden nicht-linearen Feature T
Map Φ : Rnf Rm können linear separierbare Daten (Φ (f j) , (cid:96) j) erzeugt
→ werden. Die Nebenbedingung in Gleichung (1.14) entspricht in Rm somit
FOOTNOTE:(cid:96) (ωtΦ (f ) + ω ) 1 und durch die Darstellung von ω als Linearkombina- j j 0
≥ tion der Support Vektoren im Raum Rm ergibt sich


FOOTNOTE:ωtΦ (f ) + ω = (cid:88) (cid:96) λ Φ (f )t Φ (f ) + ω = (cid:88) (cid:96) λ K (f , f ) + ω , (1.17) i 0 j j j i 0 j j j i 0
SV SV


wobei die letzte Identität eine Funktion K(f , f ) enthält, die den Satz von j i
1Eigene Abbildung in Anlehnung an [32].


Mercer erfüllt. [32] Durch solche Kernel Funktionen K muss die Funktion Φ
und die Dimension des Bildraums m nicht bekannt sein, solange das inne-
re Produkt des entsprechenden Raums bekannt ist. [34, 191] Beispielsweise
(cid:16) (cid:17) erfüllen radiale Basisfunktionen
K(x, y) = exp
FOOTNOTE:−(cid:107)x σ−2y
FOOTNOTE:(cid:107) oder Polynome
K(x, y) = (xty + 1)q , q > 0 den Satz von Mercer. [32, 195]


Im Falle einer Regression werden die Daten (f , r ) , j = 1, . . . , n , f Rn, r R j i T j
∈ f
j
∈ durch eine lineare Funktion h(f) = ωtΦ (f) + ω im Raum Rm approximiert. Vap- 0
nik [195] bestimmt dabei die optimalen Parameter ω und ω , indem das Funk- 0
FOOTNOTE:tional 1 ω 2 + (cid:80)nT E (f , r ) mit einer ε-insensitiven Loss-Funktion E (f, r) = 2
FOOTNOTE:(cid:107) (cid:107) j=1
ε j j ε
1 ( h(f) r > ε) h(f) r ε minimiert wird. Es wird gezeigt, dass die optimale
FOOTNOTE:Lös| ung d− er| Form| h(f ,− λ, λ| − ) = (cid:80)nT (cid:0) λ λ (cid:1) K (f , f ) + ω entspricht, wobei i ∗ j=1 ∗j
−
j j i 0
λ und λ Lösungen eines quadratischen Optimierungsproblems sind und K eine ∗
Kernel Funktion ist. [48, 74, 195, 196]


Nachdem die Bestimmung von ω durch die SVM in allen Fällen auf ein qua-
dratisches Optimierungsproblem zurückgeführt wird, ist die Lösung dieses Pro-
blems [53, 103, 152] der bestimmende Faktor bezüglich der Trainingszeit und des
Speicherbedarfs.


1.1.3.5 Neuronales Netz


Multilayer Perzeptron Rosenblatt [162, 163] beschreibt ein Perzeptron, das li-
near separierbare Daten trennen kann. Der Input f i Rnf der Instanz i wird
∈ der Klasse l 0, 1 zuweisen, indem eine lineare Trennebene zur Klassifikation i
FOOTNOTE:∈ { } benutzt wird. Das Label wird also durch l (f , ω) = 1 (ωtf + ω > 0) 0, 1 i i i 0
∈ { } berechnet. Wird zur Bestimmung des Outputs


anstatt der binären Treppenfunktion eine andere Transferfunktion F verwendet, so
wird von einem Neuron gesprochen. [13] (Abbildung 1.2) Es werden unter anderem
folgende Transferfunktionen verwendet: [9, 171]


i) Identität:


ii) Tangens hyperbolicus:


iii) Logistische Sigmoid-Funktion:


iv) Softmax-Funktion:
Erweiterung der logistischen Sigmoid-Funktion auf den d-dimensionalen Fall
mit d > 1 exp (x )


v) Exponential Linear Unit (ELU):


(1.23)


Dabei bezeichnet ψ > 0 einen wählbaren Parameter (häufig ψ = 1).


Der Zusammenschluss mehrerer Perzeptronen bzw. Neuronen in mehreren Schich-
ten wird Multilayer Perzeptron oder auch feed-forward Neuronales Netz (NN)
genannt. [13]


Training Die Gewichte ω des NN sind trainierbar und werden aus dem gelabelten
Datensatz gelernt, indem diese so angepasst werden, dass der Fehler zwischen
der input-spezifischen Netzausgabe und dem tatsächlichen Output bezüglich einer


2Eigene Abbildung in Anlehnung an [209].


sogenannten Loss-Funktion E minimal ist. Im Fall einer binären Klassifizierung loss
wird für die Loss-Funktion beispielsweise die Kreuzentropie


verwendet, die sich aus der tatsächlichen Klasse (cid:96) 0, 1 und der vorhergesag- j
∈ { } ten Wahrscheinlichkeit NN (f ) = p (0, 1) des Netzes in Abhängigkeit von ω ω j j
∈ berechnet. Im Falle einer Regression wird beispielsweise der MSE


zwischen den tatsächlichen Regressionswerten r und den ausgegebenen Werten j
NN (f ) = r R, j = 1, . . . , n bestimmt und minimiert. ω j j T ∈


Das Update de∈r Netzwerkgewichte ω erfolgt iterativ, beispielsweise über den Back-
propagation-Algorithmus [164]. Dabei werden die Gewichte im Schritt τ +1 durch


berechnet, wobei ∆ω(τ) das Gewichtsupdate bezeichnet. Das Update für eine Klas-
sifikation (analog für eine Regression) kann wie folgt berechnet werden:


i) Offline oder Batch:
Das Gewichtsupdate


wird durch alle Trainingsinstanzen (f , (cid:96) ) , j = 1, . . . , n des Klassifizie- j j T
rungsproblems bestimmt. Der Parameter ν > 0 beschreibt dabei die soge-
nannte Lernrate. Der Aufwand zur Bestimmung des Updates liegt in (n ) T
O und ist somit abhängig von der Größe des Trainingssets. [13, 71]


ii) Online:


Für jede Trainingsinstanz (f , (cid:96) ) eines Klassifizierungsproblems werden die j j
Gewichte durch NN


aktualisiert. Dieses Training besitzt somit sehr viele Gewichtsupdates. [13]


iii) Mini-Batch:


Aus dem Trainingsdatensatz werden n Elemente zu Mini-Batches zusam- b
mengefasst und die Parameter werden durch ∆ω(τ) = ν (cid:80)nb ∆ω(τ) ak-
FOOTNOTE:− j=1 j tualisiert. Dieser Ansatz aktualisiert die Gewichte seltener als das Online
Training und die Anzahl der Updates kann unabhängig von der Trainings-
größe gestaltet werden. [71]


Zur Verbesserung der Konvergenz erweitern Rumelhart und McClelland [164] das
beschriebene Gradientenabstiegsverfahren durch einen Momentum Term, sodass
das Gewichtsupdate durch


berechnet wird, wobei η einen Momentum-Parameter beschreibt. Der Adam Trai-
ningsalgorithmus von Kingma und Ba [105] benutzt ebenfalls eine Momentum-
strategie und bestimmt die Lernraten adaptiv.


Quasi-Newton Verfahren [77, 124] versuchen optimale Lernraten zu erreichen,
indem die Hesse-Matrix approximiert wird. Das skalierte konjugierte Gradien-
ten (SCG) Verfahren von Møller [142] basiert auf dem Verfahren konjugierter
Gradienten [96], führt allerdings keine Liniensuche durch. Diese Trainingsmetho-
de ist passend für große NNs, weist einen geringen Speicheraufwand und eine
schnelle Optimierung des Netzes auf und erzielt gleichzeitig gute Ergebnisse für
eine Vielzahl an Problemen. [9]


Nachdem die Gewichte ω frei wählbar sind, ist eine Normalisierung der Features
wie in Abschnitt 1.1.2.2 beschrieben für Mehrschichtnetzwerke theoretisch nicht
notwendig, da die unterschiedlichen Skalierungen auch durch die erste Netzwerk-
schicht ausgeglichen werden können. Dennoch wird dadurch oft die Prognosegüte
verbessert. [12, 191]


Generalisierung Durch die passende Wahl der Anzahl und Hierarchie der Neu-
ronen sowie der Transferfunktionen kann gezeigt werden, dass jede stetige Funk-
tion auf einer kompakten Eingangsmenge mit beliebiger Genauigkeit durch ein
NN approximiert werden kann. [36, 63, 87, 151] Im Allgemeinen ist allerdings
nicht bekannt, wie die Anzahl der Neuronen n in den versteckten Schichten und n
die Netztiefe n optimal bezüglich des zu analysierenden Problems und den zur l
Verfügung gestellten Trainingsdaten zu wählen sind. Dennoch zeigt dieses Resul-
tat, dass zumindest theoretisch eine beliebig genaue Anpassung an entsprechende
Trainingsdaten möglich ist. Die Schwierigkeit besteht vielmehr darin das trainierte


Modell nicht zu stark oder zu schwach an die Trainingsdaten anzupassen, sodass
die gelernte Relation zwischen den Features und Labels auf die neuen Daten ver-
allgemeinert wird. Im ML Kontext wird dabei von Über- und Unteranpassung
bzw. der Verallgemeinerungsleistung gesprochen. Abbildung 1.3 veranschaulicht
verschiedene Anpassungen an die Trainingsdaten anhand eines verrauschten si-
nusförmigen Regressionsproblems. [13, 191]


Um die Verallgemeinerungsleistung eines Netzes zu verbessern, sind in der Lite-
ratur (beispielsweise [9, 13, 71]) unter anderem folgende Ansätze bekannt:


i) Feature Erweiterung:


Durch das Training mit mehr Daten kann die Performance eines Netz-
werks gesteigert werden. Da aber oftmals nur eine limitierte Anzahl an Trai-
ningsdaten zur Verfügung steht, werden synthetisch neue Daten erstellt, die
dann ebenfalls zum Training des Netzes herangezogen werden. Beispielswei-
se können neue Daten erzeugt werden, indem ein Rauschen aufaddiert oder
Klassen-abhängige Inter- bzw. Extrapolationen der Features durchgeführt
werden. [43, 71]


ii) Early Stopping:


Dazu wird der Datensatz in disjunkte Trainings-, Validierungs- und Test-
daten aufgeteilt und die Performance auf den Test- und Validierungsdaten
während des Trainingsprozesses für jede Epoche ermittelt. Das Training wird
abgebrochen wenn einer der folgenden Fälle eintritt: [9]


a) Die vordefinierte maximale Anzahl an Epochen wird überstiegen.


b) Die vordefinierte Trainingsdauer wird überschritten.


3Eigene Abbildung in Anlehnung an [13].


c) Die Performance auf den Trainingsdaten unterschreitet einen vordefi-
nierten Zielwert.


d) Die Gewichtsupdates unterschreiten einen vordefinierten Schwellwert.


e) Die Performance auf den Validierungsdaten verbessert sich für eine vor-
definierte Anzahl an Epochen nicht.


Die Validierungsdaten gehen über den letzten Punkt indirekt in das Training
des NNs ein.


iii) Regularisierung:


Die Loss-Funktion E wird um einen Regularisierungsterm zu loss


erweitert. [9] Dadurch werden zusätzlich die Gewichte des Netzes und somit
die Modellkomplexität reduziert, solange die Trainingsdaten durch das NN
hinreichend gut klassifiziert werden können. Durch die Wahl des Regulari-
sierungsparameters γ wird der Trade-off zwischen Modellkomplexität und
Anpassungsfähigkeit festgelegt.


Nachdem diese Festlegung oft schwer zu bestimmen ist, benutzt MacKay [127]
den Satz von Bayes, um Regularisierungsparameter automatisch berechnen
zu können. Dan Foresee und Hagan [38] verwenden diese Methode zusammen
mit dem Levenberg-Marquardt Algorithmus, um Netze mit hoher Verallge-
meinerungsleistung zu erzeugen. Für kleine Trainingsmengen funktioniert
die Bayes’sche Regularisierung häufig besser als das Early Stopping, denn
es werden keine Daten der Trainingsmenge in eine Validierungsmenge aus-
gelagert, wodurch mehr Daten zur Optimierung des Netzes zur Verfügung
stehen. [9]


iv) Dropout:


Während des Trainings wird eine vordefinierte Anzahl an Neuronen zusam-
men mit den Verbindungen zufällig aus dem Netz entfernt (Dropout) und
stattdessen das ausgedünnte Netz optimiert. Somit lernt jedes Neuron all-
gemein nützliche Features aus den vorhergehenden Schichten zu detektieren
und zu verrechnen, anstatt nur in einem ganz spezifischen Kontext aktiviert
zu werden. In der Anwendungsphase werden die ausgedünnten Netzwerke
zu dem ursprünglichen Netzwerk zusammengefasst, indem die Netzgewichte
durch die Wahrscheinlichkeit des Vorkommens des Knotens in den ausge-
dünnten Netzwerken adaptiert werden. [83, 180]


Zeitreihenanalyse Die folgenden speziellen NN Typen werden zur Analyse von
Zeitreihenfeatures und Regressionswerten verwendet. Für diese Daten sind Z Z
F R alle Feature-Werte-Paare (f(z ), r(z )) , k = 1, . . . , n abhängig vom Zeitpunkt z . k k z k


Ein TDNN benutzt n zeitverzögerte Inputs in der Eingabeschicht des feed- I
forward Netzes zur Vorhersage von


Waibel et al. [200] verwenden die Verzögerung auch zwischen den verstecken
Schichten des feed-forward Netzes.


ii) Nonlinear Autoregressive Exogenous Model (NARX):
Leontaritis und Billings [120, 121] beschreiben ein rekurrentes NN, das so-
wohl n zeitverzögerte Inputs als auch n Outputs als Eingabe in das Netz I O
verwenden. Der Output zum Zeitpunkt z des NNs wird also durch k


bestimmt. Werden die bekannten Regressionwerte r(z ), . . . , r(z ) der
Trainingsdaten statt der Netzvorhersagen benutzt, so hk a−n1 delt es sick h−n uO m ein
reines feed-forward Netz.


Hochreiter und Schmidhuber [85] beschreiben ein rekurrentes Netzwerk mit
speziellen Netzeinheiten, wodurch Langzeitabhängigkeiten gelernt werden.
Im Gegensatz zu rekurrenten Netzwerken besitzen die Netzeinheiten eines
LSTMs Input-, Output- und Forget-Gates [64], die die in den Einheiten ab-
gespeicherten Informationen organisieren. Dadurch werden verschwindende
oder explodierende Gradienten im Training (Error Backpropagation) des re-
kurrenten Netzwerkes vermieden, wodurch eine Analyse von Zeitreihen mit
vielen Zeitschritten ermöglicht wird.


Der Loss für ein LSTM Netzwerk wird oftmals bezüglich der in einem Mini-
Batch enthaltenen Zeitreihen ausgewertet. Für ein Regressionsproblem er-
gibt sich damit beispielsweise


1.1.3.6 Boosting


Schapire [59, 168] führt sogenannte Boosting Algorithmen [59, 60, 62, 174, 193] ein,
die durch die gewichtete Summe von „schwachen Lernern“ einen „starken Lerner“
erzeugen. Als schwache Lerner können beispielsweise SVMs [195] oder Entschei-
dungsstümpfe [174, 193] verwendet werden. Entscheidungsstümpfe treffen Ent-
scheidungen der Form f δ und können somit als Entscheidungsbäume der ik k
≤ Tiefe 1 betrachtet werden. Auf diese Weise wird durch die Entscheidungsstümpfe
die Relevanz der Features bestimmt. [193]


1.1.4 Bewertungsmetriken


1.1.4.1 Klassifikation


Zur Bewertung der Prognosegüte des Modells werden die tatsächlichen Markierun-
gen (cid:96) den vom Algorithmus ausgegebenen Klassen l gegenübergestellt. Im binären
Fall ergeben sich dadurch folgende vier Fälle. Es werden die Bezeichnungen po-
sitiv bzw. negativ für die Ergebnisklassifizierung und wahr bzw. falsch für die
tatsächliche Klassifizierung verwendet.


i) True Positive (TP):


Die Anzahl der vom Algorithmus positiv klassifizierten Entitäten, die tat-
sächlich wahr sind. (l = (cid:96) = 1)


Die Anzahl der vom Algorithmus negativ klassifizierten Entitäten, die tat-
sächlich falsch sind. (l = (cid:96) = 0)


iii) False Positive (FP):


Die Anzahl der vom Algorithmus positiv klassifizierten Entitäten, die tat-
sächlich falsch sind. (l = 1, (cid:96) = 0)


iv) False Negative (FN):


Die Anzahl der vom Algorithmus negativ klassifizierten Entitäten, die tat-
sächlich wahr sind. (l = 0, (cid:96) = 1)


Auf Basis dieser lassen sich wichtige Kennziffern der Prognosegüte des ausgewer-
teten Modells definieren, die neben anderen [153] entnommen werden können:


i) Der Recall oder die True Positive Rate (TPR) messen die Wahrscheinlich-
keit, dass eine tatsächlich wahre Entität als positiv klassifiziert wird.


ii) Die Präzision oder die True Positive Accuracy (TPA) messen die Wahr-
scheinlichkeit, dass eine als positiv klassifizierte Entität auch tatsächlich
wahr ist. TP


iii) Die Genauigkeit misst die Wahrscheinlichkeit mit der der Algorithmus den
N Entitäten die tatsächliche Klasse zuweist.


iv) Das F-Maß [189] kombiniert den Recall und die Präzision über ein gewich-
tetes harmonisches Mittel


v) Der Matthews Korrelationskoeffizient (MCC) [136] bündelt alle vier Fälle
eines binären Klassifizierungsproblems durch


Werden die zu identifizierenden Entitäten durch (cid:96) = 1 gelabelt, so sind insbeson-
dere die Wahrscheinlichkeit und die Präzision der Detektion, also TPR und TPA
relevant. Gute Segmentierungen resultieren in hohen Werten für beide Kennzif-
fern, allerdings ist für die direkte Vergleichbarkeit der dazugehörigen Algorithmen
eine einzelne Metrik, wie Genauigkeit, F oder MCC notwendig. 2


Für den Fall eines schiefen Datensatzes ergibt die Klassifikation aller Instanzen
mit der mehrheitlich vorkommenden Klasse bereits eine hohe Genauigkeit, ob-
wohl die Güte des Klassifikators schlecht ist. Wenn die Codierung der Klassen
FOOTNOTE:getauscht wird, schwankt der F Wert, denn dieser ist unabhängig von der Anzahl 2
der TN Instanzen. Der MCC berücksichtigt dagegen alle Fälle des Klassifizie-
rungsproblems, ist invariant unter der Codierung der Klassen und ist für schiefe
Datensätze geeignet. [29, 30, 189]


1.1.4.2 Regression


Im Falle der Regression wird die Abweichung der eigentlichen Regressionswerte r i
mit den durch den Algorithmus vorhergesagten Werten r für alle Testinstanzen i
i = 1, . . . , n verglichen. Dazu eignen sich unter anderem folgende Metriken: [19] A


i) Maximum Absolute Error (MaxAE):


ii) Mean Average Error (MAE):


iii) Mean Squared Error (MSE):


iv) Root Mean Squared Error (RMSE): RMSE (r, r) = √MSE


v) Mean Absolute Percentage Error (MAPE):


vi) Symmetric Mean Absolute Percentage Error (SMAPE):


Der MAE, MSE, RMSE und der MAPE sind dabei in der Literatur am häufigsten
vertreten. [19]


1.1.5 Hyperparameter Optimierung


Die oben beschriebenen Methoden der KI besitzen verschiedene Hyperparameter,
die die Ergebnisse der Methoden signifikant beeinflussen können. Beispielsweise
kann für ein NN die Anzahl der Neuronen pro Schicht n oder die Anzahl der n
Schichten n variiert werden. Da die optimale Wahl der Hyperparameter im All- l
gemeinen nicht a priori bekannt ist, wird oftmals eine Rastersuche oder eine ma-
nuelle Suche zur Hyperparameter Optimierung verwendet. Dazu werden im ersten
Fall alle in einem Hyperparameterraum vorkommenden Werte der Parameter
P kombiniert und das Modell bezüglich dieser Parameterkonfiguration hinsichtlich
einer Bewertungsmetrik (Abschnitt 1.1.4) evaluiert. Für kontinuierliche Parameter
wird eine feste Anzahl an Auswertungspunkten festgesetzt. Die manuelle Suche
wird verwendet, um Bereiche des Parameterraums zu identifizieren, die vielver-
sprechend sind. [11]


Eine andere Möglichkeit zur Hyperparameter Optimierung besteht darin, die Pa-
rameter zufällig zu wählen. Dies führt insbesondere dann zu einer wesentlichen
Verbesserung der Ergebnisse, wenn das Modell bezüglich des einen Parameters
sehr robust und eines anderen sensitiv ist, denn wie Abbildung 1.4 veranschau-
licht, wird mit der selben Anzahl an Auswertungen der sensitive Parameter durch
die Zufallssuche öfter abgetastet. Aus diesem Grund resultiert eine zufällige Wahl
an Parametern oft in besseren Ergebnissen. [11]


Neben diesen einfachen Optimierungsmethoden sind weitere Algorithmen [37, 92,
147] zur Wahl der Hyperparameter in der Literatur bekannt.


4Eigene Abbildung in Anlehnung an [11].


1 Aktueller Kenntnisstand


1.2 Segmentierung von 3D Objekten


1.2.1 Geometrierepräsentation


1.2.1.1 Computer Aided Design Austauschdatenformate


Die Mercedes-Benz AG verwendet zur Darstellung, Verwaltung und Dokumen-
tation von Design- und Konstruktions-Daten der verschiedenen Baureihen eine
Computer Aided Design (CAD) Applikation namens Engineering Client. Abbil-
dung 1.5 veranschaulicht, wie Experten korrosionskritische Konstruktionen (Ab-
schnitt 1.3.1.2), wie Kanten (orange) und Flansche (rote Box) mithilfe dieses Tools
visuell ermitteln. Die Geometriedaten sind so aus dem Engineering Client auszule-
FOOTNOTE:sen, dass diese in eine programmierbare Umgebung, wie beispielsweise Matlab R , (cid:13)
importiert und analysiert werden können.


Abbildung 1.5: Querschnitt durch eine Kotflügel Baugruppe zur Darstellung von
Bauteilkanten (orange) und eines Flansches (rot).


Die Masterarbeit [201] beschreibt die Extraktion der Geometriedaten der digi-
talen CAD Daten aus der Mercedes-Benz internen Datenbank Smaragd. Dazu
wird zunächst über den Engineering Client das zu extrahierende Bauteil oder die
Bauteilgruppe ausgewählt und an das Programm Siemens NX weitergeleitet, das
wiederum die Speicherung der Geometrie als


i) Stereo Lithography (STL) [94],


ii) Initial Graphics Exchange Specification (IGES) [177] oder


iii) Standard for the Exchange of Product Model Data (STEP) [95]


Datei erlaubt. Es stehen jeweils rund 40 Fahrzeugmodelle zur Verfügung, die in
Siemens NX bzw. in Catia V5 konstruiert sind. Die älteren Catia Daten weisen
allerdings eine schlechtere Datenqualität auf und können nur als STL extrahiert
werden, da diese Daten plattformübergreifend migriert werden.
Außerdem werden in der Masterarbeit [201] die verschiedenen Austauschdatenfor-
mate hinsichtlich ihrer Eignung zur semantischen Segmentierung, also dem Zuord-
nen einzelner Entitäten in bestimmte Klassen, untersucht. Das STL Format bietet
den Vorteil, dass die Geometrie ausschließlich durch Dreieckselemente beschrieben
wird (siehe Abschnitt 1.2.1.2), wohingegen IGES und STEP Daten eine Vielzahl
an unterschiedlichen geometrischen Objekten nutzen, um in Summe die Geometrie
zu repräsentieren. Auch wenn IGES und STEP die Geometrie genauer beschrei-
ben, muss doch jede Objektklasse einzeln analysiert werden, was das Design von
Klassifikationsalgorithmen erschwert. Außerdem beschreiben manche geometrische
Objekte der IGES und STEP Daten große Flächen, weshalb die Klassifikation die-
ser in einer gröberen Segmentierung resultiert als dies der Fall für STL Daten ist.
Des Weiteren werden STL Daten auch für andere Untersuchungen der Mercedes-
Benz AG, wie beispielsweise zur Steinschlag- oder Wassersimulationen verwendet.
Aus diesen Gründen empfiehlt die Masterarbeit [201] die Verwendung der STL
Daten, weswegen auf diese im nächsten Kapitel nochmals genauer eingegangen
wird.


1.2.1.2 Stereo Lithography Daten


Das von 3D Systems beschriebene Stereo Lithography (STL) Datenformat [94]
besitzt folgende Struktur:


solid <name>
facet normal <single> <single> <single>
outer loop
vertex <single> <single> <single>
vertex <single> <single> <single>
vertex <single> <single> <single>
endloop
endfacet
facet normal <single> <single> <single>
outer loop
. .
.
endfacet
endsolid


